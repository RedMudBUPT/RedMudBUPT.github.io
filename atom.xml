<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RedMudBUPT</title>
  
  <subtitle>RedMud&#39;s blog called RedMudBUPT</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-06-14T14:43:22.644Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>redmud</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>cpp与python异同</title>
    <link href="http://yoursite.com/2018/06/14/cpp%E4%B8%8Epython%E5%BC%82%E5%90%8C/"/>
    <id>http://yoursite.com/2018/06/14/cpp与python异同/</id>
    <published>2018-06-14T14:43:22.000Z</published>
    <updated>2018-06-14T14:43:22.644Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习——图像语义分割</title>
    <link href="http://yoursite.com/2018/05/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    <id>http://yoursite.com/2018/05/30/机器学习——图像语义分割/</id>
    <published>2018-05-30T06:57:16.000Z</published>
    <updated>2018-06-03T12:57:40.789Z</updated>
    
    <content type="html"><![CDATA[<h2 id="全卷积网络（FCN）"><a href="#全卷积网络（FCN）" class="headerlink" title="全卷积网络（FCN）"></a>全卷积网络（FCN）</h2><blockquote><p>全卷积网络是从抽象的特征中恢复出每个像素所属的类别，即从图像级别的分类进一步延伸到像素级别的分类。</p></blockquote><p>将最后的输出是1000张heatmap经过upsampling变为原图大小的图片，然后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。</p><a id="more"></a> ]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;全卷积网络（FCN）&quot;&gt;&lt;a href=&quot;#全卷积网络（FCN）&quot; class=&quot;headerlink&quot; title=&quot;全卷积网络（FCN）&quot;&gt;&lt;/a&gt;全卷积网络（FCN）&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;全卷积网络是从抽象的特征中恢复出每个像素所属的类别，即从图像级别的分类进一步延伸到像素级别的分类。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;将最后的输出是1000张heatmap经过upsampling变为原图大小的图片，然后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——特征工程</title>
    <link href="http://yoursite.com/2018/05/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/05/27/机器学习——特征工程/</id>
    <published>2018-05-27T07:51:29.000Z</published>
    <updated>2018-06-10T09:13:19.506Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>PERSPECT：好的特征能降低建模的难度。</p></blockquote><h2 id="简单数字的花式技巧"><a href="#简单数字的花式技巧" class="headerlink" title="简单数字的花式技巧"></a>简单数字的花式技巧</h2><blockquote><p>特征空间与数据空间:<br>特征（如，每个人喜欢的一个歌单）是在特征空间中的一个向量，一个人可由特征空间中的向量表示出来；而针对某首特定的歌，每一个人对其喜爱程度构成一个维度，所有的维度构成数据空间，一首歌可由数据空间中的向量表示出来。若将一个人的所有特征并成一条向量作为矩阵的行向量，那么该矩阵（称为数据矩阵）的列向量空间便是特征空间，行向量空间是数据空间。</p></blockquote><ol><li>scale<ul><li>基于输入数据的光滑函数的模型（如K-means聚类，线性函数，最近邻，REB核等）对输入数据的尺度很敏感，故通常将输入数据进行归一化，以使得输出数据在统一的尺度上。</li></ul></li><li>distribution<ul><li>有些模型对输入数据的分布有一定要求，如线性回归模型要求预测误差服从高斯分布，通常这一要求是无需考虑的，但当输出值跨越好几个量级时，这一要求便无法满足，须将输出值取对数（log 转换，一种量级转换），这可使得预测误差更接近高斯分布（准确说，这属于 target engineering）。比如，在 Faster-R-CNN 中的目标框回归中对尺度因子的预测，将尺度因子取对数作为回归量。</li></ul></li></ol><a id="more"></a> <h3 id="当某维的特征没有边界时（scale）"><a href="#当某维的特征没有边界时（scale）" class="headerlink" title="当某维的特征没有边界时（scale）"></a>当某维的特征没有边界时（scale）</h3><p>feature scaling 无法改变特征的分布，只能够改变特征的范围（range）。</p><h4 id="min-max-scaling"><a href="#min-max-scaling" class="headerlink" title="min-max scaling"></a>min-max scaling</h4><p>$$<br>x^* = \frac{x - min(x)}{max(x) - min(x)}<br>$$</p><p>scaled feature 的范围为 (0, 1)。</p><h4 id="variance-scaling（standardization）"><a href="#variance-scaling（standardization）" class="headerlink" title="variance scaling（standardization）"></a>variance scaling（standardization）</h4><p>$$<br>x^* = \frac{x- mean(x)}{var(x)}<br>$$</p><p>scaled feature 的均值为0，方差为1，而其范围视情况而定，不是固定数值。</p><h4 id="L-2-normalization"><a href="#L-2-normalization" class="headerlink" title="L-2 normalization"></a>L-2 normalization</h4><p>$$<br>x* = \frac{x}{||x||_2}<br>$$</p><h3 id="当某维特征的量级跨越过多时（distribution）"><a href="#当某维特征的量级跨越过多时（distribution）" class="headerlink" title="当某维特征的量级跨越过多时（distribution）"></a>当某维特征的量级跨越过多时（distribution）</h3><p>当某维特征的量级跨越过多时，很多模型可能会出问题。如在线性模型中，因为线性系数被期望可以接受所有可能的输入值，故当某维特征的量级跨越过多时，该维特征会在不知情的情况下被偏向成为重要性远高于其他特征的主要特征，换句话说其他特征会被自动忽略掉。</p><p>最简单暴力的解决方法为 quantization / binning，如将少年年龄映射为1，青年年龄映射为2，中年年龄映射为3，老年年龄映射为4；或者将个位数映射为1，十位数映射为2，百位数映射为3，千位数映射为4，等等。</p><h4 id="log-transform"><a href="#log-transform" class="headerlink" title="log transform"></a>log transform</h4><p>有个常用的连续的映射，log 转换 $data<em>{transformed} = \log (data</em>{raw} + 1)$ ：它的作用是压缩大数（大于1）的尺度，放大小数（小于1）的尺度（一般不用），常用于处理重尾分布（A heavy-tailed distribution places more probability mass in the tail ranges than a Gaussian distribution.）的正数特征，在将重尾分布的正数特征做 log 转换后，特征的分布会更趋向于高斯分布。</p><p><a href="http://www.360doc.com/content/12/1201/15/10595682_251380435.shtml" target="_blank" rel="noopener">R square</a>是决定系数，意思是你拟合的模型能解释因变量的变化的百分数，例如 R square = 0.81，表示你拟合的方程能解释因变量81%的变化，还有19%是不能够解释的。<br>$$<br>R_square_score = \frac{\sum(wx - \bar y )^2}{\sum(y - \bar y )^2}<br>$$</p><h4 id="Box-Cox-变换-（generalization-of-log-transform）"><a href="#Box-Cox-变换-（generalization-of-log-transform）" class="headerlink" title="Box-Cox 变换 （generalization of log transform）"></a><a href="https://blog.csdn.net/sinat_26917383/article/details/77864582" target="_blank" rel="noopener">Box-Cox 变换</a> （generalization of log transform）</h4><blockquote><p>在单因子方差分析的统计模型中，需满足三个假设：</p><ol><li>正态性，每一维度下观察值的总体是正态总体；</li><li>独立性，从每一总体中抽取的样本是相互独立的；</li><li>方差齐性，各总体的方差等于常数。</li></ol><p>试验过程中保证随机性便可满足独立性的假设，而对于特征的选择却很难保证正态性和方差齐性。虽然正态性与方差齐性会相辅相成，互为影响，但因为方差齐性比正态性更为重要，故方差齐性变换（又称为方差稳定性变换）尤为重要，详见：</p><p><a href="https://wenku.baidu.com/view/81db5e56852458fb770b562d.html" target="_blank" rel="noopener">方差稳定化变换综述</a></p></blockquote><p>$$<br>x^* = {^{\frac{x^\lambda - 1}{\lambda}, \lambda \ne 0}<br>_{\log x, \lambda = 0}<br>$$</p><p>参数小于1时，压缩大数的尺度，参数大于1时放大大数的尺度。该参数可由极大似然或者贝叶斯法得到最佳值，当然可直接<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html#scipy.stats.boxcox" target="_blank" rel="noopener">调包</a>求得。</p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a><a href="https://www.cnblogs.com/AHappyCat/p/5318042.html" target="_blank" rel="noopener">特征选择</a></h3><blockquote><p>特征选择和降维都是为了减少数据集的属性数量，但是降维是通过创建新的属性组合，特征选择挑选数据中的属性而并不改变它们。</p><p>特征选择的目的：提高预测器的预测性能，提供更快和更低成本的预测器，和提供更好的了解产生的数据的基本过程。</p></blockquote><p>分为以下三步：</p><h4 id="filtering"><a href="#filtering" class="headerlink" title="filtering"></a>filtering</h4><p>计算每个特征与响应变量之间的相关信息和互信息，过滤掉低于阈值的特征。缺点是它不考虑模型是否采用该特征。</p><h4 id="wrapper"><a href="#wrapper" class="headerlink" title="wrapper"></a>wrapper</h4><p>当模型需要特征融合时很有用。缺点是它计算很昂贵。</p><h4 id="embedded-methods"><a href="#embedded-methods" class="headerlink" title="embedded methods"></a>embedded methods</h4><p>将模型选择作为模型训练的一部分，如决策树，再如L-1正则化。它平衡了以上两点。</p><h2 id="用PCA压缩数据集"><a href="#用PCA压缩数据集" class="headerlink" title="用PCA压缩数据集"></a>用PCA压缩数据集</h2><blockquote><p>主成分分析命名：投影数据被称为原始数据的主成分。</p><p>其优化方向为两个：最近重构性（样本点到这个超平面的距离都足够近）和最大可分性（样本点在这个超平面上的投影能尽可能分开，常用）</p><p>PCA 通过查找线性相关模式来减少特征空间的维度。</p></blockquote><p>PCA 侧重于线性依赖的概念，关键思想是<strong>用一些充分总结原始特征空间中包含信息的新特征取代冗余特征</strong>。</p><p>为保证新特征的最大可分性，可以寻找一个超平面使得投影后的任意两点之间的距离最大化。但事实证明，这是一个非常困难的数学优化问题。另一种方法是测量任意两点之间的平均距离，或者等价地，<strong>每个点与它们的平均值之间的平均距离</strong>，即方差。事实证明，这优化起来要容易得多。由此 PCA 的最优化问题为：<br>$$<br>\max_W tr(\sum_i (x_iW)^T(x_iW))<br>\<br>s.t.  \quad W^TW = I<br>$$<br>即：<br>$$<br>\max_W tr(W^T X^TXW)<br>\<br>s.t.  \quad W^TW = I<br>$$<br>其中，X 为数据矩阵，每一行为一条数据，记为$x_i$，W 为基，每一列为一个基向量，投影后的数据点为$XW$ 。</p><p>该问题的数学本质为：输入为单位向量，寻找该单位向量能够使得输出的范数最大化的方向。故$W$ 为$X^T X$ 的主要特征值（按从大到小排序，较大的那些特征值）对应的单位特征向量，或者说 <strong>X 的主要右奇异向量</strong>（奇异值按从大到小排序），两者一样。数学原理见<a href="http://redmud.xyz/2018/02/28/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E7%A1%80/" target="_blank" rel="noopener">对称阵的特征向量与特征值</a>与<a href="https://www.sciencedirect.com/science/article/pii/0024379587901145" target="_blank" rel="noopener">A generalization of the Eckart-Young-Mirsky matrix approximation theorem</a>。</p><p>由此通过对中心化的数据矩阵作奇异值分解，便可得到 PCA 的解：<br>$$<br>X^* = XW = (U \Sigma V^T) V_k = U_k \Sigma_k.<br>$$</p><h3 id="降维、投影与换基"><a href="#降维、投影与换基" class="headerlink" title="降维、投影与换基"></a>降维、投影与换基</h3><p>降维即投影变换，通过换基后将非投影面的维度置零实现，属于可逆操作，即可升维。</p><h3 id="k-值的选择"><a href="#k-值的选择" class="headerlink" title="k 值的选择"></a>k 值的选择</h3><p>对于在新的特征空间中的第 k 维特征的方差为<br>$$<br>||Xw_k||^2_2 = ||u_k \sigma_k||^2_2 = \sigma_k^2,<br>$$<br>故常称中心化的数据矩阵的奇异值列表为其频谱。</p><p>而 k 值的选择常依据在新的特征空间中主成分特征的方差和占总特征的方差和的比例：<br>$$<br>\frac{\sum_i^k \sigma_i^2}{\sum_i^d \sigma_i^2} \ge proportion(usually \quad 0.8)<br>$$</p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ol><li>在应用 PCA 之前最好先将特征做好 scale 和 distribution 的处理；</li><li>涉及 SVD，计算成本大；</li></ol><h2 id="非线性特征提取和模型堆叠"><a href="#非线性特征提取和模型堆叠" class="headerlink" title="非线性特征提取和模型堆叠"></a>非线性特征提取和模型堆叠</h2>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;PERSPECT：好的特征能降低建模的难度。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;简单数字的花式技巧&quot;&gt;&lt;a href=&quot;#简单数字的花式技巧&quot; class=&quot;headerlink&quot; title=&quot;简单数字的花式技巧&quot;&gt;&lt;/a&gt;简单数字的花式技巧&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;特征空间与数据空间:&lt;br&gt;特征（如，每个人喜欢的一个歌单）是在特征空间中的一个向量，一个人可由特征空间中的向量表示出来；而针对某首特定的歌，每一个人对其喜爱程度构成一个维度，所有的维度构成数据空间，一首歌可由数据空间中的向量表示出来。若将一个人的所有特征并成一条向量作为矩阵的行向量，那么该矩阵（称为数据矩阵）的列向量空间便是特征空间，行向量空间是数据空间。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;scale&lt;ul&gt;
&lt;li&gt;基于输入数据的光滑函数的模型（如K-means聚类，线性函数，最近邻，REB核等）对输入数据的尺度很敏感，故通常将输入数据进行归一化，以使得输出数据在统一的尺度上。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;distribution&lt;ul&gt;
&lt;li&gt;有些模型对输入数据的分布有一定要求，如线性回归模型要求预测误差服从高斯分布，通常这一要求是无需考虑的，但当输出值跨越好几个量级时，这一要求便无法满足，须将输出值取对数（log 转换，一种量级转换），这可使得预测误差更接近高斯分布（准确说，这属于 target engineering）。比如，在 Faster-R-CNN 中的目标框回归中对尺度因子的预测，将尺度因子取对数作为回归量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——支撑向量机</title>
    <link href="http://yoursite.com/2018/05/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%92%91%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://yoursite.com/2018/05/23/机器学习——支撑向量机/</id>
    <published>2018-05-23T06:18:55.000Z</published>
    <updated>2018-06-10T12:18:53.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>解决了两个问题：</p><ol><li>通过最大化 margin，解决 PLA 健壮性差的问题；</li><li>通过对偶问题的转换和核函数，解决了在引入特征转换解决线性不可分问题时导致的复杂度高的问题。</li></ol><p>（附）软间隔：</p><a id="more"></a> <h3 id="由问题引出的两个核心点"><a href="#由问题引出的两个核心点" class="headerlink" title="由问题引出的两个核心点"></a>由问题引出的两个核心点</h3><h4 id="margin-的表示形式"><a href="#margin-的表示形式" class="headerlink" title="margin 的表示形式"></a>margin 的表示形式</h4><h5 id="点到超平面的距离"><a href="#点到超平面的距离" class="headerlink" title="点到超平面的距离"></a>点到超平面的距离</h5><p>两种表示形式：</p><ol><li>几何间隔：常用于在欧式空间中做距离度量，通过取超平面一点指向所研究点的向量，将该向量与超平面的单位法向量做内积便得几何间隔</li></ol><p>$$<br>margin = \frac{|w^Tx + b|}{||w||}<br>$$</p><ol><li>函数间隔：常用于做正负判断，也可用于做与超平面距离远近的比较，通过将点的坐标直接带入函数得到；<strong>函数间隔本质是几何间隔乘以法向量的长度，即$||w||$。</strong></li></ol><p>$$<br>margin = |w^Tx + b|<br>$$</p><h5 id="最大化-margin-得到的-prime-SVM"><a href="#最大化-margin-得到的-prime-SVM" class="headerlink" title="最大化 margin 得到的 prime SVM"></a>最大化 margin 得到的 prime SVM</h5><p>首先假设数据 x 经特征转换后变为的 z 线性可分。</p><p>出于数学表达形式简便上的考虑，设 label 为 +1 和 -1，那么最大化 margin，便是最大化最小 margin，则优化问题为：<br>$$<br>\max<em>{w,b} { \frac{\min(label<em>(w^Tz+b))}{||w||} }<br>$$<br>因为 margin 比较的是相对大小，出于便于优化（往二次规划靠拢）上的考虑，设 $\min(label</em>(w^Tz+b))=1$ ，故优化问题化为：<br>$$<br>\max</em>{w,b} \frac{1}{||w||}<br>\<br>s.t. \quad label*(w^Tz+b) \ge 1<br>$$<br>由此，通过对函数间隔最小值的设定，将 margin 的大小化简为了法向量长度的倒数，即 $\frac{1}{||w||}$ 。</p><h5 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h5><p>当数据 x 经特征转换后变为的 z 线性不可分时，将越过margin的点做记录，并将越过的量计入损失函数中，至于是以函数间隔还是几何间隔的形式计入损失函数中，应衡量函数间隔计算上的便易性与几何间隔物理含义上的合理性。当采用几何间隔的形式计入损失函数中时，优化问题化为：<br>$$<br>\max \frac{1}{||w||} - C \sum \frac{\lambda_i}{||w||}<br>\<br>s.t. \quad {^{label_i <em> (w^T z_i + b) \ge 1 - \lambda<em>i}</em>{ \lambda_i \ge 0}<br>$$<br>其中 C 是一超参数，该数越大，在优化时会越偏向于将所有点置于分界面之外。考虑到函数间隔也能体现错误点与分界面的远近，同时超参数的存在使得几何间隔确切的物理含义的意义不大，又考虑到优化的便易性，将优化问题化为：<br>$$<br>\min \frac{1}{2}||w||_2^2 + C \sum \eta_i<br>\<br>s.t. \quad {^{label_i </em> (w^T z_i + b) \ge 1 - \eta<em>i}</em>{ \eta_i \ge 0}<br>$$</p><h4 id="对偶问题的转换"><a href="#对偶问题的转换" class="headerlink" title="对偶问题的转换"></a>对偶问题的转换</h4><p>由不等式约束的拉格朗日乘子法，得到等价的 KKT 条件下的拉格朗日对偶函数：<br>$$<br>\min_{w, b, \eta} \frac{1}{2}||w||_2^2 +C \sum \eta_i + \sum \lambda_i^0 (1- \eta_i - label_i <em>(w^T z_i + b)) +  \sum \lambda_i^1(-\eta_i)<br>\<br>s.t.<br>\<br>\quad \lambda_i^0 \ge 0<br>\<br>\quad  1- \eta_i - label_i </em>(w^T z_i + b) \le 0<br>\<br>\quad  \lambda_i^0(1- \eta_i - label_i *(w^T z_i + b) ) = 0<br>\<br>\quad  \lambda_i^1 \ge 0<br>\<br>\quad   -\eta_i \le 0<br>\<br>\quad  \lambda_i^1( -\eta_i ) = 0<br>$$<br>记拉格朗日函数为$L$，将拉格朗日函数对各变量的导数分别置零得：<br>$$<br>\frac{\partial L}{\partial w} = 0 \Rightarrow w = \sum\lambda_i^0 label_i z_i<br>\<br>\frac{\partial L}{\partial b} = 0 \Rightarrow \sum \lambda_i^0 label_i = 0<br>\<br>\frac{\partial L}{\partial \eta_i} = 0 \Rightarrow C - \lambda_i^0 - \lambda_i^1 = 0<br>$$<br>将三个结果带入最优化问题中，得拉格朗日对偶函数：<br>$$<br>L =  -\frac{1}{2}||w||_2^2 + \sum \lambda_i^0<br>\<br>=  -\frac{1}{2} \sum_i^m \sum_j^m \lambda_i^0 \lambda_j^0 label_i label_j (z_i z_j^T) + \sum \lambda_i^0  , \quad 0 \le \lambda<em>i^0 \le C.<br>$$<br>进而得到最终的对偶问题：<br>$$<br>\min</em>{\lambda^0} \frac{1}{2} \sum_i^m \sum_j^m \lambda_i^0 \lambda_j^0 label_i label_j (z_i z_j^T) - \sum \lambda_i^0<br>\<br>s.t.<br>\<br>\quad 0 \le \lambda_i^0 \le C<br>\<br>\quad  \sum \lambda_i^0 label_i = 0<br>$$</p><h5 id="对偶变量的物理含义"><a href="#对偶变量的物理含义" class="headerlink" title="对偶变量的物理含义"></a>对偶变量的物理含义</h5><p>在对偶问题的转换过程中可得到如下互补松弛条件：<br>$$<br>\lambda_i^0(1- \eta_i - label_i *(w^T z_i + b) ) = 0<br> \<br> (C - \lambda_i^0) \eta_i = 0<br>$$<br>由此可得（称 $\lambda_i^0$ 为对偶变量，称 $\eta_i$ 为越界变量）：</p><ul><li>对于对偶变量等于0的点，为 non SV，越界变量等于零；</li><li>对于对偶变量大于0小于 C（越界惩罚系数）的点，为 free SV，越界变量等于零，这些点决定了 w 和 b；</li><li>对于对偶变量等于 C 的点，为 bounded SV，越界变量大于零；</li></ul><h5 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h5><p>当采用特征转换（如高次多项式）将数据从低维空间映射到高维空间时，会带来模型复杂度的指数级增长，而核函数会解决这个问题，即即使映射到高维空间，模型复杂度依然保持低维空间时的大小。一种观点是核函数是度量两个向量在某个高维空间中的距离的度量尺，不同高维空间对应不同的核函数。具体原理以二次多项式为例：</p><p>待续。。</p><p>进而，原对偶问题化为：<br>$$<br>\min_{\lambda^0} \frac{1}{2} \sum_i^m \sum_j^m \lambda_i^0 \lambda_j^0 label_i label_j K(x_i x_j^T) - \sum \lambda_i^0<br>\<br>s.t.<br>\<br>\quad 0 \le \lambda_i^0 \le C<br>\<br>\quad  \sum \lambda_i^0 label_i = 0<br>$$<br>故计算复杂度由原先的$O(m^d)$降为了$O(m)$，d 指特征变化到的高维空间的维度。</p><h3 id="二次规划"><a href="#二次规划" class="headerlink" title="二次规划"></a>二次规划</h3><h4 id="SMO"><a href="#SMO" class="headerlink" title="SMO"></a>SMO</h4><h2 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a>SVR</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;SVM&quot;&gt;&lt;a href=&quot;#SVM&quot; class=&quot;headerlink&quot; title=&quot;SVM&quot;&gt;&lt;/a&gt;SVM&lt;/h2&gt;&lt;h3 id=&quot;解决的问题&quot;&gt;&lt;a href=&quot;#解决的问题&quot; class=&quot;headerlink&quot; title=&quot;解决的问题&quot;&gt;&lt;/a&gt;解决的问题&lt;/h3&gt;&lt;p&gt;解决了两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过最大化 margin，解决 PLA 健壮性差的问题；&lt;/li&gt;
&lt;li&gt;通过对偶问题的转换和核函数，解决了在引入特征转换解决线性不可分问题时导致的复杂度高的问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;（附）软间隔：&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>如何阅读文献</title>
    <link href="http://yoursite.com/2018/05/21/%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E6%96%87%E7%8C%AE/"/>
    <id>http://yoursite.com/2018/05/21/如何阅读文献/</id>
    <published>2018-05-21T09:27:43.000Z</published>
    <updated>2018-06-03T12:56:56.488Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a> <p><a href="http://www.sciencemag.org/careers/2016/03/how-seriously-read-scientific-paper" target="_blank" rel="noopener">http://www.sciencemag.org/careers/2016/03/how-seriously-read-scientific-paper</a>  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt; 
&lt;p&gt;&lt;a href=&quot;http://www.sciencemag.org/careers/2016/03/how-seriously-read-scientific-paper&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;
      
    
    </summary>
    
      <category term="工具的使用" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——BatchNormalization</title>
    <link href="http://yoursite.com/2018/05/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94BatchNormalization/"/>
    <id>http://yoursite.com/2018/05/12/机器学习——BatchNormalization/</id>
    <published>2018-05-12T08:25:21.000Z</published>
    <updated>2018-06-03T12:57:03.692Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a> ]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt; 
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——faster_RCNN</title>
    <link href="http://yoursite.com/2018/05/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2018/05/11/机器学习——目标检测/</id>
    <published>2018-05-11T14:14:12.000Z</published>
    <updated>2018-06-12T09:15:34.302Z</updated>
    
    <content type="html"><![CDATA[<h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><h2 id="FCN-Fully-Convolutional-Networks-全卷积神经网络"><a href="#FCN-Fully-Convolutional-Networks-全卷积神经网络" class="headerlink" title="FCN(Fully Convolutional Networks)全卷积神经网络"></a><a href="https://blog.csdn.net/app_12062011/article/details/60956357" target="_blank" rel="noopener">FCN</a>(Fully Convolutional Networks)全卷积神经网络</h2><p>顾名思义，FCN 即全为卷积层的神经网络，没有全连接层，或者说将全连接层以卷积层的方式实现，而非以两个矩阵乘积的方式实现（全连接层的实现方式），具体为：</p><ul><li>对于上一层是卷积层的全连接层，该层以与输入的 feature map 大小相同的卷积核做卷积运算实现；</li><li>对于上一层是全连接层的全连接层，该层以输入的神经元个数为 1*1 卷积核通道数做卷积运算实现，或者说是将神经元看做了一个 feature map，或者说是将一个 feature map 看做了一个神经元的激活值（激活值不一定是个标量）。</li></ul><p>这样做的优点是，通过在最后一层做 pooling（核大小为 feature map 大小）的方法，可实现让一个已设计完毕的网络可以接收任意大小的图片。</p><a id="more"></a> <h2 id="NMS-Non-Maximum-Suppression-非极大值抑制"><a href="#NMS-Non-Maximum-Suppression-非极大值抑制" class="headerlink" title="NMS(Non-Maximum Suppression)非极大值抑制"></a><a href="https://blog.csdn.net/shuzfan/article/details/50371990" target="_blank" rel="noopener">NMS</a>(Non-Maximum Suppression)非极大值抑制</h2><p>首先选定一个 IoU（Intersection-over-Union，交并比，即两框重叠部分面积占两框并集面积的比例，<strong>用于衡量两个 bounding box 重叠度的量</strong>）阈值，例如 0.25，然后将所有 4 个窗口（bounding box）按照得分由高到低排序，然后选中得分最高的窗口，遍历计算剩余的3个窗口与该窗口的交并比（IoU），如果 IoU 大于阈值 0.25，则将窗口删除；然后再从剩余的窗口中选中一个得分最高的。重复上述过程，直至所有窗口都被处理，从而得到所有的检测框。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">py_cpu_nms</span><span class="params">(dets, thresh)</span>:</span></div><div class="line">    x1, y1, x2, y2, scores = dets[:, <span class="number">0</span>], dets[:, <span class="number">1</span>], dets[:, <span class="number">2</span>], dets[:, <span class="number">3</span>], dets[:, <span class="number">4</span>]</div><div class="line">    areas = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</div><div class="line">    order = scores.argsort()[::<span class="number">-1</span>]</div><div class="line"></div><div class="line">    keep = []</div><div class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>:</div><div class="line">        i = order[<span class="number">0</span>]</div><div class="line">        keep.append(i)</div><div class="line">        order = order[<span class="number">1</span>:]</div><div class="line">        xx1 = np.maximum(x1[i], x1[order])</div><div class="line">        yy1 = np.maximum(y1[i], y1[order])</div><div class="line">        xx2 = np.minimum(x2[i], x2[order])</div><div class="line">        yy2 = np.minimum(y2[i], y2[order])</div><div class="line"></div><div class="line">        w = np.maximum(<span class="number">0.0</span>, xx2 - xx1 + <span class="number">1</span>)</div><div class="line">        h = np.maximum(<span class="number">0.0</span>, yy2 - yy1 + <span class="number">1</span>)</div><div class="line">        inter = w * h</div><div class="line">        ovr = inter / (areas[i] + areas[order] - inter)</div><div class="line">        inds = np.where(ovr &lt;= thresh)[<span class="number">0</span>]</div><div class="line">        order = order[inds]</div><div class="line"></div><div class="line">    <span class="keyword">return</span> keep</div></pre></td></tr></table></figure><h2 id="ROIs-Pooling（Region-of-Interest-Pooling）"><a href="#ROIs-Pooling（Region-of-Interest-Pooling）" class="headerlink" title="ROIs Pooling（Region of Interest Pooling）"></a><a href="https://blog.csdn.net/zj360202/article/details/78845601" target="_blank" rel="noopener">ROIs Pooling</a>（Region of Interest Pooling）</h2><ol><li>将候选框映射至 feature map；</li><li>做 ROIs Pooling，即对每个 ROI 做同等份数的分割，对每块分割做 max/avg pooling；或者说用一个单层的SPP layer将映射到的 feature map 下采样为大小固定的 feature。</li></ol><h2 id="bounding-box-regression"><a href="#bounding-box-regression" class="headerlink" title="bounding box regression"></a>bounding box regression</h2><p>因为候选框一般不够精确，故需根据候选框内的物体做精修，类似于人工打 label 的过程。</p><p>假设：当候选框与 GT 相差较小时，候选框内的 feature map 到两个平移量和两个尺度因子的映射是一种线性变换。该假设类似于人看到一本书的70%以上时，便能猜测出该书有多大，但若只看到一个书角便不能，当然也不期望能。</p><p>便于回归的两个trick（属于 target engineering）：</p><ol><li>中心点平移时，用平移量是原proposal 的宽和高的几倍做目标，保证了在候选框内有用的 feature map 相同时，候选框的大小不怎么影响回归量的值；</li><li>尺度放缩时，用尺度缩放因子取对数做目标，从而使得误差更符合高斯分布。</li></ol><p><img src="https://img-blog.csdn.net/20170614165625267" alt="bbox_delta"></p><h2 id="others"><a href="#others" class="headerlink" title="others"></a>others</h2><h3 id="selective-search"><a href="#selective-search" class="headerlink" title="selective search"></a>selective search</h3><p>首先通过过分割的方法将图片分割成无数个小区域，然后依据相似度（颜色、纹理、尺度、交叠度等）合并可能性最高的两个区域，重复合并，直至所有区域合并成为一个区域为止，    最后输出所有曾出现过的区域作为候选区域（通常约为 2000 个）。如下图所示：</p><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/dl/selective-search.png" alt="selective-search"></p><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a><a href="https://blog.csdn.net/SusanZhang1231/article/details/73249978" target="_blank" rel="noopener">迁移学习</a></h3><p>迁移学习指有监督预训练(Supervised pre-training)，即把一个任务训练好的参数，拿到另外一个任务作为初始化参数值。将一个任务得到的特征迁移到另一个任务中，故称为迁移。</p><h1 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a><a href="https://blog.csdn.net/sunpeng19960715/article/details/54891652" target="_blank" rel="noopener">发展历程</a></h1><blockquote><p>两个目标：</p></blockquote><h2 id="传统算法"><a href="#传统算法" class="headerlink" title="传统算法"></a>传统算法</h2><p>滑动窗口 + 缩放图片后再滑动窗口（等价于取的不同大小的滑动窗口）+ 分别计算是 object 的可能性，并对框内物体进行分类。</p><h2 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h2><p>selective search 提取候选框 + 每个候选框 wrap 成统一尺寸送入后 <strong>CNN 提取特征</strong> + SVM + bounding-box-regression</p><p>简单说便是：RCNN = regions + CNNs + pooling + SVM // LR</p><p>创新：首次将卷积神经网络用在目标检测上。</p><h2 id="SPP-net（spatial-pyramid-pooling）"><a href="#SPP-net（spatial-pyramid-pooling）" class="headerlink" title="SPP-net（spatial pyramid pooling）"></a>SPP-net（spatial pyramid pooling）</h2><p>所谓空间金字塔池化是与图片金字塔或特征金字塔以相同的理念出发的，即对于最终的 feature map 采用不同大小的池化框，相当于多尺度的 pooling。</p><p>SPP-net = CNN // regions + SPP + SVM // LR</p><p>创新：</p><ol><li>通过 SPP 解决了候选框尺寸不一致的问题;</li><li>通过对所有候选框共享 CNN 提取的特征，大大减少了运算量。</li></ol><h2 id="fast-RCNN"><a href="#fast-RCNN" class="headerlink" title="fast-RCNN"></a>fast-RCNN</h2><p>fast-rcnn = CNN // regions + RoI-pooling + nn(multi-task)</p><p>创新：</p><ol><li>分类器和回归器用神经网络实现，故在训练时无需用大量的硬盘空间来存储 RCNN 中独立的 SVM 和回归器所需的作为训练样本的大量特征；</li><li>除了候选框外将整个框架构建成为了一个神经网络，使得从特征提取器到最终的分类回归器都是一起被优化的，实现了端到端的训练，使得速度和精确度都上了一个台阶。</li></ol><h2 id="faster-RCNN"><a href="#faster-RCNN" class="headerlink" title="faster-RCNN"></a>faster-RCNN</h2><p>尝试将耗时的 selective search 用神经网络实现，创新性地提出了 RPN，使得区域生成网络和 fast-rcnn 能够共享特征提取网络，速度和精确度再上一个台阶。</p><p>faster-RCNN = CNN // RPN + RoI-pooling + nn(multi-task)</p><blockquote><p><a href="">faster-RCNN_tf</a></p></blockquote><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/dl/faster-rcnn.jpg" alt="faster-rcnn网络结构"></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>$$<br>rpn_cls_loss = -\sum_i (p_i^<em> \log p_i + (1-p_i^</em>) \log (1 - p_i))<br>\<br>rpn_bbox_loss = - \sum_i p_i^<em> smooth_L_1(t_i, t_i^</em>)<br>\<br>cls_loss = - \sum<em>i \log p</em>{I_i}<br>\<br>bbox_loss = - \sum_i smooth_L_1(t_i, t_i^*)<br>$$</p><h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><h4 id="anchors"><a href="#anchors" class="headerlink" title="anchors"></a>anchors</h4><h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><p><em>trick</em>：利用 (x, y, w, h) 的形式使得 (x1, y1, x2, y2) 可批量生成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_scale_enum</span><span class="params">(anchor, scales)</span>:</span></div><div class="line">    w, h, x_ctr, y_ctr = _whctrs(anchor)</div><div class="line">    ws = w * scales</div><div class="line">    hs = h * scales</div><div class="line">    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)</div><div class="line">    <span class="keyword">return</span> anchors</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_anchors</span><span class="params">(base_size=<span class="number">16</span>, ratios=[<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">2</span>],</span></span></div><div class="line">                     scales=<span class="number">2</span>**np.arange<span class="params">(<span class="number">3</span>, <span class="number">6</span>)</span>):</div><div class="line">    base_anchor = np.array([<span class="number">1</span>, <span class="number">1</span>, base_size, base_size]) - <span class="number">1</span></div><div class="line">    w, h, x_ctr, y_ctr = _whctrs(base_anchor)</div><div class="line">    size = w * h  </div><div class="line">    ws = np.round(np.sqrt(size / ratios))</div><div class="line">    hs = np.round(np.sqrt(size * ratios)) </div><div class="line">    <span class="comment"># ws/hs = [1/sqrt(ratios)] / sqrt(ratios) = 1/ratios</span></div><div class="line">    <span class="comment"># ws, hs = array([23., 16., 11.]), array([12., 16., 22.])</span></div><div class="line">    ratio_anchors = _mkanchors(ws, hs, x_ctr, y_ctr)</div><div class="line">    anchors = np.vstack([_scale_enum(ratio_anchors[i, :],</div><div class="line">                                     scales)</div><div class="line">                         <span class="keyword">for</span> i <span class="keyword">in</span> range(ratio_anchors.shape[<span class="number">0</span>])</div><div class="line">                        ])</div><div class="line">    <span class="comment">#array([[ -83.,  -39.,  100.,   56.],</span></div><div class="line">    <span class="comment">#       [-175.,  -87.,  192.,  104.],</span></div><div class="line">    <span class="comment">#       [-359., -183.,  376.,  200.],</span></div><div class="line">    <span class="comment">#       [ -55.,  -55.,   72.,   72.],</span></div><div class="line">    <span class="comment">#       [-119., -119.,  136.,  136.],</span></div><div class="line">    <span class="comment">#       [-247., -247.,  264.,  264.],</span></div><div class="line">    <span class="comment">#       [ -35.,  -79.,   52.,   96.],</span></div><div class="line">    <span class="comment">#       [ -79., -167.,   96.,  184.],</span></div><div class="line">    <span class="comment">#       [-167., -343.,  184.,  360.]])</span></div><div class="line">    <span class="keyword">return</span> anchors</div></pre></td></tr></table></figure><h4 id="anchors-–-gt-proposals"><a href="#anchors-–-gt-proposals" class="headerlink" title="anchors –&gt; proposals"></a>anchors –&gt; proposals</h4><p>a.将 anchors 与 bbox_deltas 合并（平移+放缩）得到最初的 region proposal，并将候选框小的或跨越图像边界的删掉。</p><p>b. 在训练时对每个标定的真值候选区域，与其重叠比例最大的region proposal记为前景样本（保证了每个 GT 都至少有一个候选框），剩余的region proposal，如果其与某个标定重叠比例大于0.7，记为前景样本，如果其与任意一个标定的重叠比例都小于0.3，记为背景样本（保证了候选框的合理性），其余的region proposal，弃去不用；对a)剩余的region proposal，根据每个框的 objectness 得分做排序删减。</p><p>c. 对b)剩余的region proposal 做 nms，保留前一定数量的 proposals。</p><p>简而言之，当第 i 个 anchor 与 GT 间 IoU&gt;0.7，则认为该 anchor 是 foreground，标记 label 为1；反之 IoU&lt;0.3 时，认为是该 anchor 是 background，标记 label 为0；至于那些 0.3&lt;IoU&lt;0.7 的 anchor 则不参与训练。</p><h3 id="训练网络的细节"><a href="#训练网络的细节" class="headerlink" title="训练网络的细节"></a>训练网络的细节</h3><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><a href="https://blog.csdn.net/eliudragon/article/details/78189491" target="_blank" rel="noopener">ref1: CNN目标检测与分割（一）：Faster RCNN详解</a></p><h1 id="【附】VOC2007"><a href="#【附】VOC2007" class="headerlink" title="【附】VOC2007"></a>【附】VOC2007</h1><p><a href="https://blog.csdn.net/gaohuazhao/article/details/60871886" target="_blank" rel="noopener">ref2: VOC2007数据集制作</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;预备知识&quot;&gt;&lt;a href=&quot;#预备知识&quot; class=&quot;headerlink&quot; title=&quot;预备知识&quot;&gt;&lt;/a&gt;预备知识&lt;/h1&gt;&lt;h2 id=&quot;FCN-Fully-Convolutional-Networks-全卷积神经网络&quot;&gt;&lt;a href=&quot;#FCN-Fully-Convolutional-Networks-全卷积神经网络&quot; class=&quot;headerlink&quot; title=&quot;FCN(Fully Convolutional Networks)全卷积神经网络&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://blog.csdn.net/app_12062011/article/details/60956357&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FCN&lt;/a&gt;(Fully Convolutional Networks)全卷积神经网络&lt;/h2&gt;&lt;p&gt;顾名思义，FCN 即全为卷积层的神经网络，没有全连接层，或者说将全连接层以卷积层的方式实现，而非以两个矩阵乘积的方式实现（全连接层的实现方式），具体为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于上一层是卷积层的全连接层，该层以与输入的 feature map 大小相同的卷积核做卷积运算实现；&lt;/li&gt;
&lt;li&gt;对于上一层是全连接层的全连接层，该层以输入的神经元个数为 1*1 卷积核通道数做卷积运算实现，或者说是将神经元看做了一个 feature map，或者说是将一个 feature map 看做了一个神经元的激活值（激活值不一定是个标量）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样做的优点是，通过在最后一层做 pooling（核大小为 feature map 大小）的方法，可实现让一个已设计完毕的网络可以接收任意大小的图片。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>自然语言处理——隐马尔可夫模型</title>
    <link href="http://yoursite.com/2018/04/26/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/04/26/自然语言处理——隐马尔可夫模型/</id>
    <published>2018-04-26T13:20:06.000Z</published>
    <updated>2018-06-06T03:44:02.933Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="词性（Part-of-Speech）"><a href="#词性（Part-of-Speech）" class="headerlink" title="词性（Part-of-Speech）"></a>词性（Part-of-Speech）</h3><p>POS 是依据语法功能划分，是词语在区别词类时用到的属性。</p><h3 id="词性标注的方法"><a href="#词性标注的方法" class="headerlink" title="词性标注的方法"></a>词性标注的方法</h3><ol><li><p>rule-based</p><p>语言专家根据词法及语言学知识编制的规则。</p></li><li><p>learning-based</p><p>从专家标注的语料库中学习到用于自动标注的模型</p><ul><li>统计模型：隐马尔可夫模型（HMM），条件随机域模型（CRF），神经网络模型（NN）</li><li>规则学习：基于转换的学习（TBL）</li></ul></li></ol><a id="more"></a> <h3 id="符号规定"><a href="#符号规定" class="headerlink" title="符号规定"></a>符号规定</h3><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">$N$</td><td style="text-align:left">训练数据中的句子总数</td></tr><tr><td style="text-align:center">$O_i$</td><td style="text-align:left">第 i 个句子（词序列）</td></tr><tr><td style="text-align:center">$o_i$</td><td style="text-align:left">某句子中的第 i 个词</td></tr><tr><td style="text-align:center">$Q_i$</td><td style="text-align:left">第 i 个句子对应的词性标注（词性序列）</td></tr><tr><td style="text-align:center">$q_i$</td><td style="text-align:left">某句子中的第 i 个词对应的词性</td></tr></tbody></table><h3 id="基于统计语言模型的词性标注基本模型"><a href="#基于统计语言模型的词性标注基本模型" class="headerlink" title="基于统计语言模型的词性标注基本模型"></a>基于统计语言模型的词性标注基本模型</h3><p>$$<br>\max_Q P(Q|O)<br>$$</p><p>由于语料库不可能包含所有可能出现的句子，故应得到一个更加宽泛的的表达式。利用贝叶斯法则得等价模型<br>$$<br>\max_Q P(O|Q)P(Q)<br>$$</p><h2 id="隐马尔可夫模型（HMM）"><a href="#隐马尔可夫模型（HMM）" class="headerlink" title="隐马尔可夫模型（HMM）"></a>隐马尔可夫模型（HMM）</h2><h3 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h3><ol><li>一阶马尔可夫假设，即语义相关性只涉及到前面 1 个词（也可设为 2 阶或 3 阶）：$P(Q) = P(q_1)P(q_2|q_1)…P(q<em>N|q</em>{N-1})$；</li><li>观测独立性假设，当前时刻的观测值仅与当前时刻的不可观测量的值（状态值）有关，与其他时刻的观测值无关；即单词$o_i$对应的$q_i$不受其他单词影响，即$P(o_i|q_i)$相互独立：$P(O|Q)=\prod P(o_i|q_i)$，故在该模型中，观测独立性假设也可称为条件独立性假设。注意状态之间独立性并不成立，</li></ol><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>$$<br>\max_Q \prod P(o_i|q_i) * \prod P(q<em>j|q</em>{j-1}),<br>$$</p><p>其中$P(o_i|q_i)$被称为发射概率，是通过统计每个单词在语料库中的出现情况得到的。对于因某个单词没有在语料库中出现导致发射概率为 0 进而导致整个句子出现概率为 0 的情况，须做一些平滑处理。</p><h3 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h3><h4 id="由模型定义求解（暴力遍历）"><a href="#由模型定义求解（暴力遍历）" class="headerlink" title="由模型定义求解（暴力遍历）"></a>由模型定义求解（暴力遍历）</h4><p>对于给定的观测序列，求所有可能状态序列的概率，并将最大概率的状态序列最为所求结果。设观测序列长度为 T ，可选状态数为 M，可选观测数为 N，首先在最一开始时由初始状态概率向量 $\pi$求出后续 T-1 个状态概率向量$i_t = \pi A^{t-1}$ ，那么一个可能状态序列的概率为，对一个句子的词性标注的时间复杂度为。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>使用离散时间点、离散状态，并做了马尔可夫假设，由此系统产生了马尔可夫过程的模式，它包含一个$\pi$向量和一个状态转移矩阵。</p><p>隐马尔科夫模型是在一个标准的马尔科夫过程中引入一组观察状态，以及该组观察状态与隐藏状态间的概率关系。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;h3 id=&quot;词性（Part-of-Speech）&quot;&gt;&lt;a href=&quot;#词性（Part-of-Speech）&quot; class=&quot;headerlink&quot; title=&quot;词性（Part-of-Speech）&quot;&gt;&lt;/a&gt;词性（Part-of-Speech）&lt;/h3&gt;&lt;p&gt;POS 是依据语法功能划分，是词语在区别词类时用到的属性。&lt;/p&gt;
&lt;h3 id=&quot;词性标注的方法&quot;&gt;&lt;a href=&quot;#词性标注的方法&quot; class=&quot;headerlink&quot; title=&quot;词性标注的方法&quot;&gt;&lt;/a&gt;词性标注的方法&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;rule-based&lt;/p&gt;
&lt;p&gt;语言专家根据词法及语言学知识编制的规则。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;learning-based&lt;/p&gt;
&lt;p&gt;从专家标注的语料库中学习到用于自动标注的模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统计模型：隐马尔可夫模型（HMM），条件随机域模型（CRF），神经网络模型（NN）&lt;/li&gt;
&lt;li&gt;规则学习：基于转换的学习（TBL）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>自然语言处理——任务</title>
    <link href="http://yoursite.com/2018/04/26/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E4%BB%BB%E5%8A%A1/"/>
    <id>http://yoursite.com/2018/04/26/自然语言处理——任务/</id>
    <published>2018-04-26T13:19:25.000Z</published>
    <updated>2018-06-03T12:59:10.895Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a> <h2 id="词性标注（POS-tagging）"><a href="#词性标注（POS-tagging）" class="headerlink" title="词性标注（POS tagging）"></a>词性标注（POS tagging）</h2><h2 id="文语转换（TTS）"><a href="#文语转换（TTS）" class="headerlink" title="文语转换（TTS）"></a>文语转换（TTS）</h2><h2 id="词形还原（lemmatization）"><a href="#词形还原（lemmatization）" class="headerlink" title="词形还原（lemmatization）"></a>词形还原（lemmatization）</h2><h2 id="名词块检测（NP-chunk-detection-noun-phrase-chunking-detection）"><a href="#名词块检测（NP-chunk-detection-noun-phrase-chunking-detection）" class="headerlink" title="名词块检测（NP-chunk detection, noun phrase chunking detection）"></a>名词块检测（NP-chunk detection, noun phrase chunking detection）</h2><h2 id="词义消歧（word-sense-disambiguation）"><a href="#词义消歧（word-sense-disambiguation）" class="headerlink" title="词义消歧（word sense disambiguation）"></a>词义消歧（word sense disambiguation）</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt; 
&lt;h2 id=&quot;词性标注（POS-tagging）&quot;&gt;&lt;a href=&quot;#词性标注（POS-tagging）&quot; class=&quot;headerlink&quot; title=&quot;词性标注（POS tagging）&quot;&gt;&lt;/a&gt;词性标注（POS tagging
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——模型融合</title>
    <link href="http://yoursite.com/2018/04/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/"/>
    <id>http://yoursite.com/2018/04/25/机器学习——模型融合/</id>
    <published>2018-04-25T00:04:49.000Z</published>
    <updated>2018-06-03T12:58:19.914Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a> ]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt; 
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>自然语言处理基础</title>
    <link href="http://yoursite.com/2018/04/19/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/04/19/自然语言处理——基础/</id>
    <published>2018-04-19T01:50:59.000Z</published>
    <updated>2018-06-03T12:59:46.472Z</updated>
    
    <content type="html"><![CDATA[<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><ul><li>词典（词表）：目标语言的所有单词，记为$\mathcal{D}$。</li><li>语料库（Corpus）：即手中的数据集，记为$\mathcal{C}$。</li><li>上下文（context）：词汇所处的共现词、语境、前邻词、后邻词等词汇出现的篇章语境信息。</li><li>词符（tocken）：目标语言中一个词的标记，即指一个单词。</li><li>词串：一系列词符前后连接成串。</li><li>词串共现：两个词串在同一个句子中。</li><li>历史词：出现在该词符之前的所有词。</li></ul><a id="more"></a> <h2 id="统计语言模型（statistical-language-model）"><a href="#统计语言模型（statistical-language-model）" class="headerlink" title="统计语言模型（statistical language model）"></a>统计语言模型（statistical language model）</h2><p>统计语言模型是基于大数定律，结合贝叶斯公式，利用语料库来计算一个句子（或词串）的概率的。n 个词串共现的概率为：<br>$$<br>P(W) = P(w_1)P(w_2|w_1)…P(w_n|w_1,w<em>2,…,w</em>{n-1})<br>\<br>where, \quad P(w_i) = \frac{count(w_i)}{count(\mathcal{C})}<br>\<br>P(w_i|w<em>1,…,w</em>{i-1}) = \frac{count(w<em>1,…,w</em>{i-1},w_i)}{count(w<em>1,…,w</em>{i-1})}<br>$$<br>求解该模型的方法有很多，n-gram 模型、决策树、最大熵模型、最大熵马尔科夫模型、条件随机场、神经网络等。</p><p>当所有的概率值都计算好之后便存储起来，下次需要计算一个词串的概率时，只需找到相关的概率参数，将之连乘即可。</p><h3 id="n-gram"><a href="#n-gram" class="headerlink" title="n-gram"></a>n-gram</h3><p>添加 n-1 阶马尔科夫假设，得到 n-gram 模型（假设为 3-gram）：<br>$$<br>P(w_i|w<em>1,…,w</em>{i-1}) = \frac{count(w<em>{i-2},w</em>{i-1},w<em>i)}{count(w</em>{i-2},w_{i-1})}<br>$$</p><p>当 n 越大：</p><ol><li>模型对历史词的关联性越强，故可区别性越好（模型复杂度越高）；</li><li>因模型复杂度呈指数级增高，大数定理的可靠性便越来越差（可理解为一种过拟合现象，因为模型复杂度相对于样本复杂度过高）。</li></ol><h4 id="数据稀疏问题"><a href="#数据稀疏问题" class="headerlink" title="数据稀疏问题"></a>数据稀疏问题</h4><blockquote><p>产生该问题的根本原因是采用了统计语言模型。</p></blockquote><ol><li>语料库中可能出现$count(w<em>1,…,w</em>{i-1},w_i) = 0$的情况，即该词串永远不会出现，但不应认为$P(w_i|w<em>1,…,w</em>{i-1})=0$；</li><li>语料库中可能出现$count(w<em>1,…,w</em>{i-1}) = count(w<em>1,…,w</em>{i-1},w_i)$的情况，但不应认为$P(w_i|w<em>1,…,w</em>{i-1})=$1。</li></ol><p>这两种问题称为数据稀疏问题，该问题的出现与语料库的大小无关，由<a href="https://www.gelbukh.com/CV/Publications/2001/CICLing-2001-Zipf.htm" target="_blank" rel="noopener">Zipf定律</a>知，在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比，故增大语料库依然无法解决数据稀疏问题。</p><p>在 n-gram 模型中，当 n 达到一定值后，即使样本复杂度足够，由于数据稀疏问题，n 越大，性能反而越差。</p><h4 id="平滑技术"><a href="#平滑技术" class="headerlink" title="平滑技术"></a>平滑技术</h4><p>平滑技术是针对数据稀疏问题引入的技术，常用的有：</p><ul><li>平滑：拉普拉斯平滑（加一平滑）、Lidstone 平滑（加 delta 平滑）、good-turing 平滑。</li><li>回退：backoff、interpolation（软回退）、kneser-ney smoothing。</li></ul><h3 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h3><p>Bengio et. al（2003）</p><h2 id="词符表示（tocken）"><a href="#词符表示（tocken）" class="headerlink" title="词符表示（tocken）"></a>词符表示（tocken）</h2><h3 id="discrete-representation"><a href="#discrete-representation" class="headerlink" title="discrete representation"></a>discrete representation</h3><p>独热（one-hot representation）：可认为是一种基于符号的词表示方法。</p><h3 id="distributed-representation"><a href="#distributed-representation" class="headerlink" title="distributed representation"></a>distributed representation</h3><blockquote><p>基于分布式相似度的表示，英文全称为 distributional similarity based representations。</p><p>You shall know a word by the company it keeps.    – J.R.Firth 1957:11</p></blockquote><h4 id="共现矩阵（cooccurence-matrix）"><a href="#共现矩阵（cooccurence-matrix）" class="headerlink" title="共现矩阵（cooccurence matrix）"></a>共现矩阵（cooccurence matrix）</h4><p>用上下文来表示一个单词的方法。有两种计算方法：</p><ul><li>基于整个段落的，又称为潜在语义分析。</li><li>基于窗口的（窗口内一般为5~10个单词，共现矩阵的行是窗口个数，列为所有不重复单词个数），优点是将语义和语法都考虑了进去。</li></ul><p>缺点是维度灾难，常用 SVD 来压缩矩阵以实现降维。</p><h4 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h4>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;术语&quot;&gt;&lt;a href=&quot;#术语&quot; class=&quot;headerlink&quot; title=&quot;术语&quot;&gt;&lt;/a&gt;术语&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;词典（词表）：目标语言的所有单词，记为$\mathcal{D}$。&lt;/li&gt;
&lt;li&gt;语料库（Corpus）：即手中的数据集，记为$\mathcal{C}$。&lt;/li&gt;
&lt;li&gt;上下文（context）：词汇所处的共现词、语境、前邻词、后邻词等词汇出现的篇章语境信息。&lt;/li&gt;
&lt;li&gt;词符（tocken）：目标语言中一个词的标记，即指一个单词。&lt;/li&gt;
&lt;li&gt;词串：一系列词符前后连接成串。&lt;/li&gt;
&lt;li&gt;词串共现：两个词串在同一个句子中。&lt;/li&gt;
&lt;li&gt;历史词：出现在该词符之前的所有词。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——卷积神经网络基础</title>
    <link href="http://yoursite.com/2018/04/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/04/15/机器学习——卷积神经网络基础/</id>
    <published>2018-04-15T08:57:43.000Z</published>
    <updated>2018-06-14T14:41:24.828Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>卷积神经网络适用场景：输入信号在空间上和时间上存在一定关联性的场景。</p></blockquote><p><img src="http://img.mp.itc.cn/upload/20170301/dfccbb9271cc4ecfb2374e9d4e344348.png" alt="history"></p><p>左侧为基础网络结构的调整，右侧为基础网络深度的增加。</p><a id="more"></a> <h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><h3 id="卷积（convolution）"><a href="#卷积（convolution）" class="headerlink" title="卷积（convolution）"></a>卷积（convolution）</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>$$<br>(f*g)(n) = \int_{-\infin}^{+\infin}f(x)g(n-x) \mathrm{d} x<br>$$</p><p>两个在某种角度上具有一定关联性的函数 f 与 g，在二者自变量之和为一常数 n 的约束（更一般化地，在二者自变量的线性组合为一常数的约束）下，两函数之积在某个区间上的积分称为这两个函数在该区间上的卷积。</p><p>总之，两函数的卷积是两函数之积在某种线性组合（如 x+y=n）的约束下的特殊积分。</p><h4 id="命名"><a href="#命名" class="headerlink" title="命名"></a><a href="https://www.zhihu.com/question/54677157/answer/141245297" target="_blank" rel="noopener">命名</a></h4><p>之所以称为卷积，是因为该运算的结果是两函数的张量积构成的平面（或超平面）沿两函数各自自变量线性组合等式约束的轨迹做卷褶得到的降维的线（或超平面）上的某一点（线性组合等式约束决定的点）的值。</p><p>简而言之就是<strong>将张量积卷褶后的重合点之和即为卷积</strong>。</p><h4 id="理解角度"><a href="#理解角度" class="headerlink" title="理解角度"></a><a href="https://blog.csdn.net/tiger_v/article/details/79675359" target="_blank" rel="noopener">理解角度</a></h4><ol><li><p>从对单通道二维图像做卷积的角度理解：</p><p>f 为图像像素值对位置的函数，g 为实现某种功能的滤波器（又称为卷积核、模板），其具体操作为对两矩阵的哈达玛积的所有元素求和。</p><ul><li>能实现这种这种功能的原因是由卷积结果的形式决定的：卷积结果的形式可表示为$f*g = g(f) = af_{0}+bf_2+…$，对邻域像素的线性组合即为<a href="https://blog.csdn.net/purgle/article/details/73728940" target="_blank" rel="noopener">线性滤波器</a>。</li><li>而为了实现滤波器这种功能，所以采用卷积的形式来操作：在单通道二维图像上实现对邻域像素的线性组合抽象为数学表示便是，两向量在索引服从一定关系的约束下在某个索引区间内的内积，即卷积。</li></ul></li><li><p>要想实现更加复杂的非线性滤波器，需要用一大堆的简易非线性滤波器，并通过很多层的组合得到。NIN（network in network）提出了一种“偷吃步”的方法（MLPconv）来降低计算复杂度和模型复杂度。可从两个角度来理解 MLPconv：</p><p>（假设第一层的卷积核与输入图像相同大小）</p><ul><li><p>从结构形式的角度：$k_1$ 个卷积核输出到 $k_1$ 个神经元上，再假设第二层的卷积核与第一层神经元数大小相同，$k_2$ 个卷积核输出到 $k_2$ 个神经元上，即对$k_1$种线性滤波的非线性激活结果再次进行$k_2$种线性滤波，依此继续连接；能够由此实现任意一种非线性滤波器的原因与传统神经网络能够模拟任意函数的原理一样。</p><p>故 <strong>MLPconv 本质为一个传统的神经网络（理解本质时用）</strong>，即将一个传统的神经网络作为卷积核，神经网络隐藏神经元数由卷积核数决定，即希望得到的非线性滤波器个数。</p><p>由此，可将线性滤波器看作 MLPconv 的特殊情况，即一个感知机。</p></li><li><p>从物理含义的角度：$k_1$ 个卷积核输出$k_1$个特征图（此处每个图仅为一个点），而第二层的每个感知机是对$k_1$个通道的特征图的同一个位置作线性组合操作，然后做非线性激活，该操作可理解为对$k_1$个通道的特征图的同一个位置做 1×1 大小卷积核的卷积，或者理解为对$k_1$级联的特征图做 1×1 大小卷积核的池化（理解为池化，是目的导向的，因为此操作的目的是对不同特征进行不同方向的聚合；其与池化不同，因为参数都是要学习的，而非固定的）；能够由此实现任意一种非线性滤波器的原因是不同特征的不同方向的多次聚合能够得到一种任意一种非线性特征。</p><p>故 <strong>MLPconv 等价于一个 k×k 卷积层后缀数个 1×1 卷积层（思考网络结构及实际编码时用）</strong>。</p><p>其中 1×1 卷积层也有人称之为跨通道参数的级联池化（cccp），实现跨通道的信息整合。</p><p>1×1 卷积层的意义主要在于线性组合，并作非线性激活；想当于做了一次非线性组合。</p></li></ul><p>简单来说，<strong>非 NIN 结构的多层卷积是跨越了不同尺寸的感受野，在相同范围内的感受野只有一次简易非线性滤波；而 NIN 结构的多层卷积是作用在同一尺寸上的感受野，可认为在相同范围内的感受野由一次复杂的非线性滤波，能够提取更强的非线性特征。</strong></p></li><li><p>针对不同尺度特征自然需要不同大小的卷积核，考虑到不同尺度特征可能属于同一级别的抽象，故提出了 inception 结构。该结构本质是对几个不同大小卷积核的 MLPconv 的结果的 <em>concat</em>。该结构的合理性是基于以下两点原因的：</p><ul><li>MLPconv 本质为一个小型神经网络，1×1 卷积层与 k×k 卷积层的先后顺序影响不大，又考虑到通道数庞大的可能性，先做 1×1 卷积可以极大地减少参数数量，提高泛化能力。</li><li>不同尺度特征自然需要不同大小的卷积核，而不同尺度特征可能属于同一级别的抽象。</li></ul><p>inception-v1 结构如下图所示（从左至右，为提取尺度越来越大的特征，最左侧的特征可认为是在该抽象程度上尺度为零的特征）：</p></li></ol><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/inception_v1.jpg" alt="inception_v1"></p><p>​    对 inception module 如此设计的理解：</p><ul><li>结合 NIN 结构和 1 * 1 卷积可升降维减少参数，实现在相同尺寸的感受野中提取到更加非线性的特征；<ul><li>考虑到 GPU 只对密集矩阵计算作了优化，没有对稀疏矩阵计算做优化，又考虑到稀疏矩阵可分解成密集矩阵计算的原理，通过在多个尺寸上同时进行卷积再聚合的方式，实现网络的稀疏结构。比如当要输出一个 256 通道的 feature map 时，若只用一种卷积核进行卷积，256 个输出特征便均是在同一尺度范围上的，极有可能是一个稀疏分布的特征集；而 inception 结构会在多个尺度上提取特征（如 1 <em> 1 的提取 96 个特征，3 </em> 3 的提取 96 个特征，5 × 5 的提取 64 个特征），这样相关性更强的低级特征便会被组合称为高级特征，而<strong>不相关的低级特征（非关键特征）便会被弱化而不使用</strong>，同样输出 256 个特征，inception 结构输出的特征“冗余”信息会更少，从而间接地提高了网络的稀疏性。（因为网络越稀疏收敛速度越快，且越不易过拟合，故都追求更加稀疏的网络）</li></ul></li></ul><ol><li>VGG-net 中的 trick：用多个 3×3 卷积核代替更大面积的卷积核，这不仅减少了参数，减轻过拟合，而且多次卷积代替一次卷积的方案更增加了非线性，有利于特征的提取。后来在 inception-v3 中发现，非对称的卷积结构拆分比用多个 3×3 卷积进行拆分的效果更明显，参数也更少。这被称为 Factorization in Small Convolutions 思想。</li><li>ResNet 中的 trick：引入shortcut，<ul><li>解决了网络退化问题；</li><li>使得信息更容易在各层间流动，实现了前向的特征重用；</li><li>使得深层网络成为了很多浅层网络的集成，比如一个残差块只有两条通路的 resnet，若有 4 个残差块，那便是 $2^4$ 个浅层网络的集成。</li></ul></li></ol><h4 id="在神经网络上的实现方法及理由（任务依赖的正则化）"><a href="#在神经网络上的实现方法及理由（任务依赖的正则化）" class="headerlink" title="在神经网络上的实现方法及理由（任务依赖的正则化）"></a>在神经网络上的实现方法及理由（任务依赖的正则化）</h4><ol><li>局部感知：每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息；</li><li>参数共享：图像具有一种“静态性”的属性：图像的一部分的统计特性与某些其他部分是一样的，即平移不变性的特征。由此提出了卷积核和特征图的概念。</li></ol><h3 id="池化（polling）"><a href="#池化（polling）" class="headerlink" title="池化（polling）"></a>池化（polling）</h3><p>池化过程本质为特征突出过程，去除特征图中的无用像素点。（查看西瓜书的样本不均衡问题）</p><h2 id="对深度模型的一些优化"><a href="#对深度模型的一些优化" class="headerlink" title="对深度模型的一些优化"></a>对深度模型的一些优化</h2><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/issues_deeplearning.jpg" alt="issues_deeplearning"></p><h3 id="ReLu"><a href="#ReLu" class="headerlink" title="ReLu"></a>ReLu</h3><p><a href="https://www.zhihu.com/question/59031444/answer/177786603" target="_blank" rel="noopener">https://www.zhihu.com/question/59031444/answer/177786603</a></p><h3 id="LRN"><a href="#LRN" class="headerlink" title="LRN"></a><a href="https://blog.csdn.net/hduxiejun/article/details/70570086" target="_blank" rel="noopener">LRN</a></h3><p>LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p><p>很少再用，现多用 dropout。</p><h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><blockquote><ol><li>一种更恶化环境训练、更舒服环境战斗的思想。</li><li>Hinton 认为 dropout 是通过特殊的训练策略实现的隐式的模型集成。</li></ol></blockquote><ol><li>达到了一种Vote的作用。对于全连接神经网络而言，我们用相同的数据去训练5个不同的神经网络可能会得到多个不同的结果，我们可以通过一种vote机制来决定多票者胜出，因此相对而言提升了网络的精度与鲁棒性。同理，对于单个神经网络而言，如果我们将其进行分批，虽然不同的网络可能会产生不同程度的过拟合，但是将其公用一个损失函数，相当于对其同时进行了优化，取了平均，因此可以较为有效地防止过拟合的发生。</li><li>减少神经元之间复杂的共适应性。当隐藏层神经元被随机删除之后，使得全连接网络具有了一定的稀疏化，从而有效地减轻了不同特征的协同效应。也就是说，有些特征可能会依赖于固定关系的隐含节点的共同作用，而通过Dropout的话，就有效地组织了某些特征在其他特征存在下才有效果的情况，增加了神经网络的鲁棒性。</li></ol><p>简而言之：Dropout在实践中能很好工作是因为其在训练阶段阻止了神经元的共适应。</p><h3 id="应对梯度爆炸-弥散"><a href="#应对梯度爆炸-弥散" class="headerlink" title="应对梯度爆炸/弥散"></a>应对梯度爆炸/弥散</h3><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><blockquote><p>ResNet 是一种学习残差的框架，即用非线性层去显式地拟合一个残差映射，而不再是去拟合一个期望的潜在映射。</p><p>解决了随着网络层数不断加深，求解器不能找到解决途径的问题。</p><p><a href="https://github.com/bkseastone/Amadeus/tree/master/my_toolbox/resnet" target="_blank" rel="noopener">code: resnet_tf</a></p></blockquote><p>深度神经网络在 ReLU 和 BN 层的加入后，网络变深不再有梯度弥散的问题，<br>但却会出现随着网络的加深，准确度反而下降的现象，即退化现象。</p><p>ResNet 使得下述所需的函数变得更易拟合求解得到：通过几个非线性映射的堆叠去拟合一个恒等映射，由此使得对某个低级特征贡献不大的输入可以一下子被拉到更深的层，其对更高级特征的贡献由更深层的权重学习到。至于输入对哪种抽象特征的贡献度更大，在求解最优化的过程中可自动学习到。</p><p>从集成学习的观点，可以将更深的网络看做是在增加该指数的幂次（ ResNet ），将更宽的网络看做是在增加该指数的基底（ ResNeXt ）。</p><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/resnet_pros.jpg" alt="resnet_pros"></p><h2 id="实现方面的细节"><a href="#实现方面的细节" class="headerlink" title="实现方面的细节"></a>实现方面的细节</h2><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><h4 id="卷积层的梯度反传"><a href="#卷积层的梯度反传" class="headerlink" title="卷积层的梯度反传"></a>卷积层的梯度反传</h4><p>卷积神经网络中的梯度反传是 2D，与传统神经网络中的 1D 反传不同，但其数学本质相同（见<a href="http://redmud.xyz/2018/03/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">梯度反传</a>），只是表现形式上有略微的<a href="https://blog.csdn.net/zy3381/article/details/44409535" target="_blank" rel="noopener">不同</a>：</p><p><img src="https://img-blog.csdn.net/20150318141719205?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenkzMzgx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="conv_delta_backprocess"></p><ol><li>当已得到卷积层误差，求对卷积核参数的梯度时，以卷积层误差为卷积核对与卷积核参数处于同一层的 feature map 做卷积（1D 反传时是误差与激活值直接做内积）得到在卷积核参数处的梯度；</li><li>若要继续反传得到图中卷积层的上一层（图中称为池化层是默认一个卷机后接一个池化）误差，须将卷积核参数旋转 180 度后对卷积层误差做 full 类型（其他类型还有 valid，same ）的卷积，便得到了上一层的误差，进而递归地用第一条。</li></ol><h4 id="池化层的梯度反传"><a href="#池化层的梯度反传" class="headerlink" title="池化层的梯度反传"></a>池化层的梯度反传</h4><p>池化层不需要计算梯度，因为没有要训练的参数，但在梯度反传时，误差的形状需要发生变化以保证和上一层的参数能够对位，处理方式的原则是：保证传递的误差总和不变：</p><ol><li>对于均值池化，便是把误差每个元素等分成n份传递给前一层；</li><li>对于最大值池化，需要先在前传时记住最大值的位置，反传时将误差对应到所记录的位置上，其他位置置零。</li></ol><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>对 CNN 的研究主要有三种方法：</p><ol><li>观察学习到的 filter</li><li>通过对抗样本</li><li>通过破坏网络结构，观察性能变化</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;卷积神经网络适用场景：输入信号在空间上和时间上存在一定关联性的场景。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://img.mp.itc.cn/upload/20170301/dfccbb9271cc4ecfb2374e9d4e344348.png&quot; alt=&quot;history&quot;&gt;&lt;/p&gt;
&lt;p&gt;左侧为基础网络结构的调整，右侧为基础网络深度的增加。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>待办事项</title>
    <link href="http://yoursite.com/2018/04/10/%E5%BE%85%E5%8A%9E%E4%BA%8B%E9%A1%B9/"/>
    <id>http://yoursite.com/2018/04/10/待办事项/</id>
    <published>2018-04-10T13:00:07.000Z</published>
    <updated>2018-06-03T12:56:46.800Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a> <ul><li>[ ] 回归为何能用于分类【机器学习——线性模型】</li><li>[ ] weight-decay 中的$\lambda$与常数 C 的解析关系是什么？【机器学习——线性模型】</li><li>[ ] 为什么 X 存在多重共线性时，最小二乘法求得的 w 在数值上会很大？【机器学习——线性模型】</li><li>[ ] 为什么l1正则化更易导致模型稀疏（即解更易在角上取得）</li><li>[ ] SGD原理</li><li>[ ] ​</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt; 
&lt;ul&gt;
&lt;li&gt;[ ] 回归为何能用于分类【机器学习——线性模型】&lt;/li&gt;
&lt;li&gt;[ ] weight-decay 中的$\lambda$与常数 C 的解析关系是什么？【机器学习——线性模型】&lt;/li&gt;
&lt;li&gt;[ ] 为什么 X 存在
      
    
    </summary>
    
      <category term="board" scheme="http://yoursite.com/categories/board/"/>
    
    
  </entry>
  
  <entry>
    <title>信息检索与知识管理</title>
    <link href="http://yoursite.com/2018/04/02/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E4%B8%8E%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86/"/>
    <id>http://yoursite.com/2018/04/02/信息检索与知识管理/</id>
    <published>2018-04-02T04:01:57.000Z</published>
    <updated>2018-06-03T12:56:20.619Z</updated>
    
    <content type="html"><![CDATA[<h1 id="信息检索"><a href="#信息检索" class="headerlink" title="信息检索"></a>信息检索</h1><h2 id="电子期刊数据库"><a href="#电子期刊数据库" class="headerlink" title="电子期刊数据库"></a>电子期刊数据库</h2><ol><li><a href="http://www.wanfangdata.com.cn/" target="_blank" rel="noopener">北京万方数据股份有限公司网上数据库联机检索系统</a></li><li><a href="http://www.onelib.cn/online.aspx" target="_blank" rel="noopener">文献索取：图书馆学术交流与文献互助联盟</a></li><li><a href="http://www.cqvip.com/" target="_blank" rel="noopener">重庆维普中文科技期刊数据库</a></li><li><a href="http://www.cnki.net/" target="_blank" rel="noopener">知网：中国学术期刊全文数据库</a></li></ol><a id="more"></a> <h2 id="电子图书数据库"><a href="#电子图书数据库" class="headerlink" title="电子图书数据库"></a>电子图书数据库</h2><ol><li><a href="http://book.chaoxing.com/" target="_blank" rel="noopener">全球最大的中文在线图书馆：超星电子图书</a></li><li><a href="http://www.chineseall.com/book.html" target="_blank" rel="noopener">中文在线电子图书</a></li></ol><h2 id="学位论文全文数据库"><a href="#学位论文全文数据库" class="headerlink" title="学位论文全文数据库"></a>学位论文全文数据库</h2><ol><li><a href="http://c.g.wanfangdata.com.cn/Thesis.aspx" target="_blank" rel="noopener">万方“中国学位论文全文数据库“</a></li><li><a href="http://www.cnki.net/" target="_blank" rel="noopener">中国知网“中国博士学位论文全文数据库”；中国知网“中国优秀硕士学位论文全文数据库”</a></li></ol><h2 id="读秀知识库"><a href="#读秀知识库" class="headerlink" title="读秀知识库"></a><a href="http://www.duxiu.com/" target="_blank" rel="noopener">读秀知识库</a></h2><h2 id="百链中英文学术搜索"><a href="#百链中英文学术搜索" class="headerlink" title="百链中英文学术搜索"></a><a href="http://www.blyun.com/" target="_blank" rel="noopener">百链中英文学术搜索</a></h2><h2 id="国家科技图书文献中心"><a href="#国家科技图书文献中心" class="headerlink" title="国家科技图书文献中心"></a><a href="http://www.nstl.gov.cn/NSTL/" target="_blank" rel="noopener">国家科技图书文献中心</a></h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;信息检索&quot;&gt;&lt;a href=&quot;#信息检索&quot; class=&quot;headerlink&quot; title=&quot;信息检索&quot;&gt;&lt;/a&gt;信息检索&lt;/h1&gt;&lt;h2 id=&quot;电子期刊数据库&quot;&gt;&lt;a href=&quot;#电子期刊数据库&quot; class=&quot;headerlink&quot; title=&quot;电子期刊数据库&quot;&gt;&lt;/a&gt;电子期刊数据库&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://www.wanfangdata.com.cn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;北京万方数据股份有限公司网上数据库联机检索系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.onelib.cn/online.aspx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;文献索取：图书馆学术交流与文献互助联盟&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cqvip.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;重庆维普中文科技期刊数据库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnki.net/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;知网：中国学术期刊全文数据库&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="工具的使用" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——广义线性模型</title>
    <link href="http://yoursite.com/2018/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/04/01/机器学习——广义线性模型/</id>
    <published>2018-04-01T08:52:18.000Z</published>
    <updated>2018-06-12T09:41:33.086Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><blockquote><p>所有变量的可表示的数量上的减少、衰退 regress，称为回归，即确认独立变量的过程。比如，独立变量和其他非独立变量之间的关系近似于线性时称为线性回归。</p><p>回归是一种降维方法，减少的维度为非独立变量个数。</p></blockquote><h3 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h3><blockquote><p>输入空间映射到整个实数区间</p></blockquote><p>$$<br>\min_w ||Xw-y||<br>$$</p><a id="more"></a> <h3 id="最小二乘-VS-正交投影"><a href="#最小二乘-VS-正交投影" class="headerlink" title="最小二乘 VS 正交投影"></a>最小二乘 VS 正交投影</h3><p>最小二乘法与正交投影变换是对同一个问题两种不同等价形式的解决方法，且为相同的结果：</p><p>同一个问题：对一堆点求其回归方程，即<br>$$<br> \min_w ||Xw - y ||<br>$$<br>该问题的两种等价形式为</p><ol><li>线性方程组$Xw=y$ 的近似解</li><li>最小二乘问题$x^*=\min_w (Xw-y)^T(Xw-y)$</li></ol><p>对于第一个等价形式，X 的列向量的线性组合无法得到 y，那么将 y 正交投影至 X 的列空间，便可得到线性方程组的一种近似解：$Xw=X(X^TX)^{-1}X^T y \Rightarrow w = (X^TX)^{-1}X^T y$ 。</p><p>对于第二个等价形式，可用导数零点解决该无约束最优化问题：$\frac{\partial (Xw-y)^T(Xw-y)}{\partial w}=2X^T(Xw-y)=0 \Rightarrow w=(X^TX)^{-1}X^T y$ 。</p><blockquote><p>更进一步：</p><ol><li>若$X^TX$不是满秩矩阵，即$X$非列满秩，则最小二乘法与正交投影变换都失效了，此时$w$自由度为$n-rank(X^TX)$，常用的一个解是加入一个归纳偏好$\min ||w||_2$：将$X^TX$进行奇异值分解进而求得伪逆，从而得到$w$。</li><li>通常$X^TX$是可逆的，因为样本数$\gg$ 特征维度+1。但实践中常因为数值稳定性用$X^+$代替$(X^TX)^{-1}X^T$。</li><li>如果输入矩阵X中存在线性相关或者近似线性相关的列，那么输入矩阵 X 就会变成或者近似变成奇异矩阵（singular matrix）。这是一种病态矩阵，矩阵中任何一个元素发生一点变动，整个矩阵的行列式的值和逆矩阵都会发生巨大变化。这将导致最小二乘法对观测数据的随机误差极为敏感，进而使得最后的线性模型产生非常大的方差，这个在数学上称为多重共线性（multicollinearity）。在实际数据中，变量之间的多重共线性是一个非常普遍的现象。</li></ol></blockquote><h2 id="对数几率回归（逻辑回归）"><a href="#对数几率回归（逻辑回归）" class="headerlink" title="对数几率回归（逻辑回归）"></a>对数几率回归（逻辑回归）</h2><blockquote><p>几率（odds）：（样本作为正例的可能性 / 反例的可能性，即$\frac{y}{1-y}$）正例的赔率，反映了样本作为正例的相对可能性。</p></blockquote><h3 id="对数几率回归模型"><a href="#对数几率回归模型" class="headerlink" title="对数几率回归模型"></a>对数几率回归模型</h3><blockquote><p>输入空间映射到  [0, 1] 区间</p></blockquote><p>$$<br>\min_w ||Xw- \ln \frac{y}{1-y}||<br>$$</p><p>在形式上仍是线性回归，是对线性回归模型的扩展，即令模型逼近$y$的衍生物，这里的对数几率函数起到了将线性回归模型的预测值与真实标记联系起来的作用，称之为联系函数。当考虑其使用时称之为逻辑回归模型$\frac{1}{1+e^{-Xw}}$。</p><h2 id="局部加权线性回归（LWR）"><a href="#局部加权线性回归（LWR）" class="headerlink" title="局部加权线性回归（LWR）"></a>局部加权线性回归（LWR）</h2><blockquote><p>解决对不规则函数进行回归时容易出现的欠拟合与过拟合问题</p></blockquote><p>其流程是，每次预测时都需要调用所有的样本$X$，结合预测点来拟合回归曲线。</p><p>其原理是，选择与预测点$x^*$相近的点来做线性回归，忽略远处的点对预测的影响。</p><p>其实现方式是加权最小二乘：<br>$$<br>\min<em>w ||\lambda (Xw - y)||<br>\<br>s.t. \quad \lambda</em>{i} = e^{-||x_i - x^*||_2^2}<br>$$</p><p>缺点：对于每一个要预测的点，都要重新依据整个数据集计算一个线性回归模型出来，使得算法代价极高。</p><h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h2><p>待加</p><h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a><a href="https://github.com/RedMudBUPT/gitpage_img/blob/master/ml/LIN/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_%E9%99%88%E5%B8%8C%E5%AD%BA.pdf" target="_blank" rel="noopener">广义线性模型</a></h2><blockquote><p>大多数的概率分布都能表示成指数分布族的形式，如高斯分布，对噪声和不确定性进行建模；伯努利分布，对有两个离散结果的事件建模；多项式分布（Multinomial），对有K个离散结果的事件建模；泊松分布（Poisson），对计数过程进行建模，如网站访问量的计数问题；指数分布（Exponential），对有间隔的证书进行建模，如预测公交车的到站时间的问题；等等。通过进一步的推导，就能得到各自的线性模型，这大大扩展了线性模型可解决问题的范围。</p><p><strong>广义线性模型：用某种指数分布去逼近数据真实分布的广义线性回归。</strong></p></blockquote><p>$$<br>P(y; \eta)=be^{\eta^T T(y) - a}<br>$$</p><p>GLM 的三个假设：</p><ol><li>$p(y | x; w ) \sim be^{\eta^T T(y) - a}$ ：y 基于 x 的条件概率服从指数分布族中以$\eta$为参数的某个分布；</li><li>学习的目标是预测 T(y) 基于 x 的条件期望，因为 T(y) 通常为 y，即目标函数为$E(y∣x; w)$，故<strong>线性模型的本质是让某个指数分布的期望去逼近 y</strong>；</li><li>$\eta$ 和 x 的关联是线性的，即$\eta = w^T x$，从而</li></ol><h3 id="linear-regression"><a href="#linear-regression" class="headerlink" title="linear regression"></a>linear regression</h3><blockquote><p>线性函数是高斯分布的期望 $u$ 在线性回归模型上的表现形式，即在噪声影响下最可能的值。</p><p>因为$u$ 在实数区间取值，故应用于回归。</p></blockquote><p>$$<br>\sigma = z<br>$$</p><p>高斯分布：<br>$$<br>p(y;u) = \frac{1}{\sqrt{2 \pi} \sigma}e^{- \frac{(y-u)^2}{2 \sigma^2}}<br>\<br>= \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2}y^2} e^{uy - \frac{1}{2}u^2}<br>$$<br>将高斯分布与广义线性模型对比得到：<br>$$<br>\eta = u \Rightarrow u = \eta<br>$$<br>从而得到线性回归的数学模型（假设集）。</p><h3 id="sigmod"><a href="#sigmod" class="headerlink" title="sigmod"></a>sigmod</h3><blockquote><p>logistic 函数是伯努利分布的期望 $\phi$ 在线性回归模型上的表现形式，即单次正例事件发生的概率；或说是<strong>伯努利分布体现在线性回归模型上的函数</strong>。</p><p>因$\phi$在 (0, 1) 区间取值，故应用于分类。</p></blockquote><p><img src="https://github.com/RedMudBUPT/gitpage_img/blob/master/ml/lin1.jpg?raw=true" alt="lin1"><br>$$<br>logisticfunction  \quad \sigma = \frac{1}{1+e^{- z}}<br>$$</p><p>伯努利分布：<br>$$<br>p(y; \phi) = \phi^y (1-\phi)^{N-y}<br>\<br>\quad \quad \quad \quad \quad = e^{(\ln \frac{\phi}{1-\phi})y + \ln (1-\phi)^N}<br>$$<br>当只针对一个样本来看时，伯努利分布便降为 0 / 1 分布；当针对整体来看时，y 便是正例发生的次数统计，目标函数为$sum( \phi)$ 。</p><p>将伯努利分布与广义线性模型对比得到：<br>$$<br>\eta = \ln \frac{\phi}{1- \phi} \Rightarrow \phi = \frac{1}{1+e^{- \eta}}<br>$$<br>从而得到逻辑回归的数学模型（假设集）。</p><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><blockquote><p>softmax 函数是多项式分布的期望$\phi_i$在线性回归模型上的表现形式，即某类事件发生的概率。</p><p>因$\phi_i$在 (0, 1) 区间取值，故应用于分类。</p></blockquote><p>$$<br>softmaxfunction  \quad \sigma = \frac{e^{z<em>i}}{\sum</em>{j=i}^{k} e^{z_j}}<br>$$</p><p><img src="https://github.com/RedMudBUPT/gitpage_img/blob/master/ml/lin2_softmax.jpg?raw=true" alt="lin2_softmax_"></p><blockquote><p>上述的伯努利分布与多项式分布均是针对一个样本来说的。</p></blockquote><h3 id="回归用于分类"><a href="#回归用于分类" class="headerlink" title="回归用于分类"></a>回归用于分类</h3><p>其本质仍是回归，只是在最后一步做了处理以用于分类，</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>线性模型的思想是用某种指数分布去逼近数据真实分布，从而实现某种预测；其实现方法是让某个指数分布的期望去逼近 y，然后用最小二乘或最大似然之类的方法构建模型。</p><p>任务的不同类型是依据期望的取值空间划分的。</p><h2 id="Q-次多项式回归"><a href="#Q-次多项式回归" class="headerlink" title="Q 次多项式回归"></a>Q 次多项式回归</h2><blockquote><p>线性不可分数据可能圆形可分，或者其他二次曲线可分，甚至更一般化 Q 次曲线可分，由此引出 Q 次多项式回归的想法。</p></blockquote><h3 id="Q-次多项式特征转换"><a href="#Q-次多项式特征转换" class="headerlink" title="Q 次多项式特征转换"></a>Q 次多项式特征转换</h3><blockquote><p>人工提取特征的过程可认为是某种特征转换，此处提出的是一般化的 Q 次多项式特征转换。</p></blockquote><p>$$<br>z = \phi (x) = (1, x_1, x_2, x_1^2, x_2^2, x_1 x_2)<br>$$</p><p>对于线性不可分的 X 可经过 Q 次多项式特征转换$\phi$（下文简称为特征转换）变为线性可分的 z。该过程可细述为：</p><p>特征转换$\phi$将 d+1 维的 X 空间转换为 $C<em>{d+Q}^Q$维的 Z 空间，其中 $C</em>{d+Q}^Q = O(d^Q)$ ；VC 维也由 d+1 变为了 $C_{d+Q}^Q$，模型复杂度呈指数级增长，记为$\mathcal{H}_1$升级为了$\mathcal{H}_Q$，其关系为$\mathcal{H}_1 \subset \mathcal{H}_2 \subset … \subset  \mathcal{H}_Q$。</p><p>legendre polynomials：正交化的特征转换。避免同数量级的 x 经普通的特征转换后在不同特征下存在好几个数量级的差距，如 $x$与$x^{10}$，从而使得在特征转换后 w 仍能具有同等的影响力。</p><h4 id="特征转换后的模型的-VC-维的推导"><a href="#特征转换后的模型的-VC-维的推导" class="headerlink" title="特征转换后的模型的 VC 维的推导"></a>特征转换后的模型的 VC 维的推导</h4><p>由公式<br>$$<br>C_n^m+C<em>n^{m-1} = C</em>{n+1}^m<br>$$<br>得转换后 x 的 Q 次项的项数为：<br>$$<br>C_d^Q+(Q-1)C_d^{Q-1}+…+C<em>d^1=C</em>{d+Q-1}^Q ,<br>$$<br>故，转换后 x 小于等于 Q 次项的项数为：<br>$$<br>C<em>{d+Q-1}^Q + C</em>{d+Q-2}^{Q-1}+…+C<em>{d+Q-Q}^1+C</em>{d-1}^0<br>\<br>=C<em>{d+Q-1}^Q + C</em>{d+Q-2}^{Q-1}+…+C<em>{d}^1+C</em>{d}^0<br>\<br>=C<em>{d+Q-1}^Q + C</em>{d+Q-2}^{Q-1}+…+C<em>{d+1}^1<br>\<br>=C</em>{d+Q-1}^Q + C<em>{d+Q-1}^{Q-1}<br>\<br>=C</em>{d+Q}^Q<br>$$</p><h2 id="线性软间隔支撑向量机"><a href="#线性软间隔支撑向量机" class="headerlink" title="线性软间隔支撑向量机"></a>线性软间隔支撑向量机</h2><p>详见支撑向量机</p><h1 id="核模型"><a href="#核模型" class="headerlink" title="核模型"></a>核模型</h1><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><h2 id="核脊回归（KRR）"><a href="#核脊回归（KRR）" class="headerlink" title="核脊回归（KRR）"></a>核脊回归（KRR）</h2><p>常用 SVR 替代</p><h2 id="核逻辑回归（KLR）"><a href="#核逻辑回归（KLR）" class="headerlink" title="核逻辑回归（KLR）"></a>核逻辑回归（KLR）</h2><p>常用 probabilistic SVM 替代</p><h1 id="模型的选择"><a href="#模型的选择" class="headerlink" title="模型的选择"></a>模型的选择</h1><h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><blockquote><ul><li>过拟合：指先前的模型$E<em>{in}&lt;E</em>{out}$，而现在升级后的模型$E<em>{in}$更小、$E</em>{out}$更大，称这种现象为过拟合；</li><li>泛化能力差：指该模型$E<em>{in}&lt;&lt;E</em>{out}$。</li></ul></blockquote><h3 id="出现原因（4个）"><a href="#出现原因（4个）" class="headerlink" title="出现原因（4个）"></a>出现原因（4个）</h3><p><strong>模型复杂度过高、数据量有限、随机噪声或确定性噪声过大</strong>。</p><ol><li><p>低复杂度数据下，为什么高复杂度模型会出问题：</p><p>在保证泛化误差的置信度为$1-\delta$ 的前提下（若无此保证，机器学习则无从谈起），由 VC bound 得到泛化误差为：</p></li></ol><p>$$<br>\epsilon = \sqrt{\frac{8}{N} \ln (\frac{4(2N)^{d_{VC}}}{\delta})},<br>$$</p><p>​    故当在低复杂度数据的情况下，复杂度越高的模型会出现更高的泛化误差。</p><ol><li>确定性噪声指隐含模式可能是复杂度很高的模式，而这种高复杂度的模式就像在某种低复杂度模式上加入了随机噪声，从可行性考虑将其归为噪声。称之为确定性是因为在确定了输入数据 X 后，“噪声”便可由隐含模式确定地得到，而不再是随机的。</li><li>直觉上，噪声相当于降低了有效数据的数量，相较于有效数据的数量模型成为高复杂度的模型，进而出现问题，解释见第一条。也有人解释为该模型更有可能将噪声的模式也学习进去。</li></ol><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/ml/overfit_1.png" alt="overfit_1"></p><ol><li>根本原因还是第一条。</li></ol><h3 id="解决方法（4个）"><a href="#解决方法（4个）" class="headerlink" title="解决方法（4个）"></a>解决方法（4个）</h3><ol><li>从简单模型开始；</li><li>对数据进行清理（删掉高噪声数据）或修剪（修正错误标记数据）；</li><li>人工增加数据（可能影响原始数据的真实分布，数据的增加方式应尽可能依照原始数据的隐含模式）；</li><li>正则化。</li></ol><h3 id="附：正则化（regularization"><a href="#附：正则化（regularization" class="headerlink" title="附：正则化（regularization)"></a>附：正则化（regularization)</h3><blockquote><p>其思想是将高复杂度模型进行退化。</p><p>命名由来：对<a href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96/5739561?fr=aladdin&amp;fromid=18081730&amp;fromtitle=regularization" target="_blank" rel="noopener">不适定问题（ill-posed problems）</a>的近似解。</p></blockquote><h4 id="两种解释"><a href="#两种解释" class="headerlink" title="两种解释"></a>两种解释</h4><ol><li><p>从机器学习的角度：</p><p>其思想是将高复杂度模型进行退化，故用稀疏假设集来降低模型的复杂度。比如加入约束条件$\sum bool(w \neq 0) \leqslant 2$记为$\mathcal{H}_2’$，显然$\mathcal{H}_2 \subset \mathcal{H}<em>2’ \subset \mathcal{H}</em>{10}$。不过因为布尔操作，该问题的求解已被证明是 NP-hard 问题。</p><p>为使原 NP-hard 问题易解，将其转化为 soft 版本：约束条件换成$\sum w^2 \leqslant C$，记为$\mathcal{H}(C)$，显然$\mathcal{H}<em>{10}(0)\subset \mathcal{H}</em>{10}(1)\subset … \subset \mathcal{H}<em>{10}( \infin) = \mathcal{H}</em>{10}$。</p><p>进而机器学习模型（将约束条件看做$\mathcal{A}$的一部分，假设集$\mathcal{H}$仍为那个大的假设集$\mathcal{H}_{10}$，能产生多大数量的假设要看数据）变为<br>$$<br>\min<em>w E</em>{in}(w)<br>\<br>s.t. \quad \sum w^2 \leqslant C<br>$$<br>由拉格朗日乘子法及一些粗略的化简得到等价问题：<br>$$<br>\min<em>w E</em>{in}(w) + \lambda ||w||_2^2, \quad \lambda &gt; 0.<br>$$<br>其中常数 C 隐含在 $\lambda$中。</p></li><li><p>从统计的角度：</p><p>X 存在多重共线性时，最小二乘法求得的 w 在数值上会很大，如果 x 有一点小小的变化，输出结果会有很大的差异，即对X 中的噪声非常敏感，所以其解会非常不稳定。而若能限制 w 的增长，模型对噪声的敏感度便会降低，由此引出<a href="https://blog.csdn.net/daunxx/article/details/51578787" target="_blank" rel="noopener">脊回归（Ridge regression）</a>：<br>$$<br>L = \min<em>w ||E</em>{in}(w)||_2^2+ \lambda ||w||_2^2 ,<br>$$<br>即在原损失函数上加入 w 的2-范数的惩罚项。</p><p>对损失函数求导置零得到解：<br>$$<br>w = (X^TX+ \lambda I)^{-1} X^Ty<br>$$</p></li><li><p>从贝叶斯推断的角度：</p><p>正则化是对待推断参数的先验。</p></li></ol><h4 id="正则化方向"><a href="#正则化方向" class="headerlink" title="正则化方向"></a>正则化方向</h4><blockquote><p>损失函数的构造也是从这三个方向出发考虑的。</p></blockquote><ol><li>任务依赖：看任务的具体特性，进行某种特殊的正则化。</li><li>普遍看似合理的方向：使模型更平滑或更简单（因为随机噪声或确定性噪声都是非平滑的），由此得到 sparsity regularizer（稀疏正则化）$||w||_1$。</li><li>更易于求解的方向：由此得到 weight-decay regularizer（权重衰减正则化）$||w||_2^2$。</li></ol><h2 id="验证集作为-mathcal-D-out-的替代来进行模型选择的可行性"><a href="#验证集作为-mathcal-D-out-的替代来进行模型选择的可行性" class="headerlink" title="验证集作为$\mathcal{D}_{out}$的替代来进行模型选择的可行性"></a>验证集作为$\mathcal{D}_{out}$的替代来进行模型选择的可行性</h2><ul><li><p>用$E_{in}$作选择易过拟合，不可靠；</p><p>假设$g_1$、$g_2$是$\mathcal{H}_1$、$\mathcal{H}<em>2$由$E</em>{in}$选出的，若再由$E<em>{in}$选出$g^<em>$，那么你的$g^</em>$对应的模型复杂度便是$d</em>{VC}(\mathcal{H}_1 \cup \mathcal{H}_2)$，自然易出现过拟合。</p></li><li><p>用$E<em>{test}$作选择是不诚实的做法，因为$E</em>{test}$的结果是用于作报告用的。</p></li><li><p>理应用$\mathcal{D}<em>{out}$做选择，但这是不可能的，由此引出$\mathcal{D}</em>{val}$作为替代的解决方案。</p></li></ul><p>$$<br>\mathcal{D} = \mathcal{D}<em>{train} \cup \mathcal{D}</em>{val}<br>$$</p><ul><li>验证集需在数据集中随机选（<strong>经验值为$\frac{N}{5}$</strong>），这保证了验证集与总体独立同分布。</li><li>根据<a href="http://redmud.xyz/2018/03/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" target="_blank" rel="noopener">霍夫丁不等式</a>，当验证集与总体独立同分布时，$E<em>{val}$能在一定程度上代表$E</em>{out}$：</li></ul><p>$$<br>E<em>{out}-E</em>{val} = \mathcal{O}(\sqrt{\frac{\log M}{N_{val}}})<br>$$</p><p>模型选择的流程：不同复杂度的模型在训练集上得到各自的解$g$后，再测试各个解在$E_{val}$上的表现，将表现最好的模型在整个数据集上再次训练得到最终的$g^*$，并将该解在测试集上的表现作为最终汇报结果。</p><h2 id="E-val-的几种操作方案"><a href="#E-val-的几种操作方案" class="headerlink" title="$E_{val}$的几种操作方案"></a>$E_{val}$的几种操作方案</h2><blockquote><p>当计算力允许的情况下，选择 K 折交叉验证；否则，选用20%作为验证集直接得结果；而若模型有解析解，可以选用留一交叉验证。</p></blockquote><p>因为验证集是从手中的数据集中分离出来的，故其大小的选择存在如下困境：</p><p>过小，则$E<em>{val}$到$E</em>{out}$的泛化误差太大，即验证集没有作模型选择的能力；</p><p>过大，则$E<em>{in}$到$E</em>{out}$的泛化误差太大，即训练集没有训练该模型的能力，或说更易出现过拟合现象。</p><p>经验值为$\frac{N}{5}$，但考虑到$E_{val}$的稳定性，还提出了以下两种方案：</p><ol><li>leave-one-out cross validation：</li></ol><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/ml/loocv_1.png" alt="loocv_1"></p><p>​    除上图所示的保证之外，每次做训练的训练集也是极大化的，故模型的解也是最可靠的，自然得到的验证结果也是最可靠的。由此可见$E_{loocv}$是极佳方案，但当模型没有解析解，而是用迭代优化来求解时，该模型选择的方法因计算消耗太大而在实践中不常用。</p><ol><li><p>K-fold cross validation（经验值 K=10）：</p><p>$E<em>{CV} = \frac{1}{K} \sum</em>{i=1}^K E_{val}^{(i)}$</p></li></ol><p>### </p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;线性模型&quot;&gt;&lt;a href=&quot;#线性模型&quot; class=&quot;headerlink&quot; title=&quot;线性模型&quot;&gt;&lt;/a&gt;线性模型&lt;/h1&gt;&lt;h2 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;线性回归&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;所有变量的可表示的数量上的减少、衰退 regress，称为回归，即确认独立变量的过程。比如，独立变量和其他非独立变量之间的关系近似于线性时称为线性回归。&lt;/p&gt;
&lt;p&gt;回归是一种降维方法，减少的维度为非独立变量个数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;线性回归模型&quot;&gt;&lt;a href=&quot;#线性回归模型&quot; class=&quot;headerlink&quot; title=&quot;线性回归模型&quot;&gt;&lt;/a&gt;线性回归模型&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;输入空间映射到整个实数区间&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$&lt;br&gt;\min_w ||Xw-y||&lt;br&gt;$$&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——基础</title>
    <link href="http://yoursite.com/2018/03/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/03/30/机器学习——基础/</id>
    <published>2018-03-30T11:05:47.000Z</published>
    <updated>2018-06-12T09:47:58.170Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/ml/ml1.jpg" alt="ml_flow"></p><a id="more"></a> <h2 id="基础术语"><a href="#基础术语" class="headerlink" title="基础术语"></a>基础术语</h2><table><thead><tr><th style="text-align:center">名称</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">模型</td><td style="text-align:left">计算模型$\mathcal{A}$（学习法则）与数学模型$\mathcal{H}$（假设集）的合称为机器学习模型</td></tr><tr><td style="text-align:center">$E_{in}$</td><td style="text-align:left">假设 h 在已得到的资料上与真实模式 f 的误差</td></tr><tr><td style="text-align:center">$E_{out}$</td><td style="text-align:left">假设 h 在未见过的资料上与真实模式 f 的误差</td></tr><tr><td style="text-align:center">h</td><td style="text-align:left">$\mathcal{A}$ 从假设集$\mathcal{H}$ 中取出的一个假设函数</td></tr><tr><td style="text-align:center">g</td><td style="text-align:left">机器学习模型最终确定的在当前任务中用于代替真实模式 f 的估计模式</td></tr><tr><td style="text-align:center"></td></tr></tbody></table><blockquote><p>若无特殊说明，一般模型一词特指数学模型$\mathcal{H}$ 。</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="ML-DM-statistics"><a href="#ML-DM-statistics" class="headerlink" title="ML / DM / statistics"></a>ML / DM / statistics</h3><ul><li>ML 与 DM 很难区分</li><li>ML 是利用资料计算出接近真实模式 f 的估计模式 g</li><li>statistics 是利用资料推断一个尚不知结果的进程的结果的概率</li></ul><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>当任务存在某种潜在模式，但不能很容易地程式化地总结出来时。（前提是与该模式有关的资料是要能够获取到的）</p><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><h2 id="实现的可行性"><a href="#实现的可行性" class="headerlink" title="实现的可行性"></a>实现的可行性</h2><blockquote><p>模型与数据集大小两者之间没有先确定谁再确定谁的先后顺序：因为成本问题，数据集自然希望需要得越少越好，当数据量过小时便不可选复杂度过高的模型；而为使模型误差能够足够小，模型复杂度便须足够高，高复杂度的模型需要高样本复杂度的数据。总之，这是一个<strong>数据成本与模型误差博弈的过程</strong>。</p></blockquote><h3 id="VC（Vapnik-Chervonenkis）维"><a href="#VC（Vapnik-Chervonenkis）维" class="headerlink" title="VC（Vapnik-Chervonenkis）维"></a>VC（Vapnik-Chervonenkis）维</h3><blockquote><p>$d_{VC}$ 的含义：</p><ol><li>模型自由参数的个数，或称为模型的自由度（向量$w$ 的维度）</li><li>模型的强度：表示模型什么时候还能够 shatter</li></ol><p>注意：机器学习模型的 VC 维指数学模型的有效 VC 维，会根据计算模型变化（如加入不同的正则化项）而变化；常说的模型复杂度是指数学模型有效 VC 维对应的复杂度。</p></blockquote><h4 id="断点（breakpoint）"><a href="#断点（breakpoint）" class="headerlink" title="断点（breakpoint）"></a>断点（breakpoint）</h4><ul><li>shatter：若模型包含某次 N 个输入样本可能出现的所有情况（即模型能产生至少 $2^N$ 种假设），则称该输入能被该模型 $\mathcal{H}$ shatter</li><li>成长函数 $m_{\mathcal{H}}(N)$：模型能产生的最多（相同样本量不同样本的情况下，模型会产生不同的假设个数，此处为最多）的假设个数关于样本量的函数</li><li>断点 breakpoint：若N=k，成长函数首次不是指数级时，称 k 为最小断点</li><li>VC 维：$d_{VC} = $（最小断点k ）- 1，即模型能 shatter 的最大样本数</li></ul><p>当断点出现后，模型的成长函数便与模型的细节（线性分类器还是圆形分类器等细节）无关了：当 $N \geqslant k$ 时，$m<em>{\mathcal{H}}(N)  \leqslant B(N,k)$ ，进而能得到 B(N, k) 的表格，由表格可推得 $B(N,k) \leqslant \sum^{k-1}</em>{i} C_N^i \leqslant N^{k-1}$ 。</p><h4 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h4><p>由<a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality" target="_blank" rel="noopener">霍夫丁不等式</a>（给出了训练集误差无法代表整体误差的概率上限）：<br>$$<br>P{ |E<em>{in}(h) - E</em>{out}(h)|&gt; \epsilon } \leqslant 2e^{-2 \epsilon^2 N}<br>$$<br>知，针对任意一个假设 h，只要取样容量 N 足够大，不好的取样发生的概率很小。</p><p>因为数据集的好坏应该是针对模型来说的，故只有下列式子足够小，才能说数据集是好的（<strong>从直观上来说<u>数据集好</u>是指手中的资料已经可以代表所有已知和未知的资料了</strong>）：<br>$$<br>P{ \exists h \epsilon \mathcal{H},  s.t. |E<em>{in}(h) - E</em>{out}(h)|&gt; \epsilon  } = P{\sum<em>{h \epsilon \mathcal{H}} [ |E</em>{in}(h) - E<em>{out}(h)|&gt; \epsilon ] }<br>$$<br>设模型能产生的假设个数为$M$，由和事件的概率$ \leqslant$ 概率的和得到不好的取样发生的概率为：<br>$$<br>P{\sum</em>{h \epsilon \mathcal{H}} [ |E<em>{in}(h) - E</em>{out}(h)|&gt; \epsilon ] }  \leqslant 2 M e^{-2 \epsilon^2 N}<br>\<br>^{用E<em>{in}’ 替代E</em>{out}}<br><em>{又{|E</em>{in}-E<em>{in}’| &gt; \epsilon / 2} \Leftrightarrow {|E</em>{in}-(E<em>{in}+E</em>{in}’)/2| &gt; \epsilon / 4} }<br>\Longrightarrow<br>\<br>\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  \leqslant 4(2N)^{d<em>{VC}} e^{-\frac{1}{8} \epsilon^2 N}<br>\<br>\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad = \delta \quad (VC bound)<br>$$<br>由 VC bound得到：<br>$$<br>\epsilon = \sqrt{\frac{8}{N} \ln (\frac{4(2N)^{d</em>{VC}}}{\delta})}<br>$$<br>一般将该值记为$\Omega(N, \mathcal{H}, \delta)$ ，称为模型复杂度，它是样本复杂度、模型的 VC 维和 VC bound 的函数。故在置信度为$1-\delta$ 的情况下，泛化误差$|E<em>{in}-E</em>{out} |  \leqslant  \Omega(N, \mathcal{H}, \delta)$ 。模型复杂度本质是由多个因素影响（<strong>此处说的模型是数学模型即假设集，该<u>模型的 VC 维</u>决定了它能 shatter 的最大样本数，模型复杂度越高能够 shatter 的样本数越大；<u>模型的 VC bound</u> 决定了该模型结果【预测结果与泛化误差合称为模型结果】的不可信度，模型复杂度越高模型结果的不可信度越低；<u>样本复杂度</u>决定了针对该模型手中样本的好坏，模型复杂度越高手中样本变坏的可能性越大，另一种理解为样本复杂度决定了该模型能产生的假设个数，模型复杂度越高能产生的假设个数越多</strong>）的函数，而泛化误差本质是在一定置信度下的一个数，二者恰巧在数量上相等。</p><blockquote><p>虽然给出预期的置信度、泛化误差和$d<em>{VC}$ 就能得到针对“模型能从样本中学到点什么东西”这件事模型所需的样本复杂度，但一般令$N \approx 10 d</em>{VC}$ 就足够了。</p></blockquote><h4 id="常见模型的-VC-维"><a href="#常见模型的-VC-维" class="headerlink" title="常见模型的 VC 维"></a>常见模型的 VC 维</h4><ul><li><p>举例：n 维的二分类感知机，VC 维为 n+1</p><ol><li>当样本量 N=n+1 时，$X \epsilon R^{(n+1)*(n+1)}$ ，存在能够被该模型 shatter 的样本：令样本 X 可逆，则任意一种二分类情况 y 都可以被一个 w 划分出，因为 $w = X^{-1}y$ ；</li><li>当样本量 N=n+2 时，$X \epsilon R^{(n+2)*(n+1)}$ ，没有一个能够被该模型 shatter 的样本：因为 n+2 个样本中总有一个样本能被其它 n+1 个样本线性表示，设线性表示的系数为 $a<em>1, … , a</em>{n+1}$ ，则模型无法产生 $(sign(a<em>1), …, sign(a</em>{n+1}), -1)$ 这种二分类情况。</li></ol><p>故VC 维为 n+1。</p></li></ul><h3 id="可行性分析"><a href="#可行性分析" class="headerlink" title="可行性分析"></a>可行性分析</h3><ol><li><p>no free lunch 定理：若只坚持 f 是未知的，而不作出任何假设，那么在已知资料以外的部分去说我一定学到了什么东西（即找到了能够满足在已知资料以外的部分 $g \approx f$ 的$g$ ）这件事是做不到的。故机器学习的模型一般都是有某种归纳偏好的。<br>$$<br>\downarrow 对已知和未知资料作出假设：所有数据均是独立同分布的<br>$$</p></li><li><p>若<strong>样本量够大，模型的 VC 维为有限值</strong>，则由霍夫丁不等式可认为样本内误差可以泛化到样本外误差，即 g 能够具有很好的泛化能力；又若计算模型能够从数学模型中找到使得<strong>样本内误差趋于零</strong>的假设 h 作为 g，则认为学习成功。</p></li></ol><h2 id="加入噪声后"><a href="#加入噪声后" class="headerlink" title="加入噪声后"></a>加入噪声后</h2><p>噪声是指标签中的噪声，来源多为：</p><ol><li>打标签过程人一时走神打错标签</li><li>打标签时不同的人有不同的标准</li><li>打标签的数据本身有噪声</li></ol><p>对于含有噪声的标签，可以认为产生样本的数据源从真实模式 f(X) 变成了 P(y|X) ，即加入了些微抖动，但这只是一个变量的替换，并不影响 VC bound 的成立，故依然认为能够成功学习。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>机器学习能否成功，就考虑两件事：</p><ol><li>数学<strong>模型复杂度</strong>是否足够高，以有能力使得训练集误差够小；</li><li><strong>泛化误差</strong>（样本复杂度与模型 VC 维决定）是否足够低，以保证训练集误差能够代表整体误差。</li></ol><p>至于学习速度便是计算模型去考虑的事情了。</p><blockquote><p>在此可做个比拟：数学模型是天资，是人本身的智商，而计算模型是学习方法。方法用对了再加上高天资便能平步青云。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习基础&quot;&gt;&lt;a href=&quot;#机器学习基础&quot; class=&quot;headerlink&quot; title=&quot;机器学习基础&quot;&gt;&lt;/a&gt;机器学习基础&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/ml/ml1.jpg&quot; alt=&quot;ml_flow&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机硬件基础</title>
    <link href="http://yoursite.com/2018/03/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/03/29/计算机硬件基础/</id>
    <published>2018-03-29T01:27:43.000Z</published>
    <updated>2018-06-03T12:59:52.696Z</updated>
    
    <content type="html"><![CDATA[<h2 id="硬盘基础知识"><a href="#硬盘基础知识" class="headerlink" title="硬盘基础知识"></a>硬盘基础知识</h2><p>主引导扇区（512字节）包括三部分：</p><ul><li>MBR：主引导记录或主引导程序，用于硬盘启动时将系统控制转给指定的操作系统。</li><li>4个分区表（总64字节）：可为三个主分区表和一个扩展分区表或者四个主分区表。扩展分区表作为特殊的存在，标定了任意数量个逻辑分区表的位置；而主分区表和逻辑分区表标定的是主分区和逻辑分区的位置。</li><li>结束标志（2字节）</li></ul><a id="more"></a> ]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;硬盘基础知识&quot;&gt;&lt;a href=&quot;#硬盘基础知识&quot; class=&quot;headerlink&quot; title=&quot;硬盘基础知识&quot;&gt;&lt;/a&gt;硬盘基础知识&lt;/h2&gt;&lt;p&gt;主引导扇区（512字节）包括三部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MBR：主引导记录或主引导程序，用于硬盘启动时将系统控制转给指定的操作系统。&lt;/li&gt;
&lt;li&gt;4个分区表（总64字节）：可为三个主分区表和一个扩展分区表或者四个主分区表。扩展分区表作为特殊的存在，标定了任意数量个逻辑分区表的位置；而主分区表和逻辑分区表标定的是主分区和逻辑分区的位置。&lt;/li&gt;
&lt;li&gt;结束标志（2字节）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="cs" scheme="http://yoursite.com/categories/cs/"/>
    
    
  </entry>
  
  <entry>
    <title>数学——信息论基础</title>
    <link href="http://yoursite.com/2018/03/29/%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/03/29/数学——信息论基础/</id>
    <published>2018-03-29T01:26:58.000Z</published>
    <updated>2018-06-12T07:24:12.128Z</updated>
    
    <content type="html"><![CDATA[<h1 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h1><blockquote><p>熵可以认为是随机变量的数字特征，其含义为为确定某一随机变量所需信息量的平均。</p><p>设$ X \sim p(x)$ ，则 $H(X) = H(p) = -\sum p(x) \log_2 p(x)$ ，单位为比特。</p><p>若设 $Y = -\log_2 p(X)$ ，则$H(X)=E(Y)$ ，即随机变量的熵为随机变量函数的期望。$ -\log_2 p(X)$ <strong>意为为确定随机变量 $X$ 的值所需的信息量，</strong>若$p(x)$ 越小，则所需信息量越大。</p><p>综上，熵$H(X)$ 是 $X$ 信息量的期望。</p><blockquote><p>注：$X$ 指随机变量，$x$ 指随机变量取的值。</p></blockquote></blockquote><a id="more"></a> <h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li>联合熵：描述二维随机变量$(X_1, X_2)$的熵，$(X_1 , X_2) \sim p(x_1, x_2)$ 。</li><li>条件熵：描述条件随机变量$(X_2 | X_1)$的熵，$H(X_2 | X_1) = E(H(X_2| X_1=x_1))$ 。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;熵&quot;&gt;&lt;a href=&quot;#熵&quot; class=&quot;headerlink&quot; title=&quot;熵&quot;&gt;&lt;/a&gt;熵&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;熵可以认为是随机变量的数字特征，其含义为为确定某一随机变量所需信息量的平均。&lt;/p&gt;
&lt;p&gt;设$ X \sim p(x)$ ，则 $H(X) = H(p) = -\sum p(x) \log_2 p(x)$ ，单位为比特。&lt;/p&gt;
&lt;p&gt;若设 $Y = -\log_2 p(X)$ ，则$H(X)=E(Y)$ ，即随机变量的熵为随机变量函数的期望。$ -\log_2 p(X)$ &lt;strong&gt;意为为确定随机变量 $X$ 的值所需的信息量，&lt;/strong&gt;若$p(x)$ 越小，则所需信息量越大。&lt;/p&gt;
&lt;p&gt;综上，熵$H(X)$ 是 $X$ 信息量的期望。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注：$X$ 指随机变量，$x$ 指随机变量取的值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="数学" scheme="http://yoursite.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——凸优化</title>
    <link href="http://yoursite.com/2018/03/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/03/14/机器学习——凸优化/</id>
    <published>2018-03-14T13:48:31.000Z</published>
    <updated>2018-06-03T12:57:17.220Z</updated>
    
    <content type="html"><![CDATA[<h2 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h2><blockquote><p>集合$C$ 内任意两点间的线段上的点仍在该集合内，则称该集合为凸集。表示为$\theta x+(1-\theta)y \in C, \theta \in (0,1)$ ，其中$\theta x+(1-\theta)y$ 称为点$x$ 与$y$ 的凸组合。</p></blockquote><a id="more"></a> <h2 id="无约束最优化"><a href="#无约束最优化" class="headerlink" title="无约束最优化"></a>无约束最优化</h2><h3 id="数值优化"><a href="#数值优化" class="headerlink" title="数值优化"></a>数值优化</h3><h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><blockquote><p>本质为找函数的零点，用在最优化上便是找函数导数的零点。</p></blockquote><p>$$<br>w_{t+1} = w<em>t - (\frac{\partial ^2 L}{\partial w \partial w^T})^{-1} \frac{\partial L}{\partial w}|</em>{w=w_t}<br>$$</p><p>牛顿法相比起梯度往往法收敛速度更快，特别是迭代值距离收敛值比较近的时候，每次迭代都能使误差变成原来的平方，但是在高维时矩阵的逆计算会非常耗时。</p><h3 id="解析优化"><a href="#解析优化" class="headerlink" title="解析优化"></a>解析优化</h3><h2 id="有约束最优化"><a href="#有约束最优化" class="headerlink" title="有约束最优化"></a>有约束最优化</h2><h3 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h3><blockquote><p>通过引入拉格朗日乘子，可将有 d 个变量与 k 个约束条件的最优化问题转化为 d+k 个变量的无约束优化问题。</p></blockquote><h4 id="等式约束"><a href="#等式约束" class="headerlink" title="等式约束"></a>等式约束</h4><p>$$<br>\min_x f(x),<br>\<br>s.t. \quad g(x)=0.<br>$$</p><p>设 x 为 d 维向量，则约束曲面 $g(x)=0$是 d-1 维曲面，目标函数$f(x)$是 d 维曲面。</p><ul><li>因曲面梯度与自身法线共线，所以约束曲面上任意点的梯度$\nabla g(x)$必正交于约束曲面；</li><li>又目标函数在约束曲面上的最优点$x^<em>$对应的梯度$\nabla f(x^</em>)$必正交于约束曲面（不正交意味着不相切，不相切意味着至少相交于两个点，故可继续沿反梯度方向$-\nabla f(x) $找到更优解，因为梯度方向是函数增加最快的方向），故最优点$x^<em>$对应的梯度$\nabla f(x^</em>)$ 和 $\nabla g(x)$ 必共线，即必存在$\lambda \neq 0$使得</li></ul><p>$$<br>\nabla f(x) + \lambda \nabla g(x) = 0,<br>$$<br>由此得到等价的无约束最优化问题：<br>$$<br>\min_{x, \lambda} f(x)+ \lambda g(x). \quad (\lambda \neq 0)<br>$$</p><h4 id="不等式约束"><a href="#不等式约束" class="headerlink" title="不等式约束"></a>不等式约束</h4><p>$$<br>\min_x f(x),<br>\<br>s.t. \quad g(x) \le b.<br>$$</p><h5 id="等价问题（称为强对偶问题）"><a href="#等价问题（称为强对偶问题）" class="headerlink" title="等价问题（称为强对偶问题）"></a>等价问题（称为强对偶问题）</h5><p>对于不等式约束分两种情况讨论：</p><ol><li>当最优点 x 位于 g(x)-b&lt;0 区域时，约束形同虚设，可直接求 f(x) 的最小值，这等价于等式约束中将 $\lambda$ 置零；</li><li>当最优点 x 位于 g(x)-b=0 区域时，不等式约束便退化为等式约束了。</li></ol><p>将分类讨论的问题合为一个式子，如下（记 g(x)-b=h(x) ）：<br>$$<br>\min<em>x \max</em>{\lambda} f(x)+ \lambda h(x), \quad \lambda \ge 0<br>$$<br>称上式为原不等式约束最优化问题的等价问题，等价的原因为：</p><ol><li>通过对 $\lambda$ 做大于等于零的限制，便可实现在沿 $\lambda$ 方向求最大值时，将$h(x)&lt;0$的点通过置$\lambda$为零删掉（若$\lambda$可为负数，那么在求最大值时无法删掉这些点，因为在这些点上的最优解$\lambda \ne 0$），然后因为剩下的点对应的$\lambda$均为大于零的数，故基于这个结果在沿$x$方向上求最小值时，是在$h(x)=0$的区域内求得的使 f(x) 最小的最优解，从而完成上述分类讨论的第二种情况；</li><li>当然，若没有大于零的$\lambda$，那便是直接在无约束情况下求得的最优解，即分类讨论的第一种情况的解。</li></ol><h5 id="拉格朗日函数"><a href="#拉格朗日函数" class="headerlink" title="拉格朗日函数"></a>拉格朗日函数</h5><p>由此，原不等式约束最优化问题可转化为如下不等式约束的拉格朗日函数的最优化问题：<br>$$<br>\min_x f(x)+ \lambda h(x)<br>\<br>s.t. \quad h(x) \le 0<br>\<br>\quad \quad \lambda \ge 0<br>\<br>\quad \quad \lambda h(x) = 0<br>$$<br>称以上三个约束条件为KKT（Karush-Kuhn-Tucker）条件。</p><p>为使用拉格朗日函数，须将等价问题做如下变型：</p><p>对于等价问题（$\lambda \ge 0$省略不写，并记$L(x, \lambda)= f(x)+ \lambda h(x)$），因为<br>$$<br>\min<em>x \max</em>{\lambda} L(x, \lambda) \ge \min_x L(x, \lambda)<br>$$<br>所以<br>$$<br>\min<em>x \max</em>{\lambda} L(x, \lambda) \ge \max_{\lambda} \min_x L(x, \lambda)<br>$$<br>称$ L(x, \lambda)$为拉格朗日函数，称$\lambda$为对偶变量，称$\min<em>x L(x, \lambda)$为拉格朗日对偶函数，称等价问题为强对偶问题，称$\max</em>{\lambda} \min_x L(x, \lambda)$为弱对偶问题，称原不等式约束最优化问题为主问题。</p><h5 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h5><p>强对偶问题与弱对偶问题统称为对偶问题，无论主问题凸性如何，对偶问题始终是凸优化问题。一般弱对偶问题不等价于强对偶问题，但当主问题为凸优化问题，且可行域中至少有一点使不等式约束严格成立，那么弱对偶问题便可等价于强对偶问题，这称为强对偶性。</p><p>当强对偶性成立后，直接求解弱对偶问题便可得到主问题的解，具体为：通过拉格朗日乘子法可将不等式约束的最优化问题转为满足 KKT 条件的拉格朗日对偶函数，通过将拉格朗日函数对原变量和对偶变量的导数置零，并结合KKT条件便得到弱对偶问题的解，由强对偶性进而得主问题的解。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;凸集&quot;&gt;&lt;a href=&quot;#凸集&quot; class=&quot;headerlink&quot; title=&quot;凸集&quot;&gt;&lt;/a&gt;凸集&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;集合$C$ 内任意两点间的线段上的点仍在该集合内，则称该集合为凸集。表示为$\theta x+(1-\theta)y \in C, \theta \in (0,1)$ ，其中$\theta x+(1-\theta)y$ 称为点$x$ 与$y$ 的凸组合。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——前馈神经网络</title>
    <link href="http://yoursite.com/2018/03/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/03/11/机器学习——前馈神经网络/</id>
    <published>2018-03-11T10:16:33.000Z</published>
    <updated>2018-06-12T04:01:04.210Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前馈神经网络-feed-forward-nueral-network"><a href="#前馈神经网络-feed-forward-nueral-network" class="headerlink" title="前馈神经网络 (feed-forward nueral network)"></a>前馈神经网络 (feed-forward nueral network)</h1><blockquote><p>前馈神经网络一般有两种，linear perceptron network 和 RBF network，该文主要叙述前一种，其学习规则是梯度下降法，是一种无约束的最优化算法。</p><p><a href="https://github.com/bkseastone/Neural-Networks-for-Machine-Learning/blob/master/feedforwardNN/feedforwardNN.py" target="_blank" rel="noopener">NN示例代码</a></p></blockquote><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/nn_net1.jpg" alt="nn_net1"></p><a id="more"></a> <h2 id="符号及标记"><a href="#符号及标记" class="headerlink" title="符号及标记"></a>符号及标记</h2><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">$L$</td><td style="text-align:left">代价函数 / 损失函数 / 最优化目标函数</td></tr><tr><td style="text-align:center">$w$</td><td style="text-align:left">“轴突”权重</td></tr><tr><td style="text-align:center">$\sigma$</td><td style="text-align:left">神经元“树突”的激活函数</td></tr><tr><td style="text-align:center">$z$</td><td style="text-align:left">带权输入 $z^l=w^{l-1}a^{l-1}$ ，即从上个神经元轴突传到该神经元树突上的值</td></tr><tr><td style="text-align:center">$a$</td><td style="text-align:left">神经元的激活值</td></tr><tr><td style="text-align:center">$\delta$</td><td style="text-align:left">中间变量，称为某层的误差，专用于反向传播算法</td></tr></tbody></table><blockquote><p><em>注 1</em>：零层为真实数据，尚未前传，自然没有所谓的误差 $\delta^0$；因第零层的“轴突”上的权重 $w^0$ 尚未学习成功而导致的第一层的神经元的带权输入 $z^1$ 的误差，记为 $\delta^1$ 。</p><p><em>注 2</em>：每层“轴突”上的权重$w$ 的行数为希望学到的模式数，列数为输入数据的维度。</p><p><em>注 3</em>：做某一层的前传的时候，那一层的恒一神经元（用于模式中的偏置）才会被重新激活；当做为输出层时，该层的恒一神经元处于闭塞状态，是看不见的。当做某一层的反传的时候，是用该层神经元的误差$\delta$ 求<em>loss</em>对上一层“轴突”权重的偏导，而该层的恒一神经元没有所谓的误差，上一层的恒一神经元却有“轴突”权重，自然也有偏导，故只有求“轴突”权重偏导的那一层的恒一神经元会被重新激活，其余均处于闭塞状态。</p></blockquote><h2 id="学习法则"><a href="#学习法则" class="headerlink" title="学习法则"></a>学习法则</h2><blockquote><p>若 $y$ 与 $(x_1,\dotsb,x_m)$ 线性相关，且采样没有噪声，则直接采$m$个样本点求解线性方程就能得到参数 $\omega$ 的唯一解。为应对非线性相关的数据，采用迭代最优化（iterative optimization）的方法。</p></blockquote><h3 id="参数更新：梯度下降法"><a href="#参数更新：梯度下降法" class="headerlink" title="参数更新：梯度下降法"></a>参数更新：梯度下降法</h3><blockquote><p>梯度是个向量，指函数变化增加最快的地方，故沿负梯度的方向便能到达函数的极小值处。</p></blockquote><p>迭代优化的参数更新通式为：<br>$$<br>w(t+1) \leftarrow w(t)+\alpha \frac{v(t)}{||v(t)||} \quad , \alpha &gt; 0<br>$$<br>其中参数的确定由降低 <em>loss</em> 函数的方法确定。对 <em>loss</em> 函数进行一阶泰勒展开：<br>$$<br>L(w(t+1)) \approx L(w(t))+ \alpha v(t)^T \nabla L(w(t))<br>$$<br>要使 $L(w(t+1))$ 最小，须使 $ v(t)^T \nabla L(w(t))$ 最小，故令 $v(t)= -\nabla L(w(t))$ 。为使得学习率在陡的地方大，缓的地方小，令 $\alpha \propto ||\nabla L(w(t))||$ ，从而得到梯度下降法的参数更新式：<br>$$<br>w(t+1) \leftarrow w(t)-\alpha_0 \nabla L(w(t))<br>$$<br>其中 $\alpha_0$ 称为 fixed learning rate，而真实的 learning rate $\alpha$ 的大小随梯度的大小变化而变化。</p><h3 id="求梯度：反向传播算法"><a href="#求梯度：反向传播算法" class="headerlink" title="求梯度：反向传播算法"></a>求梯度：反向传播算法</h3><p>$$<br>\delta^l=(w^l\delta^{l+1})*\sigma’_{z^l}<br>\<br>\frac{\partial L}{\partial w^{l-1}}=\frac{1}{n}\delta^{l}(a^{l-1})^T<br>$$</p><p>在用矩阵编程计算梯度时，无需考虑具体矩阵乘积的细节和含义，在得到反向传播的标量表达式后，只需依照两条规则即可写出梯度的矩阵算式：</p><ol><li>依据标量表达式确定算式的结构；</li><li>依据<em>loss</em>对该层参数偏导的形状调整矩阵的顺序和形状。</li></ol><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><h4 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h4><p>待续</p><h4 id="加快-mini-batch-训练的3种方法"><a href="#加快-mini-batch-训练的3种方法" class="headerlink" title="加快 mini-batch 训练的3种方法"></a>加快 mini-batch 训练的3种方法</h4><h5 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h5><p>$$<br>M(t) = \alpha M(t-1) - \epsilon \frac{\partial L}{\partial w}<br>\<br>w += M(t)<br>$$</p><ul><li><p>动量法能够减小高曲率方向上的震荡，使得小球尽快地损失掉重力势能。窃以为，公式结合物理原则，应为（尚未测试）：<br>$$<br>v(t) = \alpha M(t-1) - \epsilon (1 + \frac{1}{|\frac{\partial L}{\partial w}|})<br>\<br>w+=v(t)<br>\<br>M(t) = \frac{v(t)}{\alpha}<br>$$</p></li><li><p>Ilya Sutskever 在2012年提出了一种优化版本：先在历史累计方向上前进一大步，然后在新位置上计算梯度并修正方向。可以这么理解，最好犯错之后去改正它。</p></li></ul><h5 id="自适应学习率"><a href="#自适应学习率" class="headerlink" title="自适应学习率"></a>自适应学习率</h5><p>每个神经元入度的不同导致了流入不同神经元的“树突”权值的最佳学习率各不相同。当入度很大时，每个“树突”权值改变一点点，累积的改变量就很大了，很容易过量；而当入度很小时反之。所以一般采用一个全局学习率，然后根据对每个神经元各自做适当调整：<br>$$<br>w+=-\epsilon g \frac{\partial L}{\partial w}<br>$$<br>初始化局部 $g=1$ ，如果下次该权值的梯度符号不变则增加 $g+=0.05$ ，否则减小为 $g*=0.95$ 。</p><blockquote><p>注意：</p><ol><li>将 $g$ 限制在某个合理的范围，比如[0.1, 10] 或 [.01, 100]。</li><li>使用 full-batch 或很大的 mini-batch，毕竟这样保证了梯度的符号不易受 mini-batch 的采样误差影响。</li><li>综合自适应学习率和动量更新法，以当前梯度和当前速度的符号来决策 $g$ 的变化。</li></ol></blockquote><h5 id="RMSProp：将梯度除以历史数量级"><a href="#RMSProp：将梯度除以历史数量级" class="headerlink" title="RMSProp：将梯度除以历史数量级"></a>RMSProp：将梯度除以历史数量级</h5><p>全局学习率之所以难选，主要是因为每个期望的最终的权值的数量级相差巨大。在 full batch 中，可以利用梯度的符号来替代权值的更新量，从而解决这个问题。<a href="https://zhidao.baidu.com/question/1367991976404469819.html" target="_blank" rel="noopener">RProp</a>结合了“只用符号”和“自适应学习率”的思想，但它违反了 SGD 的中心思想（当学习率很小的时候，权值更新量其实是当前mini-batch的梯度和历史梯度的平均。举例来讲，假设某个权值在9个批次中的梯度是+0.1，在第10个批次中的梯度是-0.9，我们希望这个权值大致不变。），故不适用于 mini-batch。而RMSProp便融合了mini-batch的高效性、mini-batch间的有效平均和RProp的稳定性。</p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li>对小数据集（10000以内）或者没有多少重复数据的大数据集，应当使用full-batch的一些优化方法，如Conjugate gradient、 LBFGS。然后试试adaptive learning rates, rprop …，它们是为神经网络而设计的方法。</li></ul><ul><li>对含有重复数据的大数据集，应当使用mini-batch。尝试动量法SGD，rmsprop 或LeCun的最新研究成果。</li></ul><h2 id="常用的激活函数及其对应的-loss-函数"><a href="#常用的激活函数及其对应的-loss-函数" class="headerlink" title="常用的激活函数及其对应的 loss 函数"></a>常用的激活函数及其对应的 <em>loss</em> 函数</h2><blockquote><p>假设激活函数$\sigma(z)=z$，则 $L(\omega , b)=\frac{1}{n}\sum ||y- { \omega<em>{L-1}[\omega</em>{L-2}(…x)] }||_2$，而我们平常所说的<em>loss</em>函数是与网络结构无关的“基础<em>loss</em>函数”。</p></blockquote><h3 id="线性激活函数与均方差-loss-函数"><a href="#线性激活函数与均方差-loss-函数" class="headerlink" title="线性激活函数与均方差 loss 函数"></a>线性激活函数与均方差 <em>loss</em> 函数</h3><p>$$<br>\sigma(z)=z<br>\<br>L=\frac{1}{2n}\sum_x||a(x)-y(x)||^2<br>$$</p><ul><li>$\delta^L=a-y$ （因为 $\sigma_z’=1$ ）；</li><li>$\delta^l=w^l\delta^{l+1},(l=1,…,L-1)$。</li></ul><h3 id="sigmod-激活函数与交叉熵-loss-函数"><a href="#sigmod-激活函数与交叉熵-loss-函数" class="headerlink" title="sigmod 激活函数与交叉熵 loss 函数"></a>sigmod 激活函数与交叉熵 <em>loss</em> 函数</h3><p>$$<br>\sigma(z)=\frac{1}{1+e^{-z}}<br>\<br>L=-\frac{1}{n}\sum_x[y\ln a+(1-y)\ln (1-a)]<br>$$</p><ul><li>$\delta^L=a-y$ （因为采用交叉熵 <em>loss</em>，约掉了 $\sigma’(z)$ ）；</li><li>$\delta^l=w^l\delta^{l+1}<em>a^l</em>(1-a^l)$ ，其中“ $*$ ”是<a href="https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/5446029?fr=aladdin#4" target="_blank" rel="noopener">Hadamard积</a>。</li></ul><blockquote><ol><li><p>用交叉熵而非均方差的原因是为了解决<strong>神经元饱和</strong>问题（指某些<strong>神经元</strong>可能因权重初始化不当或者确实已经学到了很成熟的地步，而<strong>处于 $\sigma’(z)​$ 值很小的激活值位置</strong>，进而导致在该处的梯度几近为零的现象；而其后果是神经元激活值会维持很长一段时间的近似稳定状态，这在训练终期是代表收敛的好现象，但若是在训练初期（即误差比较大时）， <em>loss</em> 下降会十分缓慢，严重拖慢收敛速度）：使用交叉熵会使得采用 sigmod 激活的神经网络中的梯度表达式中的 $\sigma’(z)​$ 被约掉，从而解决了神经元饱和问题。</p></li><li><p>可根据激活函数和希望的梯度形式反推得到所需的<em>loss</em>函数，见神经网络与深度学习（Michael Nielsen）3.1.3节。</p></li><li><p>最小化交叉熵 <em>loss</em> 函数等价于最大化以 sigmod 为参数的对数似然：<br>$$<br>\mathrm{likelyhood} = y<em>i  \ln[ \prod</em>{i=1}^N \sigma(z_i)] +(1-y<em>i) \ln[ \prod</em>{i=1}^N (1-\sigma(z_i))]<br>$$</p></li></ol></blockquote><h3 id="softmax-激活函数与对数似然-loss-函数（right-损失函数）"><a href="#softmax-激活函数与对数似然-loss-函数（right-损失函数）" class="headerlink" title="softmax 激活函数与对数似然 loss 函数（right 损失函数）"></a>softmax 激活函数与对数似然 <em>loss</em> 函数（right 损失函数）</h3><p>$$<br>\sigma(z_i)=\frac{e^{z_i}}{\sum_ie^{z_i}}<br>\<br>L=-\frac{1}{n}\sum_x\ln a_I,(a_I为真实类别对应神经元的激活值)<br>$$</p><ul><li>$\delta^L=a-y$ ；</li><li>$\delta^l=w^l\delta^{l+1}<em>a^l</em>(1-a^l)$ ，其中“ $*$ ”是<a href="https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/5446029?fr=aladdin#4" target="_blank" rel="noopener">Hadamard积</a>。</li></ul><blockquote><ol><li>因loss只求在真实类别神经元上的偏差，而激活函数的分母中所有神经元都包含，故$\delta^L$的求解分为两部分</li><li>设</li></ol></blockquote><h2 id="对该学习算法的一些理解"><a href="#对该学习算法的一些理解" class="headerlink" title="对该学习算法的一些理解"></a>对该学习算法的一些理解</h2><ol><li><ul><li>在梯度回传的过程中，$\omega$向量可能会变得非常大，则带权步长的移动只会引起在那个方向上微小的变化，以致很难有效地探索各种$\omega$模式。（大分量相对不怎么移动，小分量却相对移动很大，此处相对是指与分量自身相比，故会卡在某个方向上，以致很难有效地探索。）正则化的效果是让网络倾向于学习小一点的权重，让$\omega$只负责方向，而让$b$负责激活空间的位置。</li><li>另一个角度是，更小的权重意味着网络的行为不会因为噪声而改变太大，一个无规范化的网络可以使用大的权重来学习包含训练数据中的噪声的大量信息的复杂模型，而规范化的网络受限于根据训练数据中常见的模式来构造相对简单的模型，能够抵抗训练数据中的噪声的影响。</li></ul></li><li>在迭代过程中，还会出现神经元饱和问题，从梯度公式的角度想，是$\sigma’(z)$或$a$过小导致的梯度过小，从而引起学习缓慢的问题，解决方法便是构造合适的函数将梯度公式中的$\sigma’(z)$约掉；从网络的$loss$函数角度想，是寻找极低点时中途出现了原地踱步（小梯度）的情况，解决方法便是选用不同的激活函数与基础$loss$函数的搭配，从而得到形状更好的网络$loss$函数；从模式的可激活空间角度想，是样本$x$在模式$\omega$的激活空间上的分量值（带权输入）处于激活函数的平缓处（极低变化率）导致的模式$\omega$寻找进度迟缓，解决方法便是消除激活函数变化率对模式$\omega$的迭代寻找的影响。</li><li>​对导数的理解：导数的2-范数越大，说明在该点越不稳定，或说<strong>前面输入的波动会对输出造成很大的影响</strong>，故亦可称导数为该点的不稳定度或不成熟度。即可将 $\delta​$ 理解为该神经元的不成熟度。</li></ol><h2 id="实际应用中遇到的问题"><a href="#实际应用中遇到的问题" class="headerlink" title="实际应用中遇到的问题"></a>实际应用中遇到的问题</h2><h3 id="最优化问题：更新频率与幅度究竟应该为多大"><a href="#最优化问题：更新频率与幅度究竟应该为多大" class="headerlink" title="最优化问题：更新频率与幅度究竟应该为多大"></a>最优化问题：更新频率与幅度究竟应该为多大</h3><h3 id="泛化问题：如何防止过拟合？数据的两种噪音"><a href="#泛化问题：如何防止过拟合？数据的两种噪音" class="headerlink" title="泛化问题：如何防止过拟合？数据的两种噪音"></a>泛化问题：如何防止过拟合？数据的两种噪音</h3>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前馈神经网络-feed-forward-nueral-network&quot;&gt;&lt;a href=&quot;#前馈神经网络-feed-forward-nueral-network&quot; class=&quot;headerlink&quot; title=&quot;前馈神经网络 (feed-forward nueral network)&quot;&gt;&lt;/a&gt;前馈神经网络 (feed-forward nueral network)&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;前馈神经网络一般有两种，linear perceptron network 和 RBF network，该文主要叙述前一种，其学习规则是梯度下降法，是一种无约束的最优化算法。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/bkseastone/Neural-Networks-for-Machine-Learning/blob/master/feedforwardNN/feedforwardNN.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NN示例代码&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/nn_net1.jpg&quot; alt=&quot;nn_net1&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
</feed>
