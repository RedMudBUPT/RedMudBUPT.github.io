<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RedMudBUPT</title>
  
  <subtitle>RedMud&#39;s blog called RedMudBUPT</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-28T08:39:54.565Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>redmud</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>自然语言处理——隐马尔可夫模型</title>
    <link href="http://yoursite.com/2018/04/26/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/04/26/自然语言处理——隐马尔可夫模型/</id>
    <published>2018-04-26T13:20:06.000Z</published>
    <updated>2018-04-28T08:39:54.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="词性（Part-of-Speech）"><a href="#词性（Part-of-Speech）" class="headerlink" title="词性（Part-of-Speech）"></a>词性（Part-of-Speech）</h3><p>POS 是依据语法功能划分，是词语在区别词类时用到的属性。</p><h3 id="词性标注的方法"><a href="#词性标注的方法" class="headerlink" title="词性标注的方法"></a>词性标注的方法</h3><ol><li><p>rule-based</p><p>语言专家根据词法及语言学知识编制的规则。</p></li><li><p>learning-based</p><p>从专家标注的语料库中学习到用于自动标注的模型</p><ul><li>统计模型：隐马尔可夫模型（HMM），条件随机域模型（CRF），神经网络模型（NN）</li><li>规则学习：基于转换的学习（TBL）</li></ul></li></ol><h3 id="符号规定"><a href="#符号规定" class="headerlink" title="符号规定"></a>符号规定</h3><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">$N$</td><td style="text-align:left">训练数据中的句子总数</td></tr><tr><td style="text-align:center">$O_i$</td><td style="text-align:left">第 i 个句子（词序列）</td></tr><tr><td style="text-align:center">$o_i$</td><td style="text-align:left">某句子中的第 i 个词</td></tr><tr><td style="text-align:center">$Q_i$</td><td style="text-align:left">第 i 个句子对应的词性标注（词性序列）</td></tr><tr><td style="text-align:center">$q_i$</td><td style="text-align:left">某句子中的第 i 个词对应的词性</td></tr></tbody></table><h3 id="基于统计语言模型的词性标注基本模型"><a href="#基于统计语言模型的词性标注基本模型" class="headerlink" title="基于统计语言模型的词性标注基本模型"></a>基于统计语言模型的词性标注基本模型</h3><p>$$<br>\max_Q P(Q|O)<br>$$</p><p>由于语料库不可能包含所有可能出现的句子，故应得到一个更加宽泛的的表达式。利用贝叶斯法则得等价模型<br>$$<br>\max_Q P(O|Q)P(Q)<br>$$</p><h2 id="隐马尔可夫模型（HMM）"><a href="#隐马尔可夫模型（HMM）" class="headerlink" title="隐马尔可夫模型（HMM）"></a>隐马尔可夫模型（HMM）</h2><h3 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h3><ol><li>一阶马尔可夫假设，即语义相关性只涉及到前面 1 个词（也可设为 2 阶或 3 阶）：$P(Q) = P(q_1)P(q_2|q_1)…P(q<em>N|q</em>{N-1})$；</li><li>输出独立性假设，即单词$o_i$对应的$q_i$不受其他单词影响，即$P(o_i|q_i)$相互独立：$P(O|Q)=\prod P(o_i|q_i)$。</li></ol><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>$$<br>\max_Q \prod P(o_i|q_i) * \prod P(q<em>j|q</em>{j-1}),<br>$$</p><p>其中$P(o_i|q_i)$被称为发射概率，是通过统计每个单词在语料库中的出现情况得到的。对于因某个单词没有在语料库中出现导致发射概率为 0 进而导致整个句子出现概率为 0 的情况，须做一些特殊处理。</p><h3 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h3><h4 id="暴力遍历"><a href="#暴力遍历" class="headerlink" title="暴力遍历"></a>暴力遍历</h4><p>对一个句子的词性标注的时间复杂度为指数级别。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>使用离散时间点、离散状态，并做了马尔可夫假设，由此系统产生了马尔可夫过程的模式，它包含一个$\pi$向量和一个状态转移矩阵。</p><p>隐马尔科夫模型是在一个标准的马尔科夫过程中引入一组观察状态，以及该组观察状态与隐藏状态间的概率关系。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;h3 id=&quot;词性（Part-of-Speech）&quot;&gt;&lt;a href=&quot;#词性（Part-of-Speech）&quot; class=&quot;headerli
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>自然语言处理——任务</title>
    <link href="http://yoursite.com/2018/04/26/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E4%BB%BB%E5%8A%A1/"/>
    <id>http://yoursite.com/2018/04/26/自然语言处理——任务/</id>
    <published>2018-04-26T13:19:25.000Z</published>
    <updated>2018-04-26T13:57:16.784Z</updated>
    
    <content type="html"><![CDATA[<h2 id="词性标注（POS-tagging）"><a href="#词性标注（POS-tagging）" class="headerlink" title="词性标注（POS tagging）"></a>词性标注（POS tagging）</h2><h2 id="文语转换（TTS）"><a href="#文语转换（TTS）" class="headerlink" title="文语转换（TTS）"></a>文语转换（TTS）</h2><h2 id="词形还原（lemmatization）"><a href="#词形还原（lemmatization）" class="headerlink" title="词形还原（lemmatization）"></a>词形还原（lemmatization）</h2><h2 id="名词块检测（NP-chunk-detection-noun-phrase-chunking-detection）"><a href="#名词块检测（NP-chunk-detection-noun-phrase-chunking-detection）" class="headerlink" title="名词块检测（NP-chunk detection, noun phrase chunking detection）"></a>名词块检测（NP-chunk detection, noun phrase chunking detection）</h2><h2 id="词义消歧（word-sense-disambiguation）"><a href="#词义消歧（word-sense-disambiguation）" class="headerlink" title="词义消歧（word sense disambiguation）"></a>词义消歧（word sense disambiguation）</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;词性标注（POS-tagging）&quot;&gt;&lt;a href=&quot;#词性标注（POS-tagging）&quot; class=&quot;headerlink&quot; title=&quot;词性标注（POS tagging）&quot;&gt;&lt;/a&gt;词性标注（POS tagging）&lt;/h2&gt;&lt;h2 id=&quot;文语转换（
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——AdaBoost</title>
    <link href="http://yoursite.com/2018/04/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94AdaBoost/"/>
    <id>http://yoursite.com/2018/04/25/机器学习——AdaBoost/</id>
    <published>2018-04-25T00:04:49.000Z</published>
    <updated>2018-04-25T01:49:37.788Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>自然语言处理基础</title>
    <link href="http://yoursite.com/2018/04/19/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/04/19/自然语言处理基础/</id>
    <published>2018-04-19T01:50:59.000Z</published>
    <updated>2018-04-26T13:17:43.492Z</updated>
    
    <content type="html"><![CDATA[<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><ul><li>词典（词表）：目标语言的所有单词，记为$\mathcal{D}$。</li><li>语料库（Corpus）：即手中的数据集，记为$\mathcal{C}$。</li><li>上下文（context）：词汇所处的共现词、语境、前邻词、后邻词等词汇出现的篇章语境信息。</li><li>词符（tocken）：目标语言中一个词的标记，即指一个单词。</li><li>词串：一系列词符前后连接成串。</li><li>词串共现：两个词串在同一个句子中。</li><li>历史词：出现在该词符之前的所有词。</li></ul><h2 id="统计语言模型（statistical-language-model）"><a href="#统计语言模型（statistical-language-model）" class="headerlink" title="统计语言模型（statistical language model）"></a>统计语言模型（statistical language model）</h2><p>统计语言模型是基于大数定律，结合贝叶斯公式，利用语料库来计算一个句子（或词串）的概率的。n 个词串共现的概率为：<br>$$<br>P(W) = P(w_1)P(w_2|w_1)…P(w_n|w_1,w<em>2,…,w</em>{n-1})<br>\<br>where, \quad P(w_i) = \frac{count(w_i)}{count(\mathcal{C})}<br>\<br>P(w_i|w<em>1,…,w</em>{i-1}) = \frac{count(w<em>1,…,w</em>{i-1},w_i)}{count(w<em>1,…,w</em>{i-1})}<br>$$<br>求解该模型的方法有很多，n-gram 模型、决策树、最大熵模型、最大熵马尔科夫模型、条件随机场、神经网络等。</p><p>当所有的概率值都计算好之后便存储起来，下次需要计算一个词串的概率时，只需找到相关的概率参数，将之连乘即可。</p><h3 id="n-gram"><a href="#n-gram" class="headerlink" title="n-gram"></a>n-gram</h3><p>添加 n-1 阶马尔科夫假设，得到 n-gram 模型（假设为 3-gram）：<br>$$<br>P(w_i|w<em>1,…,w</em>{i-1}) = \frac{count(w<em>{i-2},w</em>{i-1},w<em>i)}{count(w</em>{i-2},w_{i-1})}<br>$$</p><p>当 n 越大：</p><ol><li>模型对历史词的关联性越强，故可区别性越好（模型复杂度越高）；</li><li>因模型复杂度呈指数级增高，大数定理的可靠性便越来越差（可理解为一种过拟合现象，因为模型复杂度相对于样本复杂度过高）。</li></ol><h4 id="数据稀疏问题"><a href="#数据稀疏问题" class="headerlink" title="数据稀疏问题"></a>数据稀疏问题</h4><blockquote><p>产生该问题的根本原因是采用了统计语言模型。</p></blockquote><ol><li>语料库中可能出现$count(w<em>1,…,w</em>{i-1},w_i) = 0$的情况，即该词串永远不会出现，但不应认为$P(w_i|w<em>1,…,w</em>{i-1})=0$；</li><li>语料库中可能出现$count(w<em>1,…,w</em>{i-1}) = count(w<em>1,…,w</em>{i-1},w_i)$的情况，但不应认为$P(w_i|w<em>1,…,w</em>{i-1})=$1。</li></ol><p>这两种问题称为数据稀疏问题，该问题的出现与语料库的大小无关，由<a href="https://www.gelbukh.com/CV/Publications/2001/CICLing-2001-Zipf.htm" target="_blank" rel="noopener">Zipf定律</a>知，在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比，故增大语料库依然无法解决数据稀疏问题。</p><p>在 n-gram 模型中，当 n 达到一定值后，即使样本复杂度足够，由于数据稀疏问题，n 越大，性能反而越差。</p><h4 id="平滑技术"><a href="#平滑技术" class="headerlink" title="平滑技术"></a>平滑技术</h4><p>平滑技术是针对数据稀疏问题引入的技术，常用的有：</p><ul><li>平滑：拉普拉斯平滑（加一平滑）、Lidstone 平滑（加 delta 平滑）、good-turing 平滑。</li><li>回退：backoff、interpolation（软回退）、kneser-ney smoothing。</li></ul><h3 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h3><p>Bengio et. al（2003）</p><h2 id="词符表示（tocken）"><a href="#词符表示（tocken）" class="headerlink" title="词符表示（tocken）"></a>词符表示（tocken）</h2><h3 id="discrete-representation"><a href="#discrete-representation" class="headerlink" title="discrete representation"></a>discrete representation</h3><p>独热（one-hot representation）：可认为是一种基于符号的词表示方法。</p><h3 id="distributed-representation"><a href="#distributed-representation" class="headerlink" title="distributed representation"></a>distributed representation</h3><blockquote><p>基于分布式相似度的表示，英文全称为 distributional similarity based representations。</p><p>You shall know a word by the company it keeps.    – J.R.Firth 1957:11</p></blockquote><h4 id="共现矩阵（cooccurence-matrix）"><a href="#共现矩阵（cooccurence-matrix）" class="headerlink" title="共现矩阵（cooccurence matrix）"></a>共现矩阵（cooccurence matrix）</h4><p>用上下文来表示一个单词的方法。有两种计算方法：</p><ul><li>基于整个段落的，又称为潜在语义分析。</li><li>基于窗口的（窗口内一般为5~10个单词，共现矩阵的行是窗口个数，列为所有不重复单词个数），优点是将语义和语法都考虑了进去。</li></ul><p>缺点是维度灾难，常用 SVD 来压缩矩阵以实现降维。</p><h4 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;术语&quot;&gt;&lt;a href=&quot;#术语&quot; class=&quot;headerlink&quot; title=&quot;术语&quot;&gt;&lt;/a&gt;术语&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;词典（词表）：目标语言的所有单词，记为$\mathcal{D}$。&lt;/li&gt;
&lt;li&gt;语料库（Corpus）：即手中的数据集，记
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>卷及神经网络基础</title>
    <link href="http://yoursite.com/2018/04/15/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/04/15/卷积神经网络基础/</id>
    <published>2018-04-15T08:57:43.000Z</published>
    <updated>2018-04-30T06:28:28.048Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><h3 id="卷积（convolution）"><a href="#卷积（convolution）" class="headerlink" title="卷积（convolution）"></a>卷积（convolution）</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>$$<br>(f*g)(n) = \int_{-\infin}^{+\infin}f(x)g(n-x) \mathrm{d} x<br>$$</p><p>两个在某种角度上具有一定关联性的函数 f 与 g，在二者自变量之和为一常数 n 的约束（更一般化地，在二者自变量的线性组合为一常数的约束）下，两函数之积在某个区间上的积分称为这两个函数在该区间上的卷积。</p><p>总之，两函数的卷积是两函数之积在某种线性组合（如 x+y=n）的约束下的特殊积分。</p><h4 id="命名"><a href="#命名" class="headerlink" title="命名"></a><a href="https://www.zhihu.com/question/54677157/answer/141245297" target="_blank" rel="noopener">命名</a></h4><p>之所以称为卷积，是因为该运算的结果是两函数的张量积构成的平面（或超平面）沿两函数各自自变量线性组合等式约束的轨迹做卷褶得到的降维的线（或超平面）上的某一点（线性组合等式约束决定的点）的值。</p><p>简而言之就是<strong>将张量积卷褶后的重合点之和即为卷积</strong>。</p><h4 id="理解角度"><a href="#理解角度" class="headerlink" title="理解角度"></a>理解角度</h4><ol><li><p>从对单通道二维图像做卷积的角度理解：</p><p>f 为图像像素值对位置的函数，g 为实现某种功能的滤波器（又称为卷积核、模板），其具体操作为对两矩阵的哈达玛积的所有元素求和。</p><p>能实现这种这种功能的原因是由卷积结果的形式决定的：卷积结果的形式可表示为$f*g = g(f) = af_{0}+bf_2+…$，对邻域像素的线性组合即为<a href="https://blog.csdn.net/purgle/article/details/73728940" target="_blank" rel="noopener">线性滤波器</a>。</p></li><li><p>要想实现非线性滤波器，需要用一大堆的线性滤波器，并通过很多层的非线性组合得到。NIN（network in network）提出了一种“偷吃步”的方法（MLPconv）来降低计算复杂度和模型复杂度。可从两个角度来理解 MLPconv：</p><p>（假设第一层的卷积核与输入图像相同大小）</p><ul><li><p>从结构形式的角度：$k_1$ 个卷积核输出到 $k_1$ 个神经元上，再假设第二层的卷积核与第一层神经元数大小相同，$k_2$ 个卷积核输出到 $k_2$ 个神经元上，即对$k_1$种线性滤波的非线性激活结果再次进行$k_2$种线性滤波，依此继续连接；能够由此实现任意一种非线性滤波器的原因与传统神经网络能够模拟任意函数的原理一样。</p><p>故 <strong>MLPconv 本质为一个传统的神经网络</strong>，即将一个传统的神经网络作为卷积核，神经网络隐藏神经元数由卷积核数决定，即希望得到的非线性滤波器个数。</p><p>由此，可将线性滤波器看作 MLPconv 的特殊情况，即一个感知机。</p></li><li><p>从物理含义的角度：$k_1$ 个卷积核输出$k_1$个特征图（此处每个图仅为一个点），而第二层的每个感知机是对$k_1$个通道的特征图的同一个位置作线性组合操作，然后做非线性激活，该操作可理解为对$k_1$个通道的特征图的同一个位置做 1×1 大小卷积核的卷积，或者理解为对$k_1$级联的特征图做 1×1 大小卷积核的池化（理解为池化，是目的导向的，因为此操作的目的是对不同特征进行不同方向的聚合；其与池化不同，因为参数都是要学习的，而非固定的）；能够由此实现任意一种非线性滤波器的原因是不同特征的不同方向的多次聚合能够得到一种任意一种非线性特征。</p><p>故 <strong>MLPconv 等价于一个 k×k 卷积层后缀数个 1×1 卷积层</strong>。</p><p>其中 1×1 卷积层也有人称之为跨通道参数的级联池化（cccp），实现跨通道的信息整合。</p></li></ul></li><li><p>针对不同尺度特征自然需要不同大小的卷积核，考虑到不同尺度特征可能属于同一级别的抽象，故提出了 inception 结构。该结构本质是对几个不同大小卷积核的 MLPconv 的结果的 <em>concat</em>。该结构的合理性是基于以下两点原因的：</p><ul><li>MLPconv 本质为一个小型神经网络，1×1 卷积层与 k×k 卷积层的先后顺序影响不大，又考虑到通道数庞大的可能性，先做 1×1 卷积可以极大地减少参数数量，提高泛化能力。</li><li>不同尺度特征自然需要不同大小的卷积核，而不同尺度特征可能属于同一级别的抽象。</li></ul><p>inception-v1 结构如下图所示（从左至右，为提取尺度越来越大的特征，最左侧的特征可认为是在该抽象程度上尺度为零的特征）：</p></li></ol><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/inception_v1.jpg" alt="inception_v1"></p><h4 id="在神经网络上的实现方法及理由（任务依赖的正则化）"><a href="#在神经网络上的实现方法及理由（任务依赖的正则化）" class="headerlink" title="在神经网络上的实现方法及理由（任务依赖的正则化）"></a>在神经网络上的实现方法及理由（任务依赖的正则化）</h4><ol><li>局部感知：每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息；</li><li>参数共享：图像具有一种“静态性”的属性：图像的一部分的统计特性与某些其他部分是一样的，即平移不变性的特征。由此提出了卷积核和特征图的概念。</li></ol><h3 id="池化（polling）"><a href="#池化（polling）" class="headerlink" title="池化（polling）"></a>池化（polling）</h3><p>池化过程本质为特征突出过程，去除特征图中的无用像素点。（查看西瓜书的样本不均衡问题）</p><h2 id="对前馈神经网络的一些优化"><a href="#对前馈神经网络的一些优化" class="headerlink" title="对前馈神经网络的一些优化"></a>对前馈神经网络的一些优化</h2><h3 id="ReLu"><a href="#ReLu" class="headerlink" title="ReLu"></a>ReLu</h3><p><a href="https://www.zhihu.com/question/59031444/answer/177786603" target="_blank" rel="noopener">https://www.zhihu.com/question/59031444/answer/177786603</a></p><h3 id="LRN"><a href="#LRN" class="headerlink" title="LRN"></a><a href="https://blog.csdn.net/hduxiejun/article/details/70570086" target="_blank" rel="noopener">LRN</a></h3><p>LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p><p>很少再用，现多用 dropout。</p><h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><blockquote><p>一种更恶化环境训练、更舒服环境战斗的思想。</p></blockquote><ol><li>达到了一种Vote的作用。对于全连接神经网络而言，我们用相同的数据去训练5个不同的神经网络可能会得到多个不同的结果，我们可以通过一种vote机制来决定多票者胜出，因此相对而言提升了网络的精度与鲁棒性。同理，对于单个神经网络而言，如果我们将其进行分批，虽然不同的网络可能会产生不同程度的过拟合，但是将其公用一个损失函数，相当于对其同时进行了优化，取了平均，因此可以较为有效地防止过拟合的发生。</li><li>减少神经元之间复杂的共适应性。当隐藏层神经元被随机删除之后，使得全连接网络具有了一定的稀疏化，从而有效地减轻了不同特征的协同效应。也就是说，有些特征可能会依赖于固定关系的隐含节点的共同作用，而通过Dropout的话，就有效地组织了某些特征在其他特征存在下才有效果的情况，增加了神经网络的鲁棒性。</li></ol><p>简而言之：Dropout在实践中能很好工作是因为其在训练阶段阻止了神经元的共适应。</p><h2 id="实现方面的细节"><a href="#实现方面的细节" class="headerlink" title="实现方面的细节"></a>实现方面的细节</h2><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p><a href="http://www.cnblogs.com/ooon/p/5418555.html" target="_blank" rel="noopener">http://www.cnblogs.com/ooon/p/5418555.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基础概念&quot;&gt;&lt;a href=&quot;#基础概念&quot; class=&quot;headerlink&quot; title=&quot;基础概念&quot;&gt;&lt;/a&gt;基础概念&lt;/h2&gt;&lt;h3 id=&quot;卷积（convolution）&quot;&gt;&lt;a href=&quot;#卷积（convolution）&quot; class=&quot;header
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>待办事项</title>
    <link href="http://yoursite.com/2018/04/10/%E5%BE%85%E5%8A%9E%E4%BA%8B%E9%A1%B9/"/>
    <id>http://yoursite.com/2018/04/10/待办事项/</id>
    <published>2018-04-10T13:00:07.000Z</published>
    <updated>2018-04-16T13:18:09.009Z</updated>
    
    <content type="html"><![CDATA[<ul><li>[ ] 回归为何能用于分类【机器学习——线性模型】</li><li>[ ] weight-decay 中的$\lambda$与常数 C 的解析关系是什么？【机器学习——线性模型】</li><li>[ ] 为什么 X 存在多重共线性时，最小二乘法求得的 w 在数值上会很大？【机器学习——线性模型】</li><li>[ ] 为什么l1正则化更易导致模型稀疏（即解更易在角上取得）</li><li>[ ] SGD原理</li><li>[ ] ​</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;[ ] 回归为何能用于分类【机器学习——线性模型】&lt;/li&gt;
&lt;li&gt;[ ] weight-decay 中的$\lambda$与常数 C 的解析关系是什么？【机器学习——线性模型】&lt;/li&gt;
&lt;li&gt;[ ] 为什么 X 存在多重共线性时，最小二乘法求得的 w 在
      
    
    </summary>
    
      <category term="board" scheme="http://yoursite.com/categories/board/"/>
    
    
  </entry>
  
  <entry>
    <title>信息检索与知识管理</title>
    <link href="http://yoursite.com/2018/04/02/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E4%B8%8E%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86/"/>
    <id>http://yoursite.com/2018/04/02/信息检索与知识管理/</id>
    <published>2018-04-02T04:01:57.000Z</published>
    <updated>2018-04-02T08:11:38.193Z</updated>
    
    <content type="html"><![CDATA[<h1 id="信息检索"><a href="#信息检索" class="headerlink" title="信息检索"></a>信息检索</h1><h2 id="电子期刊数据库"><a href="#电子期刊数据库" class="headerlink" title="电子期刊数据库"></a>电子期刊数据库</h2><ol><li><a href="http://www.wanfangdata.com.cn/" target="_blank" rel="noopener">北京万方数据股份有限公司网上数据库联机检索系统</a></li><li><a href="http://www.onelib.cn/online.aspx" target="_blank" rel="noopener">文献索取：图书馆学术交流与文献互助联盟</a></li><li><a href="http://www.cqvip.com/" target="_blank" rel="noopener">重庆维普中文科技期刊数据库</a></li><li><a href="http://www.cnki.net/" target="_blank" rel="noopener">知网：中国学术期刊全文数据库</a></li></ol><h2 id="电子图书数据库"><a href="#电子图书数据库" class="headerlink" title="电子图书数据库"></a>电子图书数据库</h2><ol><li><a href="http://book.chaoxing.com/" target="_blank" rel="noopener">全球最大的中文在线图书馆：超星电子图书</a></li><li><a href="http://www.chineseall.com/book.html" target="_blank" rel="noopener">中文在线电子图书</a></li></ol><h2 id="学位论文全文数据库"><a href="#学位论文全文数据库" class="headerlink" title="学位论文全文数据库"></a>学位论文全文数据库</h2><ol><li><a href="http://c.g.wanfangdata.com.cn/Thesis.aspx" target="_blank" rel="noopener">万方“中国学位论文全文数据库“</a></li><li><a href="http://www.cnki.net/" target="_blank" rel="noopener">中国知网“中国博士学位论文全文数据库”；中国知网“中国优秀硕士学位论文全文数据库”</a></li></ol><h2 id="读秀知识库"><a href="#读秀知识库" class="headerlink" title="读秀知识库"></a><a href="http://www.duxiu.com/" target="_blank" rel="noopener">读秀知识库</a></h2><h2 id="百链中英文学术搜索"><a href="#百链中英文学术搜索" class="headerlink" title="百链中英文学术搜索"></a><a href="http://www.blyun.com/" target="_blank" rel="noopener">百链中英文学术搜索</a></h2><h2 id="国家科技图书文献中心"><a href="#国家科技图书文献中心" class="headerlink" title="国家科技图书文献中心"></a><a href="http://www.nstl.gov.cn/NSTL/" target="_blank" rel="noopener">国家科技图书文献中心</a></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;信息检索&quot;&gt;&lt;a href=&quot;#信息检索&quot; class=&quot;headerlink&quot; title=&quot;信息检索&quot;&gt;&lt;/a&gt;信息检索&lt;/h1&gt;&lt;h2 id=&quot;电子期刊数据库&quot;&gt;&lt;a href=&quot;#电子期刊数据库&quot; class=&quot;headerlink&quot; title=&quot;电子期
      
    
    </summary>
    
      <category term="工具的使用" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习——广义线性模型</title>
    <link href="http://yoursite.com/2018/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/04/01/机器学习——广义线性模型/</id>
    <published>2018-04-01T08:52:18.000Z</published>
    <updated>2018-04-25T01:12:15.716Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><blockquote><p>所有变量的可表示的数量上的减少、衰退 regress，称为回归，即确认独立变量的过程。比如，独立变量和其他非独立变量之间的关系近似于线性时称为线性回归。</p><p>回归是一种降维方法，减少的维度为非独立变量个数。</p></blockquote><h3 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h3><blockquote><p>输入空间映射到整个实数区间</p></blockquote><p>$$<br>\min_w ||Xw-y||<br>$$</p><h3 id="最小二乘-VS-正交投影"><a href="#最小二乘-VS-正交投影" class="headerlink" title="最小二乘 VS 正交投影"></a>最小二乘 VS 正交投影</h3><p>最小二乘法与正交投影变换是对同一个问题两种不同等价形式的解决方法，且为相同的结果：</p><p>同一个问题：对一堆点求其回归方程，即<br>$$<br> \min_w ||Xw - y ||<br>$$<br>该问题的两种等价形式为</p><ol><li>线性方程组$Xw=y$ 的近似解</li><li>最小二乘问题$x^*=\min_w (Xw-y)^T(Xw-y)$</li></ol><p>对于第一个等价形式，X 的列向量的线性组合无法得到 y，那么将 y 正交投影至 X 的列空间，便可得到线性方程组的一种近似解：$Xw=X(X^TX)^{-1}X^T y \Rightarrow w = (X^TX)^{-1}X^T y$ 。</p><p>对于第二个等价形式，可用导数零点解决该无约束最优化问题：$\frac{\partial (Xw-y)^T(Xw-y)}{\partial w}=2X^T(Xw-y)=0 \Rightarrow w=(X^TX)^{-1}X^T y$ 。</p><blockquote><p>更进一步：</p><ol><li>若$X^TX$不是满秩矩阵，即$X$非列满秩，则最小二乘法与正交投影变换都失效了，此时$w$自由度为$n-rank(X^TX)$，常用的一个解是加入一个归纳偏好$\min ||w||_2$：将$X^TX$进行奇异值分解进而求得伪逆，从而得到$w$。</li><li>通常$X^TX$是可逆的，因为样本数$\gg$ 特征维度+1。但实践中常因为数值稳定性用$X^+$代替$(X^TX)^{-1}X^T$。</li><li>如果输入矩阵X中存在线性相关或者近似线性相关的列，那么输入矩阵 X 就会变成或者近似变成奇异矩阵（singular matrix）。这是一种病态矩阵，矩阵中任何一个元素发生一点变动，整个矩阵的行列式的值和逆矩阵都会发生巨大变化。这将导致最小二乘法对观测数据的随机误差极为敏感，进而使得最后的线性模型产生非常大的方差，这个在数学上称为多重共线性（multicollinearity）。在实际数据中，变量之间的多重共线性是一个非常普遍的现象，其产生机理及相关解决方案在“特征选择和评估”中有介绍。</li></ol></blockquote><h2 id="对数几率回归（逻辑回归）"><a href="#对数几率回归（逻辑回归）" class="headerlink" title="对数几率回归（逻辑回归）"></a>对数几率回归（逻辑回归）</h2><blockquote><p>几率（odds）：（样本作为正例的可能性 / 反例的可能性，即$\frac{y}{1-y}$）正例的赔率，反映了样本作为正例的相对可能性。</p></blockquote><h3 id="对数几率回归模型"><a href="#对数几率回归模型" class="headerlink" title="对数几率回归模型"></a>对数几率回归模型</h3><blockquote><p>输入空间映射到  [0, 1] 区间</p></blockquote><p>$$<br>\min_w ||Xw- \ln \frac{y}{1-y}||<br>$$</p><p>在形式上仍是线性回归，是对线性回归模型的扩展，即令模型逼近$y$的衍生物，这里的对数几率函数起到了将线性回归模型的预测值与真实标记联系起来的作用，称之为联系函数。当考虑其使用时称之为逻辑回归模型$\frac{1}{1+e^{-Xw}}$。</p><h2 id="局部加权线性回归（LWR）"><a href="#局部加权线性回归（LWR）" class="headerlink" title="局部加权线性回归（LWR）"></a>局部加权线性回归（LWR）</h2><blockquote><p>解决对不规则函数进行回归时容易出现的欠拟合与过拟合问题</p></blockquote><p>其流程是，每次预测时都需要调用所有的样本$X$，结合预测点来拟合回归曲线。</p><p>其原理是，选择与预测点$x^*$相近的点来做线性回归，忽略远处的点对预测的影响。</p><p>其实现方式是加权最小二乘：<br>$$<br>\min<em>w ||\lambda (Xw - y)||<br>\<br>s.t. \quad \lambda</em>{i} = e^{-||x_i - x^*||_2^2}<br>$$</p><p>缺点：对于每一个要预测的点，都要重新依据整个数据集计算一个线性回归模型出来，使得算法代价极高。</p><h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h2><p>待加</p><h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a><a href="https://github.com/RedMudBUPT/gitpage_img/blob/master/ml/LIN/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_%E9%99%88%E5%B8%8C%E5%AD%BA.pdf" target="_blank" rel="noopener">广义线性模型</a></h2><blockquote><p>大多数的概率分布都能表示成指数分布族的形式，如高斯分布，对噪声和不确定性进行建模；伯努利分布，对有两个离散结果的事件建模；多项式分布（Multinomial），对有K个离散结果的事件建模；泊松分布（Poisson），对计数过程进行建模，如网站访问量的计数问题；指数分布（Exponential），对有间隔的证书进行建模，如预测公交车的到站时间的问题；等等。通过进一步的推导，就能得到各自的线性模型，这大大扩展了线性模型可解决问题的范围。</p><p><strong>广义线性模型：用某种指数分布去逼近数据真实分布的广义线性回归。</strong></p></blockquote><p>$$<br>P(y; \eta)=be^{\eta^T T(y) - a}<br>$$</p><p>GLM 的三个假设：</p><ol><li>$p(y | x; w ) \sim be^{\eta^T T(y) - a}$ ：y 基于 x 的条件概率服从指数分布族中以$\eta$为参数的某个分布；</li><li>学习的目标是预测 T(y) 基于 x 的条件期望，因为 T(y) 通常为 y，即目标函数为$E(y∣x; w)$，故<strong>线性模型的本质是让某个指数分布的期望去逼近 y</strong>；</li><li>$\eta$ 和 x 的关联是线性的，即$\eta = w^T x$，从而</li></ol><h3 id="linear-regression"><a href="#linear-regression" class="headerlink" title="linear regression"></a>linear regression</h3><blockquote><p>线性函数是高斯分布的期望 $u$ 在线性回归模型上的表现形式，即在噪声影响下最可能的值。</p><p>因为$u$ 在实数区间取值，故应用于回归。</p></blockquote><p>$$<br>\sigma = z<br>$$</p><p>高斯分布：<br>$$<br>p(y;u) = \frac{1}{\sqrt{2 \pi} \sigma}e^{- \frac{(y-u)^2}{2 \sigma^2}}<br>\<br>= \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2}y^2} e^{uy - \frac{1}{2}u^2}<br>$$<br>将高斯分布与广义线性模型对比得到：<br>$$<br>\eta = u \Rightarrow u = \eta<br>$$<br>从而得到线性回归的数学模型（假设集）。</p><h3 id="sigmod"><a href="#sigmod" class="headerlink" title="sigmod"></a>sigmod</h3><blockquote><p>logistic 函数是伯努利分布的期望 $\phi$ 在线性回归模型上的表现形式，即单次正例事件发生的概率；或说是<strong>伯努利分布体现在线性回归模型上的函数</strong>。</p><p>因$\phi$在 (0, 1) 区间取值，故应用于分类。</p></blockquote><p><img src="https://github.com/RedMudBUPT/gitpage_img/blob/master/ml/lin1.jpg?raw=true" alt="lin1"><br>$$<br>logisticfunction  \quad \sigma = \frac{1}{1+e^{- z}}<br>$$</p><p>伯努利分布：<br>$$<br>p(y; \phi) = \phi^y (1-\phi)^y<br>\<br>\quad \quad \quad \quad \quad = e^{(\ln \frac{\phi}{1-\phi})y + \ln (1-\phi)}<br>$$<br>当只针对一个样本来看时，伯努利分布便降为 0 / 1 分布；当针对整体来看时，y 便是正例发生的次数统计，目标函数为$sum( \phi)$ 。</p><p>将伯努利分布与广义线性模型对比得到：<br>$$<br>\eta = \ln \frac{\phi}{1- \phi} \Rightarrow \phi = \frac{1}{1+e^{- \eta}}<br>$$<br>从而得到逻辑回归的数学模型（假设集）。</p><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><blockquote><p>softmax 函数是多项式分布的期望$\phi_i$在线性回归模型上的表现形式，即某类事件发生的概率。</p><p>因$\phi_i$在 (0, 1) 区间取值，故应用于分类。</p></blockquote><p>$$<br>softmaxfunction  \quad \sigma = \frac{e^{z<em>i}}{\sum</em>{j=i}^{k} e^{z_j}}<br>$$</p><p><img src="https://github.com/RedMudBUPT/gitpage_img/blob/master/ml/lin2_softmax.jpg?raw=true" alt="lin2_softmax_"></p><blockquote><p>上述的伯努利分布与多项式分布均是针对一个样本来说的。</p></blockquote><h3 id="回归用于分类"><a href="#回归用于分类" class="headerlink" title="回归用于分类"></a>回归用于分类</h3><p>其本质仍是回归，只是在最后一步做了处理以用于分类，</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>线性模型的思想是用某种指数分布去逼近数据真实分布，从而实现某种预测；其实现方法是让某个指数分布的期望去逼近 y，然后用最小二乘或最大似然之类的方法构建模型。</p><p>任务的不同类型是依据期望的取值空间划分的。</p><h2 id="Q-次多项式回归"><a href="#Q-次多项式回归" class="headerlink" title="Q 次多项式回归"></a>Q 次多项式回归</h2><blockquote><p>线性不可分数据可能圆形可分，或者其他二次曲线可分，甚至更一般化 Q 次曲线可分，由此引出 Q 次多项式回归的想法。</p></blockquote><h3 id="Q-次多项式特征转换"><a href="#Q-次多项式特征转换" class="headerlink" title="Q 次多项式特征转换"></a>Q 次多项式特征转换</h3><blockquote><p>人工提取特征的过程可认为是某种特征转换，此处提出的是一般化的 Q 次多项式特征转换。</p></blockquote><p>$$<br>z = \phi (x) = (1, x_1, x_2, x_1^2, x_2^2, x_1 x_2)<br>$$</p><p>对于线性不可分的 X 可经过 Q 次多项式特征转换$\phi$（下文简称为特征转换）变为线性可分的 z。该过程可细述为：</p><p>特征转换$\phi$将 d+1 维的 X 空间转换为 $C<em>{d+Q}^Q$维的 Z 空间，其中 ；$C</em>{d+Q}^Q = O(Q^d)$VC 维也由 d+1 变为了 $C_{d+Q}^Q$，模型复杂度呈幂级增长，记为$\mathcal{H}_1$升级为了$\mathcal{H}_Q$，其关系为$\mathcal{H}_1 \subset \mathcal{H}_2 \subset … \subset  \mathcal{H}_Q$。</p><p>legendre polynomials：正交化的特征转换。避免同数量级的 x 经普通的特征转换后在不同特征下存在好几个数量级的差距，如 $x$与$x^{10}$，从而使得在特征转换后 w 仍能具有同等的影响力。</p><h4 id="特征转换后的模型的-VC-维的推导"><a href="#特征转换后的模型的-VC-维的推导" class="headerlink" title="特征转换后的模型的 VC 维的推导"></a>特征转换后的模型的 VC 维的推导</h4><p>由公式<br>$$<br>C_n^m+C<em>n^{m-1} = C</em>{n+1}^m<br>$$<br>得转换后 x 的 Q 次项的项数为：<br>$$<br>C_d^Q+(Q-1)C_d^{Q-1}+…+C<em>d^1=C</em>{d+Q-1}^Q ,<br>$$<br>故，转换后 x 小于等于 Q 次项的项数为：<br>$$<br>C<em>{d+Q-1}^Q + C</em>{d+Q-2}^{Q-1}+…+C<em>{d+Q-Q}^1+C</em>{d-1}^0<br>\<br>=C<em>{d+Q-1}^Q + C</em>{d+Q-2}^{Q-1}+…+C<em>{d}^1+C</em>{d}^0<br>\<br>=C<em>{d+Q-1}^Q + C</em>{d+Q-2}^{Q-1}+…+C<em>{d+1}^1<br>\<br>=C</em>{d+Q-1}^Q + C<em>{d+Q-1}^{Q-1}<br>\<br>=C</em>{d+Q}^Q<br>$$</p><h2 id="线性软间隔支撑向量机"><a href="#线性软间隔支撑向量机" class="headerlink" title="线性软间隔支撑向量机"></a>线性软间隔支撑向量机</h2><p>详见支撑向量机</p><h1 id="核模型"><a href="#核模型" class="headerlink" title="核模型"></a>核模型</h1><h2 id="核脊回归（KRR）"><a href="#核脊回归（KRR）" class="headerlink" title="核脊回归（KRR）"></a>核脊回归（KRR）</h2><p>常用 SVR 替代</p><h2 id="核逻辑回归（KLR）"><a href="#核逻辑回归（KLR）" class="headerlink" title="核逻辑回归（KLR）"></a>核逻辑回归（KLR）</h2><p>常用 probabilistic SVM 替代</p><h1 id="模型的选择"><a href="#模型的选择" class="headerlink" title="模型的选择"></a>模型的选择</h1><h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><blockquote><ul><li>过拟合：指先前的模型$E<em>{in}&lt;E</em>{out}$，而现在升级后的模型$E<em>{in}$更小、$E</em>{out}$更大，称这种现象为过拟合；</li><li>泛化能力差：指该模型$E<em>{in}&lt;&lt;E</em>{out}$。</li></ul></blockquote><h3 id="出现原因（4个）"><a href="#出现原因（4个）" class="headerlink" title="出现原因（4个）"></a>出现原因（4个）</h3><p><strong>模型复杂度过高、数据量有限、随机噪声或确定性噪声过大</strong>。</p><ol><li><p>低复杂度数据下，为什么高复杂度模型会出问题：</p><p>在保证泛化误差的置信度为$1-\delta$ 的前提下（若无此保证，机器则无从谈起），由 VC bound 得到泛化误差为：</p></li></ol><p>$$<br>\epsilon = \sqrt{\frac{8}{N} \ln (\frac{4(2N)^{d_{VC}}}{\delta})},<br>$$</p><p>​    故当在低复杂度数据的情况下，复杂度越高的模型会出现更高的泛化误差。</p><ol><li>确定性噪声指隐含模式可能是复杂度很高的模式，而这种高复杂度的模式就像在某种低复杂度模式上加入了随机噪声，从可行性考虑将其归为噪声。称之为确定性是因为在确定了输入数据 X 后，“噪声”便可由隐含模式确定地得到，而不再是随机的。</li><li>直觉上，噪声相当于降低了有效数据的数量，相较于有效数据的数量模型成为高复杂度的模型，进而出现问题，解释见第一条。也有人解释为该模型更有可能将噪声的模式也学习进去。</li></ol><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/ml/overfit_1.png" alt="overfit_1"></p><ol><li>根本原因还是第一条。</li></ol><h3 id="解决方法（4个）"><a href="#解决方法（4个）" class="headerlink" title="解决方法（4个）"></a>解决方法（4个）</h3><ol><li>从简单模型开始；</li><li>对数据进行清理（删掉高噪声数据）或修剪（修正错误标记数据）；</li><li>人工增加数据（可能影响原始数据的真实分布，数据的增加方式应尽可能依照原始数据的隐含模式）；</li><li>正则化。</li></ol><h3 id="附：正则化（regularization"><a href="#附：正则化（regularization" class="headerlink" title="附：正则化（regularization)"></a>附：正则化（regularization)</h3><blockquote><p>其思想是将高复杂度模型进行退化。</p><p>命名由来：对<a href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96/5739561?fr=aladdin&amp;fromid=18081730&amp;fromtitle=regularization" target="_blank" rel="noopener">不适定问题（ill-posed problems）</a>的近似解。</p></blockquote><h4 id="两种解释"><a href="#两种解释" class="headerlink" title="两种解释"></a>两种解释</h4><ol><li><p>从机器学习的角度：</p><p>其思想是将高复杂度模型进行退化，故用稀疏假设集来降低模型的复杂度。比如加入约束条件$\sum bool(w \neq 0) \leqslant 2$记为$\mathcal{H}_2’$，显然$\mathcal{H}_2 \subset \mathcal{H}<em>2’ \subset \mathcal{H}</em>{10}$。不过因为布尔操作，该问题的求解已被证明是 NP-hard 问题。</p><p>为使原 NP-hard 问题易解，将其转化为 soft 版本：约束条件换成$\sum w^2 \leqslant C$，记为$\mathcal{H}(C)$，显然$\mathcal{H}<em>{10}(0)\subset \mathcal{H}</em>{10}(1)\subset … \subset \mathcal{H}<em>{10}( \infin) = \mathcal{H}</em>{10}$。</p><p>进而机器学习模型（将约束条件看做$\mathcal{A}$的一部分，假设集$\mathcal{H}$仍为那个大的假设集$\mathcal{H}_{10}$，能否产生多大数量的假设要看数据）变为<br>$$<br>\min<em>w E</em>{in}(w)<br>\<br>s.t. \quad \sum w^2 \leqslant C<br>$$<br>由拉格朗日乘子法得到等价问题：<br>$$<br>\min<em>w E</em>{in}(w) + \lambda ||w||_2^2, \quad \lambda &gt; 0.<br>$$<br>其中常数 C 隐含在 $\lambda$中，$\lambda$被称为 C 的等价常数。</p></li><li><p>从统计的角度：</p><p>X 存在多重共线性时，最小二乘法求得的 w 在数值上会很大，如果 x 有一点小小的变化，输出结果会有很大的差异，即对X 中的噪声非常敏感，所以其解会非常不稳定。而若能限制 w 的增长，模型对噪声的敏感度便会降低，由此引出<a href="https://blog.csdn.net/daunxx/article/details/51578787" target="_blank" rel="noopener">脊回归（Ridge regression）</a>：<br>$$<br>L = \min<em>w ||E</em>{in}(w)||_2^2+ \lambda ||w||_2^2 ,<br>$$<br>即在原损失函数上加入 w 的2-范数的惩罚项。</p><p>对损失函数求导置零得到解：<br>$$<br>w = (X^TX+ \lambda I)^{-1} X^Ty<br>$$</p></li></ol><h4 id="正则化方向"><a href="#正则化方向" class="headerlink" title="正则化方向"></a>正则化方向</h4><blockquote><p>损失函数的构造也是从这三个方向出发考虑的。</p></blockquote><ol><li>任务依赖：看任务的具体特性，进行某种特殊的正则化。</li><li>普遍看似合理的方向：使模型更平滑或更简单（因为随机噪声或确定性噪声都是非平滑的），由此得到 sparsity regularizer（稀疏正则化）$||w||_1$。</li><li>更易于求解的方向：由此得到 weight-decay regularizer（权重衰减正则化）$||w||_2^2$。</li></ol><h2 id="验证集作为-mathcal-D-out-的替代来进行模型选择的可行性"><a href="#验证集作为-mathcal-D-out-的替代来进行模型选择的可行性" class="headerlink" title="验证集作为$\mathcal{D}_{out}$的替代来进行模型选择的可行性"></a>验证集作为$\mathcal{D}_{out}$的替代来进行模型选择的可行性</h2><ul><li><p>用$E_{in}$作选择易过拟合，不可靠；</p><p>假设$g_1$、$g_2$是$\mathcal{H}_1$、$\mathcal{H}<em>2$由$E</em>{in}$选出的，若再由$E<em>{in}$选出$g^<em>$，那么你的$g^</em>$对应的模型复杂度便是$d</em>{VC}(\mathcal{H}_1 \cup \mathcal{H}_2)$，自然易出现过拟合。</p></li><li><p>用$E<em>{test}$作选择是不诚实的做法，因为$E</em>{test}$的结果是用于作报告用的。</p></li><li><p>理应用$\mathcal{D}<em>{out}$做选择，但这是不可能的，由此引出$\mathcal{D}</em>{val}$作为替代的解决方案。</p></li></ul><p>$$<br>\mathcal{D} = \mathcal{D}<em>{train} \cup \mathcal{D}</em>{val}<br>$$</p><ul><li>验证集需在数据集中随机选（<strong>经验值为$\frac{N}{5}$</strong>），这保证了验证集与总体独立同分布。</li><li>根据<a href="http://redmud.xyz/2018/03/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" target="_blank" rel="noopener">霍夫丁不等式</a>，当验证集与总体独立同分布时，$E<em>{val}$能在一定程度上代表$E</em>{out}$：</li></ul><p>$$<br>E<em>{out}-E</em>{val} = \mathcal{O}(\sqrt{\frac{\log M}{N_{val}}})<br>$$</p><p>模型选择的流程：不同复杂度的模型在训练集上得到各自的解$g$后，再测试各个解在$E_{val}$上的表现，将表现最好的模型在整个数据集上再次训练得到最终的$g^*$，并将该解在测试集上的表现作为最终汇报结果。</p><h2 id="E-val-的几种操作方案"><a href="#E-val-的几种操作方案" class="headerlink" title="$E_{val}$的几种操作方案"></a>$E_{val}$的几种操作方案</h2><blockquote><p>当计算力允许的情况下，选择 K 折交叉验证；否则，选用20%作为验证集直接得结果；而若模型有解析解，可以选用留一交叉验证。</p></blockquote><p>因为验证集是从手中的数据集中分离出来的，故其大小的选择存在如下困境：</p><p>过小，则$E<em>{val}$到$E</em>{out}$的泛化误差太大，即验证集没有作模型选择的能力；</p><p>过大，则$E<em>{in}$到$E</em>{out}$的泛化误差太大，即训练集没有训练该模型的能力，或说更易出现过拟合现象。</p><p>经验值为$\frac{N}{5}$，但考虑到$E_{val}$的稳定性，还提出了以下两种方案：</p><ol><li>leave-one-out cross validation：</li></ol><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/ml/loocv_1.png" alt="loocv_1"></p><p>​    除上图所示的保证之外，每次做训练的训练集也是极大化的，故模型的解也是最可靠的，自然得到的验证结果也是最可靠的。由此可见$E_{loocv}$是极佳方案，但当模型没有解析解，而是用迭代优化来求解时，该模型选择的方法因计算消耗太大而在实践中不常用。</p><ol><li><p>K-fold cross validation（经验值 K=10）：</p><p>$E<em>{CV} = \frac{1}{K} \sum</em>{i=1}^K E_{val}^{(i)}$</p></li></ol><p>### </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性模型&quot;&gt;&lt;a href=&quot;#线性模型&quot; class=&quot;headerlink&quot; title=&quot;线性模型&quot;&gt;&lt;/a&gt;线性模型&lt;/h1&gt;&lt;h2 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习基础</title>
    <link href="http://yoursite.com/2018/03/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/03/30/机器学习基础/</id>
    <published>2018-03-30T11:05:47.000Z</published>
    <updated>2018-04-12T12:17:50.617Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/ml/ml1.jpg" alt="ml_flow"></p><h2 id="基础术语"><a href="#基础术语" class="headerlink" title="基础术语"></a>基础术语</h2><table><thead><tr><th style="text-align:center">名称</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">模型</td><td style="text-align:left">计算模型$\mathcal{A}$（学习法则）与数学模型$\mathcal{H}$（假设集）的合称为机器学习模型</td></tr><tr><td style="text-align:center">$E_{in}$</td><td style="text-align:left">假设 h 在已得到的资料上与真实模式 f 的误差</td></tr><tr><td style="text-align:center">$E_{out}$</td><td style="text-align:left">假设 h 在未见过的资料上与真实模式 f 的误差</td></tr><tr><td style="text-align:center">h</td><td style="text-align:left">$\mathcal{A}$ 从假设集$\mathcal{H}$ 中取出的一个假设函数</td></tr><tr><td style="text-align:center">g</td><td style="text-align:left">机器学习模型最终确定的在当前任务中用于代替真实模式 f 的估计模式</td></tr><tr><td style="text-align:center"></td></tr></tbody></table><blockquote><p>若无特殊说明，一般模型一词特指数学模型$\mathcal{H}$ 。</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="ML-DM-statistics"><a href="#ML-DM-statistics" class="headerlink" title="ML / DM / statistics"></a>ML / DM / statistics</h3><ul><li>ML 与 DM 很难区分</li><li>ML 是利用资料计算出接近真实模式 f 的估计模式 g</li><li>statistics 是利用资料推断一个尚不知结果的进程的结果的概率</li></ul><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>当任务存在某种潜在模式，但不能很容易地程式化地总结出来时。（前提是与该模式有关的资料是要能够获取到的）</p><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><h2 id="实现的可行性"><a href="#实现的可行性" class="headerlink" title="实现的可行性"></a>实现的可行性</h2><blockquote><p>模型与数据集大小两者之间没有先确定谁再确定谁的先后顺序：因为成本问题，数据集自然希望需要得越少越好，当数据量过小时便不可选复杂度过高的模型；而为使模型误差能够足够小，模型复杂度便须足够高，高复杂度的模型需要高样本复杂度的数据。总之，这是一个<strong>数据成本与模型误差博弈的过程</strong>。</p></blockquote><h3 id="VC（Vapnik-Chervonenkis）维"><a href="#VC（Vapnik-Chervonenkis）维" class="headerlink" title="VC（Vapnik-Chervonenkis）维"></a>VC（Vapnik-Chervonenkis）维</h3><blockquote><p>$d_{VC}$ 的含义：</p><ol><li>模型自由参数的个数，或称为模型的自由度（向量$w$ 的维度）</li><li>模型的强度：表示模型什么时候还能够 shatter</li></ol><p>注意：机器学习模型的 VC 维指数学模型的有效 VC 维，会根据计算模型变化（如加入不同的正则化项）而变化；常说的模型复杂度是指数学模型有效 VC 维对应的复杂度。</p></blockquote><h4 id="断点（breakpoint）"><a href="#断点（breakpoint）" class="headerlink" title="断点（breakpoint）"></a>断点（breakpoint）</h4><ul><li>shatter：若模型包含某次 N 个输入样本可能出现的所有情况（即模型能产生至少 $2^N$ 种假设），则称该输入能被该模型 $\mathcal{H}$ shatter</li><li>成长函数 $m_{\mathcal{H}}(N)$：模型能产生的最多（相同样本量不同样本的情况下，模型会产生不同的假设个数，此处为最多）的假设个数关于样本量的函数</li><li>断点 breakpoint：若N=k，成长函数首次不是指数级时，称 k 为最小断点</li><li>VC 维：$d_{VC} = $（最小断点k ）- 1，即模型能 shatter 的最大样本数</li></ul><p>当断点出现后，模型的成长函数便与模型的细节（线性分类器还是圆形分类器等细节）无关了：当 $N \geqslant k$ 时，$m<em>{\mathcal{H}}(N)  \leqslant B(N,k)$ ，进而能得到 B(N, k) 的表格，由表格可推得 $B(N,k) \leqslant \sum^{k-1}</em>{i} C_N^i \leqslant N^{k-1}$ 。</p><h4 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h4><p>由<a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality" target="_blank" rel="noopener">霍夫丁不等式</a>（给出了训练集误差无法代表整体误差的概率上限）：<br>$$<br>P{ |E<em>{in}(h) - E</em>{out}(h)|&gt; \epsilon } \leqslant 2e^{-2 \epsilon^2 N}<br>$$<br>知，针对任意一个假设 h，只要取样容量 N 足够大，不好的取样发生的概率很小。</p><p>因为数据集的好坏应该是针对模型来说的，故只有下列式子足够小，才能说数据集是好的（<strong>从直观上来说<u>数据集好</u>是指手中的资料已经可以代表所有已知和未知的资料了</strong>）：<br>$$<br>P{ \exists h \epsilon \mathcal{H},  s.t. |E<em>{in}(h) - E</em>{out}(h)|&gt; \epsilon  } = P{\sum<em>{h \epsilon \mathcal{H}} [ |E</em>{in}(h) - E<em>{out}(h)|&gt; \epsilon ] }<br>$$<br>设模型能产生的假设个数为$M$，由和事件的概率$ \leqslant$ 概率的和得到不好的取样发生的概率为：<br>$$<br>P{\sum</em>{h \epsilon \mathcal{H}} [ |E<em>{in}(h) - E</em>{out}(h)|&gt; \epsilon ] }  \leqslant 2 M e^{-2 \epsilon^2 N}<br>\<br>^{用E<em>{in}’ 替代E</em>{out}}<br><em>{又{|E</em>{in}-E<em>{in}’| &gt; \epsilon / 2} \Leftrightarrow {|E</em>{in}-(E<em>{in}+E</em>{in}’)/2| &gt; \epsilon / 4} }<br>\Longrightarrow<br>\<br>\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  \leqslant 4(2N)^{d<em>{VC}} e^{-\frac{1}{8} \epsilon^2 N}<br>\<br>\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad = \delta \quad (VC bound)<br>$$<br>由 VC bound得到：<br>$$<br>\epsilon = \sqrt{\frac{8}{N} \ln (\frac{4(2N)^{d</em>{VC}}}{\delta})}<br>$$<br>一般将该值记为$\Omega(N, \mathcal{H}, \delta)$ ，称为模型复杂度，它是样本复杂度、模型的 VC 维和 VC bound 的函数。故在置信度为$1-\delta$ 的情况下，泛化误差$|E<em>{in}-E</em>{out} |  \leqslant  \Omega(N, \mathcal{H}, \delta)$ 。模型复杂度本质是由多个因素影响（<strong>此处说的模型是数学模型即假设集，该<u>模型的 VC 维</u>决定了它能 shatter 的最大样本数，模型复杂度越高能够 shatter 的样本数越大；<u>模型的 VC bound</u> 决定了该模型结果【预测结果与泛化误差合称为模型结果】的不可信度，模型复杂度越高模型结果的不可信度越低；<u>样本复杂度</u>决定了针对该模型手中样本的好坏，模型复杂度越高手中样本变坏的可能性越大，另一种理解为样本复杂度决定了该模型能产生的假设个数，模型复杂度越高能产生的假设个数越多</strong>）的函数，而泛化误差本质是在一定置信度下的一个数，二者恰巧在数量上相等。</p><blockquote><p>虽然给出预期的置信度、泛化误差和$d<em>{VC}$ 就能得到针对“模型能从样本中学到点什么东西”这件事模型所需的样本复杂度，但一般令$N \approx 10 d</em>{VC}$ 就足够了。</p></blockquote><h4 id="常见模型的-VC-维"><a href="#常见模型的-VC-维" class="headerlink" title="常见模型的 VC 维"></a>常见模型的 VC 维</h4><ul><li><p>举例：n 维的二分类感知机，VC 维为 n+1</p><ol><li>当样本量 N=n+1 时，$X \epsilon R^{(n+1)*(n+1)}$ ，存在能够被该模型 shatter 的样本：令样本 X 可逆，则任意一种二分类情况 y 都可以被一个 w 划分出，因为 $w = X^{-1}y$ ；</li><li>当样本量 N=n+2 时，$X \epsilon R^{(n+2)*(n+1)}$ ，没有一个能够被该模型 shatter 的样本：因为 n+2 个样本中总有一个样本能被其它 n+1 个样本线性表示，设线性表示的系数为 $a<em>1, … , a</em>{n+1}$ ，则模型无法产生 $(sign(a<em>1), …, sign(a</em>{n+1}), -1)$ 这种二分类情况。</li></ol><p>故VC 维为 n+1。</p></li></ul><h3 id="可行性分析"><a href="#可行性分析" class="headerlink" title="可行性分析"></a>可行性分析</h3><ol><li><p>no free lunch 定理：若只坚持 f 是未知的，而不作出任何假设，那么在已知资料以外的部分去说我一定学到了什么东西（即找到了能够满足在已知资料以外的部分 $g \approx f$ 的$g$ ）这件事是做不到的。故机器学习的模型一般都是有某种归纳偏好的。<br>$$<br>\downarrow 对已知和未知资料作出假设：所有数据均是独立同分布的<br>$$</p></li><li><p>若<strong>样本量够大，模型的 VC 维为有限值</strong>，则由霍夫丁不等式可认为样本内误差可以泛化到样本外误差，即 g 能够具有很好的泛化能力；又若计算模型能够从数学模型中找到使得<strong>样本内误差趋于零</strong>的假设 h 作为 g，则认为学习成功。</p></li></ol><h2 id="加入噪声后"><a href="#加入噪声后" class="headerlink" title="加入噪声后"></a>加入噪声后</h2><p>噪声是指标签中的噪声，来源多为：</p><ol><li>打标签过程人一时走神打错标签</li><li>打标签时不同的人有不同的标准</li><li>打标签的数据本身有噪声</li></ol><p>对于含有噪声的标签，可以认为产生样本的数据源从真实模式 f(X) 变成了 P(y|X) ，即加入了些微抖动，但这只是一个变量的替换，并不影响 VC bound 的成立，故依然认为能够成功学习。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>机器学习能否成功，就考虑两件事：</p><ol><li>数学<strong>模型复杂度</strong>是否足够高，以有能力使得训练集误差误差够小</li><li><strong>泛化误差</strong>（样本复杂度与模型 VC 维决定）是否足够低，以保证训练集误差能够代表整体误差。</li></ol><p>至于学习速度便是计算模型去考虑的事情了。</p><blockquote><p>在此可做个比拟：数学模型是天资，是人本身的智商，而计算模型是学习方法。方法用对了再加上高天资便能平步青云。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习基础&quot;&gt;&lt;a href=&quot;#机器学习基础&quot; class=&quot;headerlink&quot; title=&quot;机器学习基础&quot;&gt;&lt;/a&gt;机器学习基础&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/RedMudBUPT
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机硬件基础</title>
    <link href="http://yoursite.com/2018/03/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/03/29/计算机硬件基础/</id>
    <published>2018-03-29T01:27:43.000Z</published>
    <updated>2018-03-30T11:20:50.439Z</updated>
    
    <content type="html"><![CDATA[<h2 id="硬盘基础知识"><a href="#硬盘基础知识" class="headerlink" title="硬盘基础知识"></a>硬盘基础知识</h2><p>主引导扇区（512字节）包括三部分：</p><ul><li>MBR：主引导记录或主引导程序，用于硬盘启动时将系统控制转给指定的操作系统。</li><li>4个分区表（总64字节）：可为三个主分区表和一个扩展分区表或者四个主分区表。扩展分区表作为特殊的存在，标定了任意数量个逻辑分区表的位置；而主分区表和逻辑分区表标定的是主分区和逻辑分区的位置。</li><li>结束标志（2字节）</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;硬盘基础知识&quot;&gt;&lt;a href=&quot;#硬盘基础知识&quot; class=&quot;headerlink&quot; title=&quot;硬盘基础知识&quot;&gt;&lt;/a&gt;硬盘基础知识&lt;/h2&gt;&lt;p&gt;主引导扇区（512字节）包括三部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MBR：主引导记录或主引导程序，用于硬盘启动
      
    
    </summary>
    
      <category term="cs" scheme="http://yoursite.com/categories/cs/"/>
    
    
  </entry>
  
  <entry>
    <title>信息论基础</title>
    <link href="http://yoursite.com/2018/03/29/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/03/29/信息论基础/</id>
    <published>2018-03-29T01:26:58.000Z</published>
    <updated>2018-03-29T03:12:54.739Z</updated>
    
    <content type="html"><![CDATA[<h1 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h1><blockquote><p>熵可以认为是随机变量的数字特征，其含义为为确定某一随机变量所需信息量的平均。</p><p>设$ X \sim p(x)$ ，则 $H(X) = H(p) = -\sum p(x) \log_2 p(x)$ ，单位为比特。</p><p>若设 $Y = -\log_2 p(X)$ ，则$H(X)=E(Y)$ ，即随机变量的熵为随机变量函数的期望。$ -\log_2 p(X)$ <strong>意为为确定随机变量 $X$ 的值所需的信息量，</strong>若$p(x)$ 越小，则所需信息量越大。</p><p>综上，熵$H(X)$ 是 $X$ 信息量的期望。</p><blockquote><p>注：$X$ 指随机变量，$x$ 指随机变量取的值。</p></blockquote></blockquote><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li>联合熵：描述二维随机变量$(X_1, X_2)$的熵，$(X_1 , X_2) \sim p(x_1, x_2)$ 。</li><li>条件熵：描述条件随机变量$(X_2 | X_1)$的熵，$H(X_2 | X_1) = E(H(X_2| X_1=x_1))$ 。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;熵&quot;&gt;&lt;a href=&quot;#熵&quot; class=&quot;headerlink&quot; title=&quot;熵&quot;&gt;&lt;/a&gt;熵&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;熵可以认为是随机变量的数字特征，其含义为为确定某一随机变量所需信息量的平均。&lt;/p&gt;
&lt;p&gt;设$ X \sim p(x)$
      
    
    </summary>
    
      <category term="数学" scheme="http://yoursite.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>凸优化</title>
    <link href="http://yoursite.com/2018/03/14/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/03/14/凸优化/</id>
    <published>2018-03-14T13:48:31.000Z</published>
    <updated>2018-04-10T10:52:02.317Z</updated>
    
    <content type="html"><![CDATA[<h2 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h2><blockquote><p>集合$C$ 内任意两点间的线段上的点仍在该集合内，则称该集合为凸集。表示为$\theta x+(1-\theta)y \in C, \theta \in (0,1)$ ，其中$\theta x+(1-\theta)y$ 称为点$x$ 与$y$ 的凸组合。</p></blockquote><h2 id="无约束最优化"><a href="#无约束最优化" class="headerlink" title="无约束最优化"></a>无约束最优化</h2><h3 id="数值优化"><a href="#数值优化" class="headerlink" title="数值优化"></a>数值优化</h3><h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><blockquote><p>本质为找函数的零点，用在最优化上便是找函数导数的零点。</p></blockquote><p>$$<br>w_{t+1} = w<em>t - (\frac{\partial ^2 L}{\partial w \partial w^T})^{-1} \frac{\partial L}{\partial w}|</em>{w=w_t}<br>$$</p><p>牛顿法相比起梯度往往法收敛速度更快，特别是迭代值距离收敛值比较近的时候，每次迭代都能使误差变成原来的平方，但是在高维时矩阵的逆计算会非常耗时。</p><h3 id="解析优化"><a href="#解析优化" class="headerlink" title="解析优化"></a>解析优化</h3><h2 id="有约束最优化"><a href="#有约束最优化" class="headerlink" title="有约束最优化"></a>有约束最优化</h2><h3 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h3><blockquote><p>通过引入拉格朗日乘子，可将有 d 个变量与 k 个约束条件的最优化问题转化为 d+k 个变量的无约束优化问题。</p></blockquote><h4 id="原理（以等式约束为例）"><a href="#原理（以等式约束为例）" class="headerlink" title="原理（以等式约束为例）"></a>原理（以等式约束为例）</h4><p>$$<br>\min_x f(x),<br>\<br>s.t. \quad g(x)=0.<br>$$</p><p>设 x 为 d 维向量，则约束曲面 $g(x)=0$是 d-1 维曲面，目标函数$f(x)$是 d 维曲面。</p><ul><li>因曲面梯度与自身法线共线，所以约束曲面上任意点的梯度$\nabla g(x)$必正交于约束曲面；</li><li>又目标函数在约束曲面上的最优点$x^<em>$对应的梯度$\nabla f(x^</em>)$必正交于约束曲面（不正交意味着不相切，不相切意味着至少相交于两个点，故可继续沿反梯度方向$-\nabla f(x) $找到更优解），故必存在$\lambda \neq 0$使得</li></ul><p>$$<br>\nabla f(x) + \lambda \nabla g(x) = 0,<br>$$<br>由此得到等价的无约束最优化问题：<br>$$<br>\min_x f(x)+ \lambda g(x). \quad (\lambda \neq 0)<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;凸集&quot;&gt;&lt;a href=&quot;#凸集&quot; class=&quot;headerlink&quot; title=&quot;凸集&quot;&gt;&lt;/a&gt;凸集&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;集合$C$ 内任意两点间的线段上的点仍在该集合内，则称该集合为凸集。表示为$\theta x+(1-\theta)
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>前馈神经网络</title>
    <link href="http://yoursite.com/2018/03/11/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/03/11/前馈神经网络/</id>
    <published>2018-03-11T10:16:33.000Z</published>
    <updated>2018-04-16T13:29:09.197Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前馈神经网络-feed-forward-nueral-network"><a href="#前馈神经网络-feed-forward-nueral-network" class="headerlink" title="前馈神经网络 (feed-forward nueral network)"></a>前馈神经网络 (feed-forward nueral network)</h1><blockquote><p>前馈神经网络一般有两种，linear perceptron network 和 RBF network，该文主要叙述前一种，其学习规则是梯度下降法，是一种无约束的最优化算法。</p><p><a href="https://github.com/bkseastone/Neural-Networks-for-Machine-Learning/blob/master/feedforwardNN/feedforwardNN.py" target="_blank" rel="noopener">NN示例代码</a></p></blockquote><p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/nn_net1.jpg" alt="nn_net1"></p><h2 id="符号及标记"><a href="#符号及标记" class="headerlink" title="符号及标记"></a>符号及标记</h2><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">$L$</td><td style="text-align:left">代价函数 / 损失函数 / 最优化目标函数</td></tr><tr><td style="text-align:center">$w$</td><td style="text-align:left">“轴突”权重</td></tr><tr><td style="text-align:center">$\sigma$</td><td style="text-align:left">神经元“树突”的激活函数</td></tr><tr><td style="text-align:center">$z$</td><td style="text-align:left">带权输入 $z^l=w^{l-1}a^{l-1}$ ，即从上个神经元轴突传到该神经元树突上的值</td></tr><tr><td style="text-align:center">$a$</td><td style="text-align:left">神经元的激活值</td></tr><tr><td style="text-align:center">$\delta$</td><td style="text-align:left">中间变量，称为某层的误差，专用于反向传播算法</td></tr></tbody></table><blockquote><p><em>注 1</em>：零层为真实数据，尚未前传，自然没有所谓的误差 $\delta^0$；因第零层的“轴突”上的权重 $w^0$ 尚未学习成功而导致的第一层的神经元的带权输入 $z^1$ 的误差，记为 $\delta^1$ 。</p><p><em>注 2</em>：每层“轴突”上的权重$w$ 的行数为希望学到的模式数，列数为输入数据的维度。</p><p><em>注 3</em>：做某一层的前传的时候，那一层的恒一神经元（用于模式中的偏置）才会被重新激活；当做为输出层时，该层的恒一神经元处于闭塞状态，是看不见的。当做某一层的反传的时候，是用该层神经元的误差$\delta$ 求<em>loss</em>对上一层“轴突”权重的偏导，而该层的恒一神经元没有所谓的误差，上一层的恒一神经元却有“轴突”权重，自然也有偏导，故只有求“轴突”权重偏导的那一层的恒一神经元会被重新激活，其余均处于闭塞状态。</p></blockquote><h2 id="学习法则"><a href="#学习法则" class="headerlink" title="学习法则"></a>学习法则</h2><blockquote><p>若 $y$ 与 $(x_1,\dotsb,x_m)$ 线性相关，且采样没有噪声，则直接采$m$个样本点求解线性方程就能得到参数 $\omega$ 的唯一解。为应对非线性相关的数据，采用迭代最优化（iterative optimization）的方法。</p></blockquote><h3 id="参数更新：梯度下降法"><a href="#参数更新：梯度下降法" class="headerlink" title="参数更新：梯度下降法"></a>参数更新：梯度下降法</h3><blockquote><p>梯度是个向量，指函数变化增加最快的地方，故沿负梯度的方向便能到达函数的极小值处。</p></blockquote><p>迭代优化的参数更新通式为：<br>$$<br>w(t+1) \leftarrow w(t)+\alpha \frac{v(t)}{||v(t)||} \quad , \alpha &gt; 0<br>$$<br>其中参数的确定由降低 <em>loss</em> 函数的方法确定。对 <em>loss</em> 函数进行一阶泰勒展开：<br>$$<br>L(w(t+1)) \approx L(w(t))+ \alpha v(t)^T \nabla L(w(t))<br>$$<br>要使 $L(w(t+1))$ 最小，须使 $ v(t)^T \nabla L(w(t))$ 最小，故令 $v(t)= -\nabla L(w(t))$ 。为使得学习率在陡的地方大，缓的地方小，令 $\alpha \propto ||\nabla L(w(t))||$ ，从而得到梯度下降法的参数更新式：<br>$$<br>w(t+1) \leftarrow w(t)-\alpha_0 \nabla L(w(t))<br>$$<br>其中 $\alpha_0$ 称为 fixed learning rate，而真实的 learning rate $\alpha$ 的大小随梯度的大小变化而变化。</p><h3 id="求梯度：反向传播算法"><a href="#求梯度：反向传播算法" class="headerlink" title="求梯度：反向传播算法"></a>求梯度：反向传播算法</h3><p>$$<br>\delta^l=(w^l\delta^{l+1})*\sigma’_{z^l}<br>\<br>\frac{\partial L}{\partial w^{l-1}}=\frac{1}{n}\delta^{l}(a^{l-1})^T<br>$$</p><p>在用矩阵编程计算梯度时，无需考虑具体矩阵乘积的细节和含义，在得到反向传播的标量表达式后，只需依照两条规则即可写出梯度的矩阵算式：</p><ol><li>依据标量表达式确定算式的结构；</li><li>依据<em>loss</em>对该层参数偏导的形状调整矩阵的顺序和形状。</li></ol><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><h4 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h4><p>待续</p><h4 id="加快-mini-batch-训练的3种方法"><a href="#加快-mini-batch-训练的3种方法" class="headerlink" title="加快 mini-batch 训练的3种方法"></a>加快 mini-batch 训练的3种方法</h4><h5 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h5><p>$$<br>M(t) = \alpha M(t-1) - \epsilon \frac{\partial L}{\partial w}<br>\<br>w += M(t)<br>$$</p><ul><li><p>动量法能够减小高曲率方向上的震荡，使得小球尽快地损失掉重力势能。窃以为，公式结合物理原则，应为（尚未测试）：<br>$$<br>v(t) = \alpha M(t-1) - \epsilon (1 + \frac{1}{|\frac{\partial L}{\partial w}|})<br>\<br>w+=v(t)<br>\<br>M(t) = \frac{v(t)}{\alpha}<br>$$</p></li><li><p>Ilya Sutskever 在2012年提出了一种优化版本：先在历史累计方向上前进一大步，然后在新位置上计算梯度并修正方向。可以这么理解，最好犯错之后去改正它。</p></li></ul><h5 id="自适应学习率"><a href="#自适应学习率" class="headerlink" title="自适应学习率"></a>自适应学习率</h5><p>每个神经元入度的不同导致了流入不同神经元的“树突”权值的最佳学习率各不相同。当入度很大时，每个“树突”权值改变一点点，累积的改变量就很大了，很容易过量；而当入度很小时反之。所以一般采用一个全局学习率，然后根据对每个神经元各自做适当调整：<br>$$<br>w+=-\epsilon g \frac{\partial L}{\partial w}<br>$$<br>初始化局部 $g=1$ ，如果下次该权值的梯度符号不变则增加 $g+=0.05$ ，否则减小为 $g*=0.95$ 。</p><blockquote><p>注意：</p><ol><li>将 $g$ 限制在某个合理的范围，比如[0.1, 10] 或 [.01, 100]。</li><li>使用 full-batch 或很大的 mini-batch，毕竟这样保证了梯度的符号不易受 mini-batch 的采样误差影响。</li><li>综合自适应学习率和动量更新法，以当前梯度和当前速度的符号来决策 $g$ 的变化。</li></ol></blockquote><h5 id="RMSProp：将梯度除以历史数量级"><a href="#RMSProp：将梯度除以历史数量级" class="headerlink" title="RMSProp：将梯度除以历史数量级"></a>RMSProp：将梯度除以历史数量级</h5><p>全局学习率之所以难选，主要是因为每个期望的最终的权值的数量级相差巨大。在 full batch 中，可以利用梯度的符号来替代权值的更新量，从而解决这个问题。<a href="https://zhidao.baidu.com/question/1367991976404469819.html" target="_blank" rel="noopener">RProp</a>结合了“只用符号”和“自适应学习率”的思想，但它违反了 SGD 的中心思想（当学习率很小的时候，权值更新量其实是当前mini-batch的梯度和历史梯度的平均。举例来讲，假设某个权值在9个批次中的梯度是+0.1，在第10个批次中的梯度是-0.9，我们希望这个权值大致不变。），故不适用于 mini-batch。而RMSProp便融合了mini-batch的高效性、mini-batch间的有效平均和RProp的稳定性。</p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li>对小数据集（10000以内）或者没有多少重复数据的大数据集，应当使用full-batch的一些优化方法，如Conjugate gradient、 LBFGS。然后试试adaptive learning rates, rprop …，它们是为神经网络而设计的方法。</li></ul><ul><li>对含有重复数据的大数据集，应当使用mini-batch。尝试动量法SGD，rmsprop 或LeCun的最新研究成果。</li></ul><h2 id="常用的激活函数及其对应的-loss-函数"><a href="#常用的激活函数及其对应的-loss-函数" class="headerlink" title="常用的激活函数及其对应的 loss 函数"></a>常用的激活函数及其对应的 <em>loss</em> 函数</h2><blockquote><p>假设激活函数$\sigma(z)=z$，则 $L(\omega , b)=\frac{1}{n}\sum ||y- { \omega<em>{L-1}[\omega</em>{L-2}(…x)] }||_2$，而我们平常所说的<em>loss</em>函数是与网络结构无关的“基础<em>loss</em>函数”。</p></blockquote><h3 id="线性激活函数与均方差-loss-函数"><a href="#线性激活函数与均方差-loss-函数" class="headerlink" title="线性激活函数与均方差 loss 函数"></a>线性激活函数与均方差 <em>loss</em> 函数</h3><p>$$<br>\sigma(z)=z<br>\<br>L=\frac{1}{2n}\sum_x||a(x)-y(x)||^2<br>$$</p><ul><li>$\delta^L=a-y$ ；</li><li>$\delta^l=w^l\delta^{l+1},(l=1,…,L-1)$（因为 $\sigma_z’=1$ ）。</li></ul><h3 id="sigmod-激活函数与交叉熵-loss-函数"><a href="#sigmod-激活函数与交叉熵-loss-函数" class="headerlink" title="sigmod 激活函数与交叉熵 loss 函数"></a>sigmod 激活函数与交叉熵 <em>loss</em> 函数</h3><p>$$<br>\sigma(z)=\frac{1}{1+e^{-z}}<br>\<br>L=-\frac{1}{n}\sum_x[y\ln a+(1-y)\ln (1-a)]<br>$$</p><ul><li>$\delta^L=a-y$ ；</li><li>$\delta^l=w^l\delta^{l+1}<em>a^l</em>(1-a^l)$ ，其中“ $*$ ”是<a href="https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/5446029?fr=aladdin#4" target="_blank" rel="noopener">Hadamard积</a>。</li></ul><blockquote><ol><li><p>用交叉熵而非均方差的原因是为了解决神经元饱和问题（指☞神经元因处于$\sigma’(z)$ 值很小的激活值位置而导致的梯度极小，进而带权步长极小的问题），其表现为在训练初期（即误差比较大时）<em>loss</em>下降却十分缓慢。</p></li><li><p>可根据激活函数和希望的梯度形式反推得到所需的<em>loss</em>函数，见神经网络与深度学习（Michael Nielsen）3.1.3节。</p></li><li><p>最小化交叉熵 <em>loss</em> 函数等价于最大化以 sigmod 为参数的对数似然：<br>$$<br>\mathrm{likelyhood} = y<em>i  \ln[ \prod</em>{i=1}^N \sigma(z_i)] +(1-y<em>i) \ln[ \prod</em>{i=1}^N (1-\sigma(z_i))]<br>$$</p></li></ol></blockquote><h3 id="softmax-激活函数与对数似然-loss-函数（right-损失函数）"><a href="#softmax-激活函数与对数似然-loss-函数（right-损失函数）" class="headerlink" title="softmax 激活函数与对数似然 loss 函数（right 损失函数）"></a>softmax 激活函数与对数似然 <em>loss</em> 函数（right 损失函数）</h3><p>$$<br>\sigma(z_i)=\frac{e^{z_i}}{\sum_ie^{z_i}}<br>\<br>L=-\frac{1}{n}\sum_x\ln a_I,(a_I为真实类别对应神经元的激活值)<br>$$</p><ul><li>$\delta^L=a-y$ ；</li><li>$\delta^l=w^l\delta^{l+1}<em>a^l</em>(1-a^l)$ ，其中“ $*$ ”是<a href="https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/5446029?fr=aladdin#4" target="_blank" rel="noopener">Hadamard积</a>。</li></ul><blockquote><ol><li>因loss只求在真实类别神经元上的偏差，而激活函数的分母中所有神经元都包含，故$\delta^L$的求解分为两部分</li><li>设</li></ol></blockquote><h2 id="对该学习算法的一些理解"><a href="#对该学习算法的一些理解" class="headerlink" title="对该学习算法的一些理解"></a>对该学习算法的一些理解</h2><ol><li><ul><li>在梯度回传的过程中，$\omega$向量可能会变得非常大，则带权步长的移动只会引起在那个方向上微小的变化，以致很难有效地探索各种$\omega$模式。（大分量相对不怎么移动，小分量却相对移动很大，此处相对是指与分量自身相比，故会卡在某个方向上，以致很难有效地探索。）正则化的效果是让网络倾向于学习小一点的权重，让$\omega$只负责方向，而让$b$负责激活空间的位置。</li><li>另一个角度是，更小的权重意味着网络的行为不会因为噪声而改变太大，一个无规范化的网络可以使用大的权重来学习包含训练数据中的噪声的大量信息的复杂模型，而规范化的网络受限于根据训练数据中常见的模式来构造相对简单的模型，能够抵抗训练数据中的噪声的影响。</li></ul></li><li>在迭代过程中，还会出现神经元饱和问题，从梯度公式的角度想，是$\sigma’(z)$或$a$过小导致的梯度过小，从而引起学习缓慢的问题，解决方法便是构造合适的函数将梯度公式中的$\sigma’(z)$约掉；从网络的$loss$函数角度想，是寻找极低点时中途出现了原地踱步（小梯度）的情况，解决方法便是选用不同的激活函数与基础$loss$函数的搭配，从而得到形状更好的网络$loss$函数；从模式的可激活空间角度想，是样本$x$在模式$\omega$的激活空间上的分量值（带权输入）处于激活函数的平缓处（极低变化率）导致的模式$\omega$寻找进度迟缓，解决方法便是消除激活函数变化率对模式$\omega$的迭代寻找的影响。</li><li>​</li></ol><h2 id="实际应用中遇到的问题"><a href="#实际应用中遇到的问题" class="headerlink" title="实际应用中遇到的问题"></a>实际应用中遇到的问题</h2><h3 id="最优化问题：更新频率与幅度究竟应该为多大"><a href="#最优化问题：更新频率与幅度究竟应该为多大" class="headerlink" title="最优化问题：更新频率与幅度究竟应该为多大"></a>最优化问题：更新频率与幅度究竟应该为多大</h3><h3 id="泛化问题：如何防止过拟合？数据的两种噪音"><a href="#泛化问题：如何防止过拟合？数据的两种噪音" class="headerlink" title="泛化问题：如何防止过拟合？数据的两种噪音"></a>泛化问题：如何防止过拟合？数据的两种噪音</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前馈神经网络-feed-forward-nueral-network&quot;&gt;&lt;a href=&quot;#前馈神经网络-feed-forward-nueral-network&quot; class=&quot;headerlink&quot; title=&quot;前馈神经网络 (feed-forward nue
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>numpy概览</title>
    <link href="http://yoursite.com/2018/03/08/numpy%E6%A6%82%E8%A7%88/"/>
    <id>http://yoursite.com/2018/03/08/numpy概览/</id>
    <published>2018-03-08T13:46:27.000Z</published>
    <updated>2018-03-08T14:46:38.352Z</updated>
    
    <content type="html"><![CDATA[<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><h2 id="numpy数据类型"><a href="#numpy数据类型" class="headerlink" title="numpy数据类型"></a>numpy数据类型</h2><blockquote><p><a href="https://www.cnblogs.com/busui/p/7283137.html?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="noopener">python中的数据类型详解</a></p></blockquote><p>numpy中自定义的数据类型需用元组表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">np.array([[ ( np.array([...]), np.array([...]) )</div><div class="line">]], dtype=[(<span class="string">'inputs'</span>, <span class="string">'O'</span>), (<span class="string">'targets'</span>, <span class="string">'O'</span>)])</div></pre></td></tr></table></figure><p>用<code>[[]]</code>将自定义的数据类型包裹起来的优点是，可用<code>[0,0]</code>的索引方式将<code>object</code>类型的数据解析出来。</p><p><strong>ref &gt;&gt;</strong> <a href="http://blog.csdn.net/qq_16234613/article/details/65935279" target="_blank" rel="noopener">numpy dtype类详解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;术语&quot;&gt;&lt;a href=&quot;#术语&quot; class=&quot;headerlink&quot; title=&quot;术语&quot;&gt;&lt;/a&gt;术语&lt;/h2&gt;&lt;h2 id=&quot;numpy数据类型&quot;&gt;&lt;a href=&quot;#numpy数据类型&quot; class=&quot;headerlink&quot; title=&quot;numpy数据
      
    
    </summary>
    
      <category term="script" scheme="http://yoursite.com/categories/script/"/>
    
    
  </entry>
  
  <entry>
    <title>感知机学习算法</title>
    <link href="http://yoursite.com/2018/03/06/%E6%84%9F%E7%9F%A5%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2018/03/06/感知器学习算法/</id>
    <published>2018-03-06T12:17:53.000Z</published>
    <updated>2018-03-31T07:30:07.924Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PLA-perceptron-learning-algorithm"><a href="#PLA-perceptron-learning-algorithm" class="headerlink" title="PLA(perceptron learning algorithm)"></a>PLA(perceptron learning algorithm)</h1><blockquote><p>二分类感知机可简单理解为对输入向量线性组合取符号</p><p><a href="https://github.com/bkseastone/Neural-Networks-for-Machine-Learning/blob/master/assignment1/perceptron.py" target="_blank" rel="noopener">PLA示例代码</a></p></blockquote><h2 id="线性可分问题中感知机的收敛性"><a href="#线性可分问题中感知机的收敛性" class="headerlink" title="线性可分问题中感知机的收敛性"></a>线性可分问题中感知机的收敛性</h2><table><thead><tr><th style="text-align:center">符号</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">$w_{t}$</td><td style="text-align:left">第 t 次错误修正后的权重向量</td></tr><tr><td style="text-align:center">$(x_t,y_t)$</td><td style="text-align:left">第 t 次发现的错误样本点</td></tr><tr><td style="text-align:center">$w_f$</td><td style="text-align:left">真实模式 f 对应的最佳权重向量</td></tr></tbody></table><p>有所有样本中的最大长度为$R^2=\max_n||x_n||^2$，设$\rho=\min_n {y_n\frac{w_f^T}{||w_f}x_n}$，则<br>$$<br>w<em>f^Tw</em>{t+1}=w_f^T(w<em>t+y</em>{t+1}x_{t+1})=w_f^Tw<em>t+y</em>{t+1}w<em>f^Tx</em>{t+1}\geqslant w_f^Tw_t+\rho ||w<em>f||,<br>\<br>||w</em>{t+1}||^2=||w<em>t+y</em>{t+1}x_{t+1}||^2\leqslant ||w_t||^2+R^2.<br>$$<br>假设找到最佳权重向量需修正错误T次，则由以上两组迭代不等式和初始条件$w_0=0$得到<br>$$<br>w_f^Tw_T\geqslant T\rho ||w_f||,<br>\<br>||w_f^T||^2\leqslant TR^2.<br>$$<br>进而有<br>$$<br>(\frac{w_f^T}{||w_f||}\frac{w_T}{||w_T||})^2\geqslant TC \quad 其中C=\frac{\rho^2}{R^2},为一常量.<br>$$<br>故$T\leqslant \frac{R^2}{\rho^2}$，即对于有限样本的线性可分问题，PLA 收敛。</p><h2 id="感知机的学习法则"><a href="#感知机的学习法则" class="headerlink" title="感知机的学习法则"></a>感知机的学习法则</h2><blockquote><p>所谓学习法则，就是权值更新的策略，即在迭代过程中如何进行权值更新。</p></blockquote><ul><li>感知器法则：依照<strong>知错就改</strong>的演算策略寻找最佳的权重向量（学习到的是各个特征的权重），但若样本线性不可分则不能收敛；</li><li>$\Delta$法则：用最小二乘法、牛顿迭代法或梯度下降法等最优化方法寻找最小的输出误差（无法说清学习到的是什么，只是冲着最优结果去的），但对于误差曲面被拉长的情况进行学习较为困难。</li></ul><h2 id="感知机的局限"><a href="#感知机的局限" class="headerlink" title="感知机的局限"></a>感知机的局限</h2><p>感知机模式识别只学习特征权重，特征探测器需要手写，即特征需人工提取，故较依赖于人工提取特征的好坏，而该能力较依赖工作者特殊领域的专业经验。由此，发展出了第二代神经网络，即加入了隐藏层，从而实现了特征检测模块的训练，即不但学习特征权重还可以学习特征表示。</p><blockquote><p>注：学习流入 hidden units 的 weight 其实就是学习feature。对于如何表达概念目前有三种观点：</p><ol><li>特征论：概念由一系列特征表示，便于解释概念之间的相似性、便于将概念表示为向量；</li><li>结构主义论：概念不是孤立的，是关系图谱中的一个节点，由与其它概念的关系决定；</li><li>神经网络利用特征向量构成关系图谱：许多神经元表达一个概念，一个神经元同时参与很多个概念的表示，这种多对多的表示被称为“distributed representation”。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;PLA-perceptron-learning-algorithm&quot;&gt;&lt;a href=&quot;#PLA-perceptron-learning-algorithm&quot; class=&quot;headerlink&quot; title=&quot;PLA(perceptron learning al
      
    
    </summary>
    
      <category term="神经网络" scheme="http://yoursite.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>概率论基础.md</title>
    <link href="http://yoursite.com/2018/03/04/%E6%A6%82%E7%8E%87%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/03/04/概率论基础/</id>
    <published>2018-03-04T11:41:44.000Z</published>
    <updated>2018-04-10T10:17:56.207Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本假设"><a href="#基本假设" class="headerlink" title="基本假设"></a>基本假设</h1><p>为解决贝特朗悖论出现的问题，作以下三点假设（概率论公理）：</p><ul><li>样本空间$\Omega$：一个随机实验的所有可能输出的集合；</li><li>事件空间$\mathcal{F}$：$\mathcal{F}$为$\Omega$上的$\sigma$代数，是概率论中的<strong>定义域</strong>；其元素为事件，事件是$\Omega$中某些样本的集合；</li><li>测度$P$：$\mathcal{F}$上的测度$P:\mathcal{F}\to R$，是$\sigma$可加且非负的集合函数。</li></ul><h1 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h1><h1 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h1><blockquote><p>$\xi$ 的数学期望可理解为<strong><em>在 $\Omega$ 上的加权和</em></strong>（权重为概率值，对应于常规均值中某数出现的频率，有的资料称之为概率平均），采用的计算方法为<a href="https://www.zhihu.com/question/28700668?sort=created" target="_blank" rel="noopener">L-S积分</a>：如设$\xi=\sum_{i=1}^na<em>i \chi</em>{A_i}$ ，即将事件$A_i$ 映射为实数$a<em>i$ ，则$E(\xi)=\int</em>{\Omega} \xi(\omega) \mathrm{d}p(\omega)=\sum<em>{i=1}^n \int</em>{\Omega} a<em>i \chi</em>{A_i} \mathrm{d}p(\omega)$ ，即将$\Omega$ 中不可分事件$\omega$ 按值域划分为n类（每一类中的$p(\omega)$ 都相等，假设$A_1$ 中有10个$\omega$ ，则$p(A<em>1)=10*\mathrm{d}p(\omega)$ ），并在每一类中积分出一个值，最后将这n个值求和。当n为有限值，结果为$\sum</em>{i=1}^n a_i p(A<em>i)$ ；当$n\to \inf$ ，结果为$\int</em>{-\inf}^{+\inf} a f(a)\mathrm{d}a$ 。</p><p>综上，将数学期望作$\xi$ 在$\Omega$ 上的“加权和”看待。</p></blockquote><h2 id="条件数学期望"><a href="#条件数学期望" class="headerlink" title="条件数学期望"></a>条件数学期望</h2><blockquote><ul><li>$E(\xi)$ ：<ol><li>$=E(\xi \chi_{\Omega})$ ：在$\Omega$ 上$\xi$ 的加权和</li><li>$=E(\xi | \Omega)$ ：在$\Omega$ 上$\xi$ 的加权平均</li></ol></li><li>$E(\xi \chi<em>A)$ ：在$A$ 上$\xi$ 的加权和（实质为在$\Omega$ 上$\xi \chi</em>{A}$ 的加权平均，为方便思考，如此定义其含义）</li><li>$E(\xi | A)$ ：$\xi$ 在$A$ 上的加权和，然后在$A$ 上取平均，等于$\frac{E(\xi \chi_A)}{P(A)}$ </li><li>$P(A)$ ：$\chi_A$ 在$\Omega$ 上的加权和</li><li>$P(A | B)$ ：$\chi_A$ 在$B$ 上的加权平均</li></ul><blockquote><p>总结：</p><ol><li>$P(AB)=E( \chi_A \chi_B)$ ：单纯地在$A$ 、$B$ 交集上的加权和（也可认为在$\Omega$ 上做的归一化）</li><li>$P(A|B)=E(\chi_A |B)$ ：对在$A$ 、$B$ 交集上的加权和还做了归一化，谁为条件便是在哪个集上做的归一化（A、B独立的情况暂不考虑）</li><li>归一化：在哪个集上做归一化便是将全集缩减为了那个集</li></ol></blockquote></blockquote><ul><li><strong>全数学期望公式</strong></li></ul><p>$$<br>\sum<em>{i=1}^n E(\xi \chi</em>{B_i}) = E(\xi \chi<em>B)<br>\<br>\Rightarrow \sum</em>{i=1}^n P(B_i)E(\xi | B_i) = P(B)E(\xi | B)<br>$$</p><p>  将条件期望乘上条件的概率，可看做是将这个区域上的均值<strong>取消求平均</strong>的操作</p><ul><li><strong>全概率公式</strong></li></ul><p>$$<br>E(\chi<em>A)=\sum</em>{i=1}^n E(\chi<em>A \chi</em>{B<em>i})<br>\<br>\Rightarrow P(A) = \sum</em>{i=i}^n P(A | B_i) P(B_i)<br>$$</p><ul><li><strong>贝叶斯法则</strong></li></ul><p>$$<br>P(B<em>i | A) = \frac{E(\chi</em>{B_i} \chi_A)}{P(A)} = \frac{P(A|B_i)P(B_i)}{P(A)}<br>$$</p><ol><li>$A$ 为出现的新事件，$B_i$ 为欲求其概率的事件，$P(B_i)$ 为先验概率，多由经验知识得到；</li><li>​由该法则可得到基于最小错误率的贝叶斯决策规则：对于出现的$A$ ，$P(A|B_i)P(B_i)$ 最大的$B_i$ 为最可能发生的事件。其中，$A$ 为被归一化（该概念见条件数学期望的引注）到的集合，在比较大小时可去掉，不影响大小比较的结果。</li></ol><h1 id="机器学习中常用的方法"><a href="#机器学习中常用的方法" class="headerlink" title="机器学习中常用的方法"></a>机器学习中常用的方法</h1><h2 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h2><blockquote><p><a href="http://www.konstantinkashin.com/notes/stat/Maximum_Likelihood_Estimation.pdf" target="_blank" rel="noopener">Maximum_Likelihood_Estimation</a></p></blockquote><p>数学中似然函数 = $\prod_ip_i$（其中 $p_i$ 为样本$x_i$ 的概率，一般能用一个含待估计的参数和 $x_i$ 表示的解析式表示）表示已出现的样本出现的概率，而已出现的样本通常为最可能出现的，故可通过最大化该似然函数来估计想要的参数。有时会对似然函数取对数以简化运算。</p><p>在机器学习中常用最大化以假设函数$h$ 为参数的似然函数的方法使得假设函数$h$ 逼近隐藏模式$f$ 。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本假设&quot;&gt;&lt;a href=&quot;#基本假设&quot; class=&quot;headerlink&quot; title=&quot;基本假设&quot;&gt;&lt;/a&gt;基本假设&lt;/h1&gt;&lt;p&gt;为解决贝特朗悖论出现的问题，作以下三点假设（概率论公理）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;样本空间$\Omega$：一个随机实验
      
    
    </summary>
    
      <category term="数学" scheme="http://yoursite.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>linux环境编程基础</title>
    <link href="http://yoursite.com/2018/03/01/linux%E7%8E%AF%E5%A2%83%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/03/01/linux环境编程基础/</id>
    <published>2018-03-01T12:22:46.000Z</published>
    <updated>2018-03-31T09:53:39.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内核"><a href="#内核" class="headerlink" title="内核"></a>内核</h2><p>待续</p><h2 id="文件描述符"><a href="#文件描述符" class="headerlink" title="文件描述符"></a>文件描述符</h2><p>内核为每个进程维护一个文件打开记录表，文件描述符为该文件在表中的索引值。</p><h2 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h2><p>采用引用计数方式访问文件，当引用计数器为零时，内存管理机制便会对其进行垃圾回收。</p><p>对于以下选中的情况，引用计数器会自加1：</p><p>-[x] 创建文件</p><p>-[x] 对该文件建立一个硬链接</p><p>-[ ] 对该文件建立一个软链接</p><p>-[x] 每有一个进程访问该文件</p><p>一个文件包括三项，目录项（<em>dentry</em>结构）、索引节点（<em>inode</em>结构）和文件数据。</p><ol><li>目录项包括文件名和指向索引节点的指针等信息（当用<em>unlink</em>函数删除该项时，便在该目录中看不见该文件）；</li><li>索引节点包括链接数、文件所有者、文件在磁盘的位置等文件属性；</li><li>文件数据便是在磁盘上的数据块。</li></ol><p><strong>当对该文件建立硬链接（<code>ln src src_ln</code>）时，会额外创建<em>dentry</em>结构和<em>inode</em>结构，但两者的节点号一样（节点号可认为是文件数据的地址）；而当建立符号链接（<code>ln -s src src_ln</code>）时，则是额外创建了一个文本文件，里面包含了源文件的位置信息。两者完全不同，也不能抽象地比较优劣，只能就具体情况来说。</strong></p><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><ul><li>实际文件系统（linux为ext文件系统）包括引导块、超级块（记录文件系统的管理信息）、索引节点区（保证了ext文件系统只有一个根节点）和数据区，其中超级块与索引节点区是区分文件系统有无的界限；</li><li>虚拟文件系统只存于内存中，它的存在使得操作系统能够兼容足够多的文件系统，具体是将其他文件系统进行封装，并挂载到ext文件系统的目录树下，以使得内核能够以一致的方式访问其他文件系统；</li><li>特殊文件系统proc，也只存于内存中，是内核的窗口，用于查看内核运行的实时信息。</li></ul><h2 id="库"><a href="#库" class="headerlink" title="库"></a>库</h2><ul><li>在编译链接的过程中，链接器搜索静态库（archives）时会链接所有已引用却未处理的符号，而未引用或已处理的符号不会从静态库中链接出来，故链接库的链接位置应放置在命令行尾部，否则会出现符号未处理的情况；</li><li>调用动态库的方式类似于对数据文件的读取，故只有在程序执行时才会装入内存，注意编译链接时须有链接选项<code>-ldl</code>；</li><li>创建静态库的命令为<code>ar cr libtest.a test1.o test2.o</code>，编译成动态库的命令为<code>g++ -share -fPIC -o libtest.so test1.o test2.o</code>。</li></ul><blockquote><p>因为c不支持函数重载（即与c++函数签名方式不同），所以在c/c++混合编程时，须用extern “c” {}将c代码封装。</p></blockquote><h2 id="僵尸进程"><a href="#僵尸进程" class="headerlink" title="僵尸进程"></a>僵尸进程</h2><p>在每个进程退出的时候，内核释放该进程所有的资源，包括打开的文件、占用的内存等，但是仍然为其保留一定的信息(包括进程号，退出状态，运行时间等)， 只有父进程通过<em>wait/waitpid</em>来取时才释放，否则其进程号就会一定被占用，这样就导致了僵尸进程的问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内核&quot;&gt;&lt;a href=&quot;#内核&quot; class=&quot;headerlink&quot; title=&quot;内核&quot;&gt;&lt;/a&gt;内核&lt;/h2&gt;&lt;p&gt;待续&lt;/p&gt;
&lt;h2 id=&quot;文件描述符&quot;&gt;&lt;a href=&quot;#文件描述符&quot; class=&quot;headerlink&quot; title=&quot;文件描述符
      
    
    </summary>
    
      <category term="cs" scheme="http://yoursite.com/categories/cs/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数review</title>
    <link href="http://yoursite.com/2018/02/28/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/02/28/线性代数基础/</id>
    <published>2018-02-28T12:00:00.000Z</published>
    <updated>2018-04-16T13:43:12.504Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对矩阵乘积的各种viewpoint"><a href="#对矩阵乘积的各种viewpoint" class="headerlink" title="对矩阵乘积的各种viewpoint"></a>对矩阵乘积的各种viewpoint</h1><ol><li>列向量的inner product $ x^Ty $ ：n个元素之积的和</li><li>列向量的outer product $xy^T$：<ul><li>对两个列向量各元素之积的陈列【<strong>主</strong>】；</li><li>对列向量$x$分别进行n次倍乘的并行运算，并堆叠成一个矩阵。</li></ul></li><li>矩阵向量之积$Ax$：<ul><li>1个列向量分别与n个列向量的inner product的并行运算（将$A$看做行向量的堆叠）【<strong>主</strong>】；</li><li>对n个列向量的线性组合，即对n个列向量各赋予不同权重后的组合（将$A$看做列向量的堆叠）。</li></ul></li></ol><ol><li>矩阵矩阵之积$AB$：<ul><li>n组列向量的outer product的和，即n个矩阵（每个矩阵为一组列向量的outer producter）的和（$A$ by columns, $B$ by rows）【<strong>主</strong>】；<ul><li>应用：反向传播、方差</li></ul></li><li>n个列向量分别进行矩阵向量之积$Ab$的并行运算；</li><li>对两个矩阵各列向量的inner product的陈列【多用于结合标量的性质进行证明矩阵性质的证明】。</li></ul></li></ol><h1 id="矩阵的一些性质"><a href="#矩阵的一些性质" class="headerlink" title="矩阵的一些性质"></a>矩阵的一些性质</h1><h2 id="迹"><a href="#迹" class="headerlink" title="迹"></a>迹</h2><ul><li>满足交换性和轮换性：$\mathrm{tr}AB=\mathrm{tr}BA$，$\mathrm{tr}ABC=\mathrm{tr}CAB=\mathrm{tr}BCA$</li></ul><h2 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h2><blockquote><p>把线性空间的一个元素（向量、矩阵、……）与一个非负实数相联系，在许多场合下，这个非负实数可以作为向量或者矩阵大小的一种度量，这个非负实数便称为范数（当说长度时，特指的是内积空间中向量的2-范数）。元素与实数的映射关系需满足4点才能称为范数：非负性、定性、齐次性和三角不等式。</p></blockquote><p>范数分类：<em>p</em>-范数、加权范数（$||x||_A=\sqrt{x^TAx}$）、<em>F</em>-范数（$\sqrt{\mathrm{tr}(A^TA)}$）、……</p><h2 id="矩阵的列空间与零空间"><a href="#矩阵的列空间与零空间" class="headerlink" title="矩阵的列空间与零空间"></a>矩阵的列空间与零空间</h2><blockquote><ul><li>$L(x_1,x_2)$意为向量$x_1$与$x_2$张成的空间；</li><li>$R(A)$ 意为矩阵$A$的列向量张成的空间，称为$A$的列空间或$A$的值域；</li><li>$R(A)^{\bot}$意为 $A$的列空间的正交补空间；</li><li>$N(A)$意为$A$的零空间或$A$的核空间。</li></ul></blockquote><p>$A$的零空间是$A^T$的列空间的正交补空间，而A^T的列空间维度与A的列空间维度<strong>在数量上</strong>相等，故$\mathrm{dim}N(A)=\mathrm{dim}R(A^T)^{\bot}=\mathrm{dim}R(A)^{\bot}=n-rank(A)$，为简化书写与推导，常一步写为$\mathrm{dim}N(A)=n-rank(A)$。</p><h2 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h2><blockquote><p>一种几何上的解释：$\mathrm{abs}|A|$ 意为矩阵$A$的行向量限制性地张成的空间的体积。（限制性地张成，是指用于线性组合的系数在(0,1)间取值。）</p></blockquote><ul><li>$|AB|=|A||B|$</li><li>$|A^{-1}|=\frac{1}{|A|}$</li><li>如果$A$奇异，那么$A$的行向量线性相关，限制性地张成的空间是n维空间中的“<em>flat sheet</em>”，故此时$|A|=0$。</li></ul><h2 id="二次型与矩阵的定性"><a href="#二次型与矩阵的定性" class="headerlink" title="二次型与矩阵的定性"></a>二次型与矩阵的定性</h2><blockquote><p>n个变量的二次多项式（<strong>每一项的次数都为2</strong>的多项式）称为二次型。矩阵形式的二次型为一标量$x^TAx$，其中$A$称为二次型矩阵，虽然无论$A$是否对称，$x^TAx$总是一个二次型，但A的表现形式（是否对称）并不影响这个<strong>标量</strong>的结果，故若无特殊说明，默认二次型矩阵为一对称阵。</p></blockquote><ul><li>根据二次型的正负，将矩阵分为正定阵（PD）、半正定阵（PSD）、负定阵（ND）、半负定阵（NSD）和不定阵。</li><li>度量阵$A^TA$必为半正定，若$A$的形状为“$\mathrm{I}$“，且列满秩，则该矩阵必正定。</li><li>半正定阵加正定阵必为正定阵。</li></ul><h2 id="矩阵的特征向量与特征值"><a href="#矩阵的特征向量与特征值" class="headerlink" title="矩阵的特征向量与特征值"></a>矩阵的特征向量与特征值</h2><blockquote><p>默认特征向量特指单位化的特征向量（虽然因为有正负还是不唯一，但已足够）。</p></blockquote><ul><li>$\mathrm{tr}A=\sum_{i=1}^n\lambda_i$</li><li>$|A|=\prod_{i=1}^n\lambda_i$</li></ul><h3 id="对称阵的特征向量与特征值"><a href="#对称阵的特征向量与特征值" class="headerlink" title="对称阵的特征向量与特征值"></a>对称阵的特征向量与特征值</h3><ul><li><p>对称阵的特征值均为实数，特征向量两两正交。</p></li><li><p>若对称阵的特征值均为正数则必正定（由对称阵必能正交对角化推得），依此类推。</p></li><li><p>针对最优化问题<br>$$<br>\mathrm{min}x^TAx \quad s.t. ||x||_2^2=1<br>$$<br>结果为$A$的最小特征值，$x$为其对应的特征向量。<strong>因为$A$为对称阵</strong>，故能正交对角化$A=U\Lambda U^T$，原问题便转化为了：将原默认基换为$U$的列向量构成的基，于是$x$便成为了$y$，满足$y=U^{-1}x$ （其过程为$x$向$U$的各个列向量方向投影，并依列向量次序组织成为$y$），因为$U$为正交阵，故$||y||_2^2=1$依然成立，与此同时，优化目标变为$\mathrm{min}y^T\Lambda y$，可看做（问题本质并非这样，但脱离问题只就该式子的数学形式来看可如此看待；这种办法只可用于求解不能用于理解）求单位向量$y$的方向使其在度量矩阵$\Lambda$下长度最小，故结果为$A$的最小特征值，而$x=Uy$ ，即只取最小特征值对应的特征向量。</p><p>对此常用结果可记住：要使一单位向量在某一度量阵下的内积最小，须使该单位向量等于该度量阵最小特征值对应的特征向量，此时内积便为该度量阵的最小特征值。</p></li></ul><h1 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h1><h2 id="度量阵"><a href="#度量阵" class="headerlink" title="度量阵"></a>度量阵</h2><h2 id="投影阵"><a href="#投影阵" class="headerlink" title="投影阵"></a>投影阵</h2><blockquote><p>若  $L \oplus M=R^n$ ，则将 $x \epsilon R^n$ 沿着 M 到 L 的投影变换在$R^n$ 的基下的矩阵称为投影矩阵，记为$P<em>{L,M}$ ，且$P</em>{L,M} = (X,0)(X,Y)^{-1}$ 。</p></blockquote><ul><li><p>投影的本质：所谓投影便是依投影面 L 重新建立坐标系，并只保留投影面上的坐标，其余坐标置零。故私以为更准确的词应该叫截取，截取矩阵。</p></li><li><p>当 M 为 L 的正交补空间时，即$X^T Y = 0$ ，称其为正交投影：<br>$$<br>P_L=(X,0)(X,Y)^{-1} = X(X^T X)^{-1}X^T = XX^+ \quad (前提：X列满秩)<br>$$<br>在统计学中又称$P_L$ 为 hat matrix，记为 H：</p><ul><li>H：求列向量在 X 列空间上的投影</li><li>I=H：求列向量对 X 列空间的余</li></ul></li></ul><h2 id="正交阵，对称阵与投影阵"><a href="#正交阵，对称阵与投影阵" class="headerlink" title="正交阵，对称阵与投影阵"></a>正交阵，对称阵与投影阵</h2><h1 id="矩阵微积分"><a href="#矩阵微积分" class="headerlink" title="矩阵微积分"></a>矩阵微积分</h1><h1 id="附：一些常见的矩阵相关的思考方向"><a href="#附：一些常见的矩阵相关的思考方向" class="headerlink" title="附：一些常见的矩阵相关的思考方向"></a>附：一些常见的矩阵相关的思考方向</h1><ol><li>设$X=(x_1,x_1,…,x_N)$，则</li></ol><p>$$<br>|| \sum_{i=1}^N x_i || <em>2^2 = \sum</em>{i=1}^N \sum_{j=1}^N x_i^Tx_j^T = (X^TX).sum()<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;对矩阵乘积的各种viewpoint&quot;&gt;&lt;a href=&quot;#对矩阵乘积的各种viewpoint&quot; class=&quot;headerlink&quot; title=&quot;对矩阵乘积的各种viewpoint&quot;&gt;&lt;/a&gt;对矩阵乘积的各种viewpoint&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;列向量的
      
    
    </summary>
    
      <category term="数学" scheme="http://yoursite.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>makefile基础</title>
    <link href="http://yoursite.com/2018/02/20/makefile%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/02/20/makefile基础/</id>
    <published>2018-02-20T07:15:48.000Z</published>
    <updated>2018-02-20T08:31:51.573Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文件基本格式"><a href="#文件基本格式" class="headerlink" title="文件基本格式"></a>文件基本格式</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">target:prerequisites</div><div class="line">[tab] commands</div></pre></td></tr></table></figure><ul><li><p>若prerequisites中有一个文件比target文件新，则执行commands所定义的命令。<br>若显式指明target为伪目标，即<code>.PHONY:target</code>，则<code>make</code>命令将跳过文件检查，<br>直接执行对应的命令，由此避免了因当前目录下有target文件而不会执行命令的问题。</p></li><li><p>若make未指定目标则缺省执行第一个目标。</p></li><li><p>命令按行解析，每行命令都在单独的进程中执行。</p></li><li><p>在[tab]后commands前添加<code>@</code>，可关闭回显，常用于<code>echo</code>命令。</p></li></ul><blockquote><p><code>include filename</code>命令可将其他文件包含进来，若在该命令行首添加<code>-</code>，<br>表示忽略可能会出现的文件包含错误。</p></blockquote><h1 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h1><ul><li>变量在使用时展开，形式上类似宏替换，如此引用<code>$(var)</code>，而引用shell变量时用<code>$$</code>。</li><li><p>变量定义的四种格式：</p><ul><li><code>var=value</code>在执行时才扩展</li><li><code>var:=value</code>在定义时便扩展，直接使用右侧的现值</li><li><code>var?=value</code>若变量为空则设置该值，否则维持原值</li><li><code>var+=value</code>将值追加到变量尾部，继承上次的操作符，若未定义过则自动解释为<code>=</code><h2 id="特殊变量"><a href="#特殊变量" class="headerlink" title="特殊变量"></a>特殊变量</h2></li></ul></li><li><p>内置变量<code>$(CC)</code>当前使用的编译器</p></li><li><p><code>$(MAKE)</code>当前使用的make工具</p></li></ul><h2 id="自动变量"><a href="#自动变量" class="headerlink" title="自动变量"></a>自动变量</h2><ul><li><p><code>$@</code>当前目标</p></li><li><p><code>$^</code>所有先决条件，<code>$?</code>比目标更新的所有先决条件。</p></li><li><p><code>$&lt;</code>第一个先决条件</p></li></ul><h2 id="多行变量"><a href="#多行变量" class="headerlink" title="多行变量"></a>多行变量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">define var</div><div class="line">commands</div><div class="line">endef</div></pre></td></tr></table></figure><p><em>主要用于定义命令包，每行命令都在单独的进程中执行，故展开时有可能导致脚本错误。</em></p><h2 id="静态模式：以-通配"><a href="#静态模式：以-通配" class="headerlink" title="静态模式：以%通配"></a>静态模式：以%通配</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">objs=main.o library.o</div><div class="line">all:$(objs)</div><div class="line">$(cc) -o a $(objs)</div><div class="line">$(objs):%.o:%.c</div><div class="line">$(CC) -c $&lt; -o <span class="variable">$@</span></div></pre></td></tr></table></figure><h1 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a>条件判断</h1><h2 id="基本格式"><a href="#基本格式" class="headerlink" title="基本格式"></a>基本格式</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">conditional-directive</div><div class="line">commands</div><div class="line"><span class="keyword">else</span></div><div class="line">commands</div><div class="line">endif</div></pre></td></tr></table></figure><h2 id="可用的条件判断"><a href="#可用的条件判断" class="headerlink" title="可用的条件判断"></a>可用的条件判断</h2><ul><li><code>ifeq(var1,var2)</code> 两参数是否相等</li><li><code>ifneq(var1,var2)</code> 两参数是否不等</li><li><code>ifdef var</code> 变量是否已定义</li><li><code>ifndef var</code> 变量是否未定义</li></ul><h1 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h1><p><em>可使用shell循环</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">objs = file1 file2</div><div class="line">rulefor:</div><div class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> `<span class="built_in">echo</span> $(objs)`; \</div><div class="line"><span class="keyword">do</span> \</div><div class="line">touch $<span class="variable">$filename</span>; \</div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p><p><em><code>\</code>保证了多行命令在同一进程下执行，因命令是在shell下执行的，<br>故<code>filename</code>为shell变量，自然用<code>$$</code>引用。</em></p><h1 id="字符串替换函数"><a href="#字符串替换函数" class="headerlink" title="字符串替换函数"></a>字符串替换函数</h1><p><em>示例</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">comma := ,</div><div class="line">empty := </div><div class="line">space := $(empty) $(empty)</div><div class="line">str1 := a b c</div><div class="line">str2 := $(subst $(space),$(comma),$(str))</div><div class="line">result:</div><div class="line">@<span class="built_in">echo</span> $(str1)</div><div class="line">@<span class="built_in">echo</span> $(str2)</div><div class="line">@<span class="built_in">echo</span> <span class="string">"done"</span></div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;文件基本格式&quot;&gt;&lt;a href=&quot;#文件基本格式&quot; class=&quot;headerlink&quot; title=&quot;文件基本格式&quot;&gt;&lt;/a&gt;文件基本格式&lt;/h1&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt
      
    
    </summary>
    
      <category term="cpp" scheme="http://yoursite.com/categories/cpp/"/>
    
    
  </entry>
  
  <entry>
    <title>mc文件管理器命令备忘</title>
    <link href="http://yoursite.com/2018/02/20/mc%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E5%99%A8%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98/"/>
    <id>http://yoursite.com/2018/02/20/mc文件管理器命令备忘/</id>
    <published>2018-02-20T06:48:15.000Z</published>
    <updated>2018-02-20T08:32:14.442Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文件信息查看与修改"><a href="#文件信息查看与修改" class="headerlink" title="文件信息查看与修改"></a>文件信息查看与修改</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">C-x i 显示文件详细信息，重复指令关闭窗口（类似<span class="built_in">stat</span>命令）</div><div class="line">C-u 左右面板互换（用于改变列表显示模式）</div><div class="line">C-x c 修改文件权限信息（space为反选键）</div><div class="line">[alt]-t 切换列表显示模式</div></pre></td></tr></table></figure><h1 id="快捷访问："><a href="#快捷访问：" class="headerlink" title="快捷访问："></a>快捷访问：</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">C-x h 添加文件至收藏夹</div><div class="line">C-\ 显示文件收藏夹列表</div><div class="line">[tab] 切换活动面板</div><div class="line">C<span class="_">-s</span> 在当前目录下查找文件</div></pre></td></tr></table></figure><h1 id="文件移动与管理"><a href="#文件移动与管理" class="headerlink" title="文件移动与管理"></a>文件移动与管理</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">+ 可用正则匹配符选中多个文件</div><div class="line">- 取消选中</div><div class="line">[insert] 多选模式下反选当前高亮文件</div><div class="line">F-5,6,8 对当前高亮文件或选中的多个文件进行复制、更名或移动、删除操作，目标为另一个面板所在路径</div></pre></td></tr></table></figure><h1 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">C-w 删除光标前所有字符</div><div class="line">C-b C<span class="_">-f</span> C<span class="_">-a</span> C<span class="_">-e</span> 移动光标（方向键在此不起作用）</div><div class="line">[esc] [tab] 命令补全（重复两次会显示补全列表）</div></pre></td></tr></table></figure><h1 id="辅助指令"><a href="#辅助指令" class="headerlink" title="辅助指令"></a>辅助指令</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[esc] 1 帮助手册</div><div class="line">C-o 与父shell进行切换（切换后父shell工作目录为活动面板工作目录，用于更多的shell指令）</div></pre></td></tr></table></figure><h1 id="主题倾向"><a href="#主题倾向" class="headerlink" title="主题倾向"></a>主题倾向</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yadt256</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;文件信息查看与修改&quot;&gt;&lt;a href=&quot;#文件信息查看与修改&quot; class=&quot;headerlink&quot; title=&quot;文件信息查看与修改&quot;&gt;&lt;/a&gt;文件信息查看与修改&lt;/h1&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
      
    
    </summary>
    
      <category term="工具的使用" scheme="http://yoursite.com/categories/%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    
    
  </entry>
  
</feed>
