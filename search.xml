<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习——深度学习概览]]></title>
    <url>%2F2018%2F06%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[cpp与python异同]]></title>
    <url>%2F2018%2F06%2F14%2Fcpp%E4%B8%8Epython%E5%BC%82%E5%90%8C%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习——图像语义分割]]></title>
    <url>%2F2018%2F05%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%2F</url>
    <content type="text"><![CDATA[全卷积网络（FCN） 全卷积网络是从抽象的特征中恢复出每个像素所属的类别，即从图像级别的分类进一步延伸到像素级别的分类。 将最后的输出是1000张heatmap经过upsampling变为原图大小的图片，然后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——特征工程]]></title>
    <url>%2F2018%2F05%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[PERSPECT：好的特征能降低建模的难度。 简单数字的花式技巧 特征空间与数据空间:特征（如，每个人喜欢的一个歌单）是在特征空间中的一个向量，一个人可由特征空间中的向量表示出来；而针对某首特定的歌，每一个人对其喜爱程度构成一个维度，所有的维度构成数据空间，一首歌可由数据空间中的向量表示出来。若将一个人的所有特征并成一条向量作为矩阵的行向量，那么该矩阵（称为数据矩阵）的列向量空间便是特征空间，行向量空间是数据空间。 scale 基于输入数据的光滑函数的模型（如K-means聚类，线性函数，最近邻，REB核等）对输入数据的尺度很敏感，故通常将输入数据进行归一化，以使得输出数据在统一的尺度上。 distribution 有些模型对输入数据的分布有一定要求，如线性回归模型要求预测误差服从高斯分布，通常这一要求是无需考虑的，但当输出值跨越好几个量级时，这一要求便无法满足，须将输出值取对数（log 转换，一种量级转换），这可使得预测误差更接近高斯分布（准确说，这属于 target engineering）。比如，在 Faster-R-CNN 中的目标框回归中对尺度因子的预测，将尺度因子取对数作为回归量。 当某维的特征没有边界时（scale）feature scaling 无法改变特征的分布，只能够改变特征的范围（range）。 min-max scaling$$x^* = \frac{x - min(x)}{max(x) - min(x)}$$ scaled feature 的范围为 (0, 1)。 variance scaling（standardization）$$x^* = \frac{x- mean(x)}{var(x)}$$ scaled feature 的均值为0，方差为1，而其范围视情况而定，不是固定数值。 L-2 normalization$$x* = \frac{x}{||x||_2}$$ 当某维特征的量级跨越过多时（distribution）当某维特征的量级跨越过多时，很多模型可能会出问题。如在线性模型中，因为线性系数被期望可以接受所有可能的输入值，故当某维特征的量级跨越过多时，该维特征会在不知情的情况下被偏向成为重要性远高于其他特征的主要特征，换句话说其他特征会被自动忽略掉。 最简单暴力的解决方法为 quantization / binning，如将少年年龄映射为1，青年年龄映射为2，中年年龄映射为3，老年年龄映射为4；或者将个位数映射为1，十位数映射为2，百位数映射为3，千位数映射为4，等等。 log transform有个常用的连续的映射，log 转换 $data{transformed} = \log (data{raw} + 1)$ ：它的作用是压缩大数（大于1）的尺度，放大小数（小于1）的尺度（一般不用），常用于处理重尾分布（A heavy-tailed distribution places more probability mass in the tail ranges than a Gaussian distribution.）的正数特征，在将重尾分布的正数特征做 log 转换后，特征的分布会更趋向于高斯分布。 R square是决定系数，意思是你拟合的模型能解释因变量的变化的百分数，例如 R square = 0.81，表示你拟合的方程能解释因变量81%的变化，还有19%是不能够解释的。$$R_square_score = \frac{\sum(wx - \bar y )^2}{\sum(y - \bar y )^2}$$ Box-Cox 变换 （generalization of log transform） 在单因子方差分析的统计模型中，需满足三个假设： 正态性，每一维度下观察值的总体是正态总体； 独立性，从每一总体中抽取的样本是相互独立的； 方差齐性，各总体的方差等于常数。 试验过程中保证随机性便可满足独立性的假设，而对于特征的选择却很难保证正态性和方差齐性。虽然正态性与方差齐性会相辅相成，互为影响，但因为方差齐性比正态性更为重要，故方差齐性变换（又称为方差稳定性变换）尤为重要，详见： 方差稳定化变换综述 $$x^* = {^{\frac{x^\lambda - 1}{\lambda}, \lambda \ne 0}_{\log x, \lambda = 0}$$ 参数小于1时，压缩大数的尺度，参数大于1时放大大数的尺度。该参数可由极大似然或者贝叶斯法得到最佳值，当然可直接调包求得。 特征选择 特征选择和降维都是为了减少数据集的属性数量，但是降维是通过创建新的属性组合，特征选择挑选数据中的属性而并不改变它们。 特征选择的目的：提高预测器的预测性能，提供更快和更低成本的预测器，和提供更好的了解产生的数据的基本过程。 分为以下三步： filtering计算每个特征与响应变量之间的相关信息和互信息，过滤掉低于阈值的特征。缺点是它不考虑模型是否采用该特征。 wrapper当模型需要特征融合时很有用。缺点是它计算很昂贵。 embedded methods将模型选择作为模型训练的一部分，如决策树，再如L-1正则化。它平衡了以上两点。 用PCA压缩数据集 主成分分析命名：投影数据被称为原始数据的主成分。 其优化方向为两个：最近重构性（样本点到这个超平面的距离都足够近）和最大可分性（样本点在这个超平面上的投影能尽可能分开，常用） PCA 通过查找线性相关模式来减少特征空间的维度。 PCA 侧重于线性依赖的概念，关键思想是用一些充分总结原始特征空间中包含信息的新特征取代冗余特征。 为保证新特征的最大可分性，可以寻找一个超平面使得投影后的任意两点之间的距离最大化。但事实证明，这是一个非常困难的数学优化问题。另一种方法是测量任意两点之间的平均距离，或者等价地，每个点与它们的平均值之间的平均距离，即方差。事实证明，这优化起来要容易得多。由此 PCA 的最优化问题为：$$\max_W tr(\sum_i (x_iW)^T(x_iW))\s.t. \quad W^TW = I$$即：$$\max_W tr(W^T X^TXW)\s.t. \quad W^TW = I$$其中，X 为数据矩阵，每一行为一条数据，记为$x_i$，W 为基，每一列为一个基向量，投影后的数据点为$XW$ 。 该问题的数学本质为：输入为单位向量，寻找该单位向量能够使得输出的范数最大化的方向。故$W$ 为$X^T X$ 的主要特征值（按从大到小排序，较大的那些特征值）对应的单位特征向量，或者说 X 的主要右奇异向量（奇异值按从大到小排序），两者一样。数学原理见对称阵的特征向量与特征值与A generalization of the Eckart-Young-Mirsky matrix approximation theorem。 由此通过对中心化的数据矩阵作奇异值分解，便可得到 PCA 的解：$$X^* = XW = (U \Sigma V^T) V_k = U_k \Sigma_k.$$ 降维、投影与换基降维即投影变换，通过换基后将非投影面的维度置零实现，属于可逆操作，即可升维。 k 值的选择对于在新的特征空间中的第 k 维特征的方差为$$||Xw_k||^2_2 = ||u_k \sigma_k||^2_2 = \sigma_k^2,$$故常称中心化的数据矩阵的奇异值列表为其频谱。 而 k 值的选择常依据在新的特征空间中主成分特征的方差和占总特征的方差和的比例：$$\frac{\sum_i^k \sigma_i^2}{\sum_i^d \sigma_i^2} \ge proportion(usually \quad 0.8)$$ 注意 在应用 PCA 之前最好先将特征做好 scale 和 distribution 的处理； 涉及 SVD，计算成本大； 非线性特征提取和模型堆叠]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——支撑向量机]]></title>
    <url>%2F2018%2F05%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%94%AF%E6%92%91%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[SVM解决的问题解决了两个问题： 通过最大化 margin，解决 PLA 健壮性差的问题； 通过对偶问题的转换和核函数，解决了在引入特征转换解决线性不可分问题时导致的复杂度高的问题。 （附）软间隔： 由问题引出的两个核心点margin 的表示形式点到超平面的距离两种表示形式： 几何间隔：常用于在欧式空间中做距离度量，通过取超平面一点指向所研究点的向量，将该向量与超平面的单位法向量做内积便得几何间隔 $$margin = \frac{|w^Tx + b|}{||w||}$$ 函数间隔：常用于做正负判断，也可用于做与超平面距离远近的比较，通过将点的坐标直接带入函数得到；函数间隔本质是几何间隔乘以法向量的长度，即$||w||$。 $$margin = |w^Tx + b|$$ 最大化 margin 得到的 prime SVM首先假设数据 x 经特征转换后变为的 z 线性可分。 出于数学表达形式简便上的考虑，设 label 为 +1 和 -1，那么最大化 margin，便是最大化最小 margin，则优化问题为：$$\max{w,b} { \frac{\min(label(w^Tz+b))}{||w||} }$$因为 margin 比较的是相对大小，出于便于优化（往二次规划靠拢）上的考虑，设 $\min(label(w^Tz+b))=1$ ，故优化问题化为：$$\max{w,b} \frac{1}{||w||}\s.t. \quad label*(w^Tz+b) \ge 1$$由此，通过对函数间隔最小值的设定，将 margin 的大小化简为了法向量长度的倒数，即 $\frac{1}{||w||}$ 。 软间隔当数据 x 经特征转换后变为的 z 线性不可分时，将越过margin的点做记录，并将越过的量计入损失函数中，至于是以函数间隔还是几何间隔的形式计入损失函数中，应衡量函数间隔计算上的便易性与几何间隔物理含义上的合理性。当采用几何间隔的形式计入损失函数中时，优化问题化为：$$\max \frac{1}{||w||} - C \sum \frac{\lambda_i}{||w||}\s.t. \quad {^{label_i (w^T z_i + b) \ge 1 - \lambdai} { \lambda_i \ge 0}$$其中 C 是一超参数，该数越大，在优化时会越偏向于将所有点置于分界面之外。考虑到函数间隔也能体现错误点与分界面的远近，同时超参数的存在使得几何间隔确切的物理含义的意义不大，又考虑到优化的便易性，将优化问题化为：$$\min \frac{1}{2}||w||_2^2 + C \sum \eta_i\s.t. \quad {^{label_i (w^T z_i + b) \ge 1 - \etai} { \eta_i \ge 0}$$ 对偶问题的转换由不等式约束的拉格朗日乘子法，得到等价的 KKT 条件下的拉格朗日对偶函数：$$\min_{w, b, \eta} \frac{1}{2}||w||_2^2 +C \sum \eta_i + \sum \lambda_i^0 (1- \eta_i - label_i (w^T z_i + b)) + \sum \lambda_i^1(-\eta_i)\s.t.\\quad \lambda_i^0 \ge 0\\quad 1- \eta_i - label_i (w^T z_i + b) \le 0\\quad \lambda_i^0(1- \eta_i - label_i *(w^T z_i + b) ) = 0\\quad \lambda_i^1 \ge 0\\quad -\eta_i \le 0\\quad \lambda_i^1( -\eta_i ) = 0$$记拉格朗日函数为$L$，将拉格朗日函数对各变量的导数分别置零得：$$\frac{\partial L}{\partial w} = 0 \Rightarrow w = \sum\lambda_i^0 label_i z_i\\frac{\partial L}{\partial b} = 0 \Rightarrow \sum \lambda_i^0 label_i = 0\\frac{\partial L}{\partial \eta_i} = 0 \Rightarrow C - \lambda_i^0 - \lambda_i^1 = 0$$将三个结果带入最优化问题中，得拉格朗日对偶函数：$$L = -\frac{1}{2}||w||_2^2 + \sum \lambda_i^0\= -\frac{1}{2} \sum_i^m \sum_j^m \lambda_i^0 \lambda_j^0 label_i label_j (z_i z_j^T) + \sum \lambda_i^0 , \quad 0 \le \lambdai^0 \le C.$$进而得到最终的对偶问题：$$\min{\lambda^0} \frac{1}{2} \sum_i^m \sum_j^m \lambda_i^0 \lambda_j^0 label_i label_j (z_i z_j^T) - \sum \lambda_i^0\s.t.\\quad 0 \le \lambda_i^0 \le C\\quad \sum \lambda_i^0 label_i = 0$$ 对偶变量的物理含义在对偶问题的转换过程中可得到如下互补松弛条件：$$\lambda_i^0(1- \eta_i - label_i *(w^T z_i + b) ) = 0 \ (C - \lambda_i^0) \eta_i = 0$$由此可得（称 $\lambda_i^0$ 为对偶变量，称 $\eta_i$ 为越界变量）： 对于对偶变量等于0的点，为 non SV，越界变量等于零； 对于对偶变量大于0小于 C（越界惩罚系数）的点，为 free SV，越界变量等于零，这些点决定了 w 和 b； 对于对偶变量等于 C 的点，为 bounded SV，越界变量大于零； 核函数当采用特征转换（如高次多项式）将数据从低维空间映射到高维空间时，会带来模型复杂度的指数级增长，而核函数会解决这个问题，即即使映射到高维空间，模型复杂度依然保持低维空间时的大小。一种观点是核函数是度量两个向量在某个高维空间中的距离的度量尺，不同高维空间对应不同的核函数。具体原理以二次多项式为例： 待续。。 进而，原对偶问题化为：$$\min_{\lambda^0} \frac{1}{2} \sum_i^m \sum_j^m \lambda_i^0 \lambda_j^0 label_i label_j K(x_i x_j^T) - \sum \lambda_i^0\s.t.\\quad 0 \le \lambda_i^0 \le C\\quad \sum \lambda_i^0 label_i = 0$$故计算复杂度由原先的$O(m^d)$降为了$O(m)$，d 指特征变化到的高维空间的维度。 二次规划SMOSVR]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何阅读文献]]></title>
    <url>%2F2018%2F05%2F21%2F%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E6%96%87%E7%8C%AE%2F</url>
    <content type="text"><![CDATA[http://www.sciencemag.org/careers/2016/03/how-seriously-read-scientific-paper]]></content>
      <categories>
        <category>工具的使用</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——BatchNormalization]]></title>
    <url>%2F2018%2F05%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94BatchNormalization%2F</url>
    <content type="text"><![CDATA[经过 BN 层后，使得输出的均值与方差不再与前面的网络参数有复杂的关联依赖，而只由 BN 层的参数 $\beta , \gamma$ 决定，故前面的网络参数是否学好并不影响后面 BN 层参数 $\beta , \gamma$ 的学习，若前面的网络参数尚未学好，已经学好的 BN 层参数 $\beta , \gamma$ 的存在，会使得最近一层的网络参数更快地朝好的方向发展，进而一步步反传使得所有网络参数达到最优，而不必费心地去考虑网络参数如何初始化最好。 BN 层使得应处于激活函数饱和区的神经元处于饱和区，应处于激活区的神经元处于激活区，即无论网络参数如何，BN 层仍可使得信号进行有效的传播，结合对参数的正则化，学习到的权重便不会过高或过低，从而从根本上解决了因 w 的大小而导致的梯度爆炸或消失问题。 容许较高学习率的原因： 对于通用格式 conv + BN + ReLU 中的 conv 层不加 bias 是因为 BN 已有 bias 功能？：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——faster_RCNN]]></title>
    <url>%2F2018%2F05%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[预备知识FCN(Fully Convolutional Networks)全卷积神经网络顾名思义，FCN 即全为卷积层的神经网络，没有全连接层，或者说将全连接层以卷积层的方式实现，而非以两个矩阵乘积的方式实现（全连接层的实现方式），具体为： 对于上一层是卷积层的全连接层，该层以与输入的 feature map 大小相同的卷积核做卷积运算实现； 对于上一层是全连接层的全连接层，该层以输入的神经元个数为 1*1 卷积核通道数做卷积运算实现，或者说是将神经元看做了一个 feature map，或者说是将一个 feature map 看做了一个神经元的激活值（激活值不一定是个标量）。 这样做的优点是，通过在最后一层做 pooling（核大小为 feature map 大小）的方法，可实现让一个已设计完毕的网络可以接收任意大小的图片。 NMS(Non-Maximum Suppression)非极大值抑制首先选定一个 IoU（Intersection-over-Union，交并比，即两框重叠部分面积占两框并集面积的比例，用于衡量两个 bounding box 重叠度的量）阈值，例如 0.25，然后将所有 4 个窗口（bounding box）按照得分由高到低排序，然后选中得分最高的窗口，遍历计算剩余的3个窗口与该窗口的交并比（IoU），如果 IoU 大于阈值 0.25，则将窗口删除；然后再从剩余的窗口中选中一个得分最高的。重复上述过程，直至所有窗口都被处理，从而得到所有的检测框。示例代码如下： 1234567891011121314151617181920212223def py_cpu_nms(dets, thresh): x1, y1, x2, y2, scores = dets[:, 0], dets[:, 1], dets[:, 2], dets[:, 3], dets[:, 4] areas = (x2 - x1 + 1) * (y2 - y1 + 1) order = scores.argsort()[::-1] keep = [] while order.size &gt; 0: i = order[0] keep.append(i) order = order[1:] xx1 = np.maximum(x1[i], x1[order]) yy1 = np.maximum(y1[i], y1[order]) xx2 = np.minimum(x2[i], x2[order]) yy2 = np.minimum(y2[i], y2[order]) w = np.maximum(0.0, xx2 - xx1 + 1) h = np.maximum(0.0, yy2 - yy1 + 1) inter = w * h ovr = inter / (areas[i] + areas[order] - inter) inds = np.where(ovr &lt;= thresh)[0] order = order[inds] return keep ROIs Pooling（Region of Interest Pooling） 将候选框映射至 feature map； 做 ROIs Pooling，即对每个 ROI 做同等份数的分割，对每块分割做 max/avg pooling；或者说用一个单层的SPP layer将映射到的 feature map 下采样为大小固定的 feature。 bounding box regression因为候选框一般不够精确，故需根据候选框内的物体做精修，类似于人工打 label 的过程。 假设：当候选框与 GT 相差较小时，候选框内的 feature map 到两个平移量和两个尺度因子的映射是一种线性变换。该假设类似于人看到一本书的70%以上时，便能猜测出该书有多大，但若只看到一个书角便不能，当然也不期望能。 便于回归的两个trick（属于 target engineering）： 中心点平移时，用平移量是原proposal 的宽和高的几倍做目标，保证了在候选框内有用的 feature map 相同时，候选框的大小不怎么影响回归量的值； 尺度放缩时，用尺度缩放因子取对数做目标，从而使得误差更符合高斯分布。 othersselective search首先通过过分割的方法将图片分割成无数个小区域，然后依据相似度（颜色、纹理、尺度、交叠度等）合并可能性最高的两个区域，重复合并，直至所有区域合并成为一个区域为止， 最后输出所有曾出现过的区域作为候选区域（通常约为 2000 个）。如下图所示： 迁移学习迁移学习指有监督预训练(Supervised pre-training)，即把一个任务训练好的参数，拿到另外一个任务作为初始化参数值。将一个任务得到的特征迁移到另一个任务中，故称为迁移。 发展历程 两个目标： 传统算法滑动窗口 + 缩放图片后再滑动窗口（等价于取的不同大小的滑动窗口）+ 分别计算是 object 的可能性，并对框内物体进行分类。 RCNNselective search 提取候选框 + 每个候选框 wrap 成统一尺寸送入后 CNN 提取特征 + SVM + bounding-box-regression 简单说便是：RCNN = regions + CNNs + pooling + SVM // LR 创新：首次将卷积神经网络用在目标检测上。 SPP-net（spatial pyramid pooling）所谓空间金字塔池化是与图片金字塔或特征金字塔以相同的理念出发的，即对于最终的 feature map 采用不同大小的池化框，相当于多尺度的 pooling。从解决的问题出发。可认为是模仿的 bag-of-words 和 spatial-pyramid-matching。 SPP-net = CNN // regions + SPP + SVM // LR 创新： 通过 SPP 解决了候选框尺寸不一致的问题; 通过对所有候选框共享 CNN 提取的特征，大大减少了运算量。 fast-RCNNfast-rcnn = CNN // regions + RoI-pooling + nn(multi-task) 创新： 分类器和回归器用神经网络实现，故在训练时无需用大量的硬盘空间来存储 RCNN 中独立的 SVM 和回归器所需的作为训练样本的大量特征； 除了候选框外将整个框架构建成为了一个神经网络，使得从特征提取器到最终的分类回归器都是一起被优化的，实现了端到端的训练，使得速度和精确度都上了一个台阶。 faster-RCNN尝试将耗时的 selective search 用神经网络实现，创新性地提出了 RPN，使得区域生成网络和 fast-rcnn 能够共享特征提取网络，速度和精确度再上一个台阶。 faster-RCNN = CNN // RPN + RoI-pooling + nn(multi-task) faster-RCNN_tf 损失函数$$rpn_cls_loss = -\sum_i (p_i^ \log p_i + (1-p_i^) \log (1 - p_i))\rpn_bbox_loss = - \sum_i p_i^ smooth_L_1(t_i, t_i^)\cls_loss = - \sumi \log p{I_i}\bbox_loss = - \sum_i smooth_L_1(t_i, t_i^*)$$ RPN RPN 中每个候选框的得分值和 bbox_delta 都是仅通过特征图上的一个点（512通道）的信息得到的（ A1 和 A2），其合理性是因为该点是候选框的中心点，认为已粗略地包含了框中特征的大部分信息。 而分类网络中每个候选框的得分值和 bbox_delta 是通过候选框的范围映射到特征图上的全部信息得到的。 anchors代码实现trick：利用 (x, y, w, h) 的形式使得 (x1, y1, x2, y2) 可批量生成。 1234567891011121314151617181920212223242526272829303132def _scale_enum(anchor, scales): w, h, x_ctr, y_ctr = _whctrs(anchor) ws = w * scales hs = h * scales anchors = _mkanchors(ws, hs, x_ctr, y_ctr) return anchorsdef generate_anchors(base_size=16, ratios=[0.5, 1, 2], scales=2**np.arange(3, 6)): base_anchor = np.array([1, 1, base_size, base_size]) - 1 w, h, x_ctr, y_ctr = _whctrs(base_anchor) size = w * h ws = np.round(np.sqrt(size / ratios)) hs = np.round(np.sqrt(size * ratios)) # ws/hs = [1/sqrt(ratios)] / sqrt(ratios) = 1/ratios # ws, hs = array([23., 16., 11.]), array([12., 16., 22.]) ratio_anchors = _mkanchors(ws, hs, x_ctr, y_ctr) anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales) for i in range(ratio_anchors.shape[0]) ]) #array([[ -83., -39., 100., 56.], # [-175., -87., 192., 104.], # [-359., -183., 376., 200.], # [ -55., -55., 72., 72.], # [-119., -119., 136., 136.], # [-247., -247., 264., 264.], # [ -35., -79., 52., 96.], # [ -79., -167., 96., 184.], # [-167., -343., 184., 360.]]) return anchors 通过利用 numpy 的 broadcast 机制，可得到所有点的候选框，举例： 123_anchors = np.ones((1,9,4))shift = np.ones((1,32*32,4)).transpose((1,0,2))print(np.shape(_anchors + shift)) # array(32*32, 9, 4) anchors –&gt; proposalsa.将 anchors 与 bbox_deltas 合并（平移+放缩）得到最初的 region proposal，并将候选框小的或跨越图像边界的删掉。 b. 在训练时对每个标定的真值候选区域，与其重叠比例最大的region proposal记为前景样本（保证了每个 GT 都至少有一个候选框），剩余的region proposal，如果其与某个标定重叠比例大于0.7，记为前景样本，如果其与任意一个标定的重叠比例都小于0.3，记为背景样本（保证了候选框的合理性），其余的region proposal，弃去不用；对a)剩余的region proposal，根据每个框的 objectness 得分做排序删减。 c. 对b)剩余的region proposal 做 nms，保留前一定数量的 proposals。 简而言之，当第 i 个 anchor 与 GT 间 IoU&gt;0.7，则认为该 anchor 是 foreground，标记 label 为1；反之 IoU&lt;0.3 时，认为是该 anchor 是 background，标记 label 为0；至于那些 0.3&lt;IoU&lt;0.7 的 anchor 则不参与训练。 训练网络的细节总结ref1: CNN目标检测与分割（一）：Faster RCNN详解 【附】VOC2007ref2: VOC2007数据集制作]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理——隐马尔可夫模型]]></title>
    <url>%2F2018%2F04%2F26%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[前言词性（Part-of-Speech）POS 是依据语法功能划分，是词语在区别词类时用到的属性。 词性标注的方法 rule-based 语言专家根据词法及语言学知识编制的规则。 learning-based 从专家标注的语料库中学习到用于自动标注的模型 统计模型：隐马尔可夫模型（HMM），条件随机域模型（CRF），神经网络模型（NN） 规则学习：基于转换的学习（TBL） 符号规定 符号 含义 $N$ 训练数据中的句子总数 $O_i$ 第 i 个句子（词序列） $o_i$ 某句子中的第 i 个词 $Q_i$ 第 i 个句子对应的词性标注（词性序列） $q_i$ 某句子中的第 i 个词对应的词性 基于统计语言模型的词性标注基本模型$$\max_Q P(Q|O)$$ 由于语料库不可能包含所有可能出现的句子，故应得到一个更加宽泛的的表达式。利用贝叶斯法则得等价模型$$\max_Q P(O|Q)P(Q)$$ 隐马尔可夫模型（HMM）假设 一阶马尔可夫假设，即语义相关性只涉及到前面 1 个词（也可设为 2 阶或 3 阶）：$P(Q) = P(q_1)P(q_2|q_1)…P(qN|q{N-1})$； 观测独立性假设，当前时刻的观测值仅与当前时刻的不可观测量的值（状态值）有关，与其他时刻的观测值无关；即单词$o_i$对应的$q_i$不受其他单词影响，即$P(o_i|q_i)$相互独立：$P(O|Q)=\prod P(o_i|q_i)$，故在该模型中，观测独立性假设也可称为条件独立性假设。注意状态之间独立性并不成立， 模型$$\max_Q \prod P(o_i|q_i) * \prod P(qj|q{j-1}),$$ 其中$P(o_i|q_i)$被称为发射概率，是通过统计每个单词在语料库中的出现情况得到的。对于因某个单词没有在语料库中出现导致发射概率为 0 进而导致整个句子出现概率为 0 的情况，须做一些平滑处理。 求解由模型定义求解（暴力遍历）对于给定的观测序列，求所有可能状态序列的概率，并将最大概率的状态序列最为所求结果。设观测序列长度为 T ，可选状态数为 M，可选观测数为 N，首先在最一开始时由初始状态概率向量 $\pi$求出后续 T-1 个状态概率向量$i_t = \pi A^{t-1}$ ，那么一个可能状态序列的概率为，对一个句子的词性标注的时间复杂度为。 总结使用离散时间点、离散状态，并做了马尔可夫假设，由此系统产生了马尔可夫过程的模式，它包含一个$\pi$向量和一个状态转移矩阵。 隐马尔科夫模型是在一个标准的马尔科夫过程中引入一组观察状态，以及该组观察状态与隐藏状态间的概率关系。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理——任务]]></title>
    <url>%2F2018%2F04%2F26%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[词性标注（POS tagging）文语转换（TTS）词形还原（lemmatization）名词块检测（NP-chunk detection, noun phrase chunking detection）词义消歧（word sense disambiguation）]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——集成学习]]></title>
    <url>%2F2018%2F04%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[集成学习名词定义 名词 含义 学习器 学习算法，即$\cal{A} + \cal{H}$ 模型 由学习器和数据集学习得到的解，记为 g 真实模式 数据集服从的真实分布，记为 f 分类 选用在验证集上表现最好的模型； 给每个模型赋予各自的权重系数：$sign(\sum_t \alpha_t g_t (x))$ ； 给每个模型赋予的权重系数依赖输入变化而变化：$sign(\sum_t q_t (x) g_t (x))$ ； 理论依据若能证明基于各子模型赋予相同的权重系数得到的 G 的表现比各子模型（可由不同学习器同批数据得到，也可由同一学习器不同批数据得到，当然这些数据应是独立同分布）的平均表现更好，那么集成学习的类别2、3便可认为是更强的学习器： 设 $G = \frac{1}{T} \sum_t g_t$ ，真实模式为 f ，则$$avg[ (g_t - f)^2 ] = avg(g_t^2 - 2g_t f + f^2)\= avg(g_t^2) - 2Gf + f^2\=avg(g_t^2) - G^2 + (G-f)^2\= avg(g_t^2 - 2G^2 + G^2) + (G-f)^2\= avg(g_t^2 - 2g_t G + G^2) + (G-f)^2\= avg[(g_t - G)^2] + (G-f)^2$$由此各子模型的平均泛化误差必比 G 大，得证。 blending 当各子模型由不同学习器同批数据得到，称集成学习为 blending 。 因各子模型的权重需要再在验证集上学习才能得到，故称为 two-level learning 。 linear blending先由训练集和不同的学习器得到各自的 $g_i, (i=1, 2, …, M)$ ，设 $z=( g_1(x), g_2(x), …, g_M(x))$ ，再由验证集和线性学习器 $Lin({z_n, y_n})$ 得到解析解 $\alpha$ ，于是 $G = \alpha^T z$ 。 any blending（stacking）先由训练集和不同的学习器得到各自的 $g_i, (i=1, 2, …, M)$ ，设 $z=( g_1(x), g_2(x), …, g_M(x))$ ，再由验证集和任意学习器 $Any({z_n, y_n})$ 求得解，于是 $G = q(z)$ 。 boosting 当各子模型由同一个学习器不同批数据得到，称集成学习为 boosting 。 偏差-方差分解偏差-方差分解是对学习器的期望泛化误差进行物理含义上的拆解，以理解学习任务内在的误差决定因素。 设有 T 批独立同分布的数据 $D_t, (t = 1, 2, …, T)$ ，同一个学习器会在不同批的数据下产生不同的模型，即 $\cal{A} (D_t) = g_t$ 。 设 $\bar{g} = \lim{T \to \inf} \frac{1}{T} \sum{t=1}^T g_t$ ，那么学习器的期望预测为 $\bar{g} (x)$ 。 于是学习器的期望泛化误差为$$avg(E_{out} (g_t)) = avg((gt - \bar{g})^2) + E{out} (\bar{g}).$$也就是说，泛化误差可以分解为方差与偏差之和：偏差 $E_{out} (\bar{g})$ 度量了学习器的期望预测与真实结果的偏离程度，即学习器本身的拟合能力；方差 $avg((g_t - \bar{g})^2) $ 度量了不同批数据所导致的学习性能的变化，即数据扰动对学习器的影响。 当训练不足时，学习器的拟合能力不够强，数据扰动的影响不大，此时偏差 $E_{out} (\bar{g})$ 主导了泛化误差，偏差越大表明学习器越难以学到数据的真实模式；随训练程度加深，学习器的拟合程度越来越高，数据扰动渐渐能够被学习器学到，此时方差 $avg((g_t - \bar{g})^2) $ 主导了泛化误差，方差越大表明学习器越易发生过拟合。决策树的层数、神经网络的训练轮数、集成学习的基学习器个数都是用于控制训练程度的。 bagging bootstrap aggregation; uniform aggregation. 若基学习器对数据的随机性很敏感，那么 bagging 会工作得很不错，因为它会降低学习器的方差。可以看做是一种正则化的处理。 bagging 得到的是更中庸的 G，而 blending 得到的是更强的 G。 AdaBoost（adaptive boosting） linear aggregation. bagging 可看作是对每个样本赋不同的权重（即不同的惩罚系数），由这一思想延伸，设法通过 re-weighting 使得每次得到的 $g_t$ 差别都很大，进而使得中庸的 $G$ 更强。 其实现方法为：设置本次为每个样本赋的权重，使得上次的 $gt$ 表现很差（即准确率为 0.5），其具体迭代式为$${^{u{t+1}^F = u_t^F (1- \epsilont)} {u_{t+1}^T = u_t^T \epsilont} ,$$或$${^{u{t+1}^F = u_t^F * \Deltat} {u_{t+1}^T = u_t^T / \Delta_t} , \Delta_t = \sqrt{\frac{1- \epsilon_t}{\epsilon_t}} .$$其中 $u_t^F$ 为上次犯错的样本的权重，$\epsilon_t$ 为上次的错误率。 其本质是把上次分错的样本加大错误惩罚力度，把上次分对的样本减小错误惩罚力度，从而得到不一样的 $g_t$ ，综合后便能得到很强的 G 。 综合时与 blending 不同，采用 aggregation on the fly 策略（这也是称为 boosting 的原因）：在得到每一个 $g_t$ 的同时得到其对应的权重 $\alpha_t$ ，$\alpha_t = \ln \sqrt{\frac{1- \epsilon_t}{\epsilon_t}}$ ，若 $\epsilon_t &lt; 0.5$ ，那么 $\alpha_t &gt; 0$ ，与常理相符。最终$$G(x) = sign ( \sum_t \alpha_t g_t(x)).$$ 决策树]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理基础]]></title>
    <url>%2F2018%2F04%2F19%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[术语 词典（词表）：目标语言的所有单词，记为$\mathcal{D}$。 语料库（Corpus）：即手中的数据集，记为$\mathcal{C}$。 上下文（context）：词汇所处的共现词、语境、前邻词、后邻词等词汇出现的篇章语境信息。 词符（tocken）：目标语言中一个词的标记，即指一个单词。 词串：一系列词符前后连接成串。 词串共现：两个词串在同一个句子中。 历史词：出现在该词符之前的所有词。 统计语言模型（statistical language model）统计语言模型是基于大数定律，结合贝叶斯公式，利用语料库来计算一个句子（或词串）的概率的。n 个词串共现的概率为：$$P(W) = P(w_1)P(w_2|w_1)…P(w_n|w_1,w2,…,w{n-1})\where, \quad P(w_i) = \frac{count(w_i)}{count(\mathcal{C})}\P(w_i|w1,…,w{i-1}) = \frac{count(w1,…,w{i-1},w_i)}{count(w1,…,w{i-1})}$$求解该模型的方法有很多，n-gram 模型、决策树、最大熵模型、最大熵马尔科夫模型、条件随机场、神经网络等。 当所有的概率值都计算好之后便存储起来，下次需要计算一个词串的概率时，只需找到相关的概率参数，将之连乘即可。 n-gram添加 n-1 阶马尔科夫假设，得到 n-gram 模型（假设为 3-gram）：$$P(w_i|w1,…,w{i-1}) = \frac{count(w{i-2},w{i-1},wi)}{count(w{i-2},w_{i-1})}$$ 当 n 越大： 模型对历史词的关联性越强，故可区别性越好（模型复杂度越高）； 因模型复杂度呈指数级增高，大数定理的可靠性便越来越差（可理解为一种过拟合现象，因为模型复杂度相对于样本复杂度过高）。 数据稀疏问题 产生该问题的根本原因是采用了统计语言模型。 语料库中可能出现$count(w1,…,w{i-1},w_i) = 0$的情况，即该词串永远不会出现，但不应认为$P(w_i|w1,…,w{i-1})=0$； 语料库中可能出现$count(w1,…,w{i-1}) = count(w1,…,w{i-1},w_i)$的情况，但不应认为$P(w_i|w1,…,w{i-1})=$1。 这两种问题称为数据稀疏问题，该问题的出现与语料库的大小无关，由Zipf定律知，在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比，故增大语料库依然无法解决数据稀疏问题。 在 n-gram 模型中，当 n 达到一定值后，即使样本复杂度足够，由于数据稀疏问题，n 越大，性能反而越差。 平滑技术平滑技术是针对数据稀疏问题引入的技术，常用的有： 平滑：拉普拉斯平滑（加一平滑）、Lidstone 平滑（加 delta 平滑）、good-turing 平滑。 回退：backoff、interpolation（软回退）、kneser-ney smoothing。 NNLMBengio et. al（2003） 词符表示（tocken）discrete representation独热（one-hot representation）：可认为是一种基于符号的词表示方法。 distributed representation 基于分布式相似度的表示，英文全称为 distributional similarity based representations。 You shall know a word by the company it keeps. – J.R.Firth 1957:11 共现矩阵（cooccurence matrix）用上下文来表示一个单词的方法。有两种计算方法： 基于整个段落的，又称为潜在语义分析。 基于窗口的（窗口内一般为5~10个单词，共现矩阵的行是窗口个数，列为所有不重复单词个数），优点是将语义和语法都考虑了进去。 缺点是维度灾难，常用 SVD 来压缩矩阵以实现降维。 词向量]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——卷积神经网络基础]]></title>
    <url>%2F2018%2F04%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[前言 卷积神经网络适用场景：输入信号在空间上和时间上存在一定关联性的场景。 左侧为基础网络结构的调整，右侧为基础网络深度的增加。 基础概念卷积（convolution）定义$$(f*g)(n) = \int_{-\infin}^{+\infin}f(x)g(n-x) \mathrm{d} x$$ 两个在某种角度上具有一定关联性的函数 f 与 g，在二者自变量之和为一常数 n 的约束（更一般化地，在二者自变量的线性组合为一常数的约束）下，两函数之积在某个区间上的积分称为这两个函数在该区间上的卷积。 总之，两函数的卷积是两函数之积在某种线性组合（如 x+y=n）的约束下的特殊积分。 命名之所以称为卷积，是因为该运算的结果是两函数的张量积构成的平面（或超平面）沿两函数各自自变量线性组合等式约束的轨迹做卷褶得到的降维的线（或超平面）上的某一点（线性组合等式约束决定的点）的值。 简而言之就是将张量积卷褶后的重合点之和即为卷积。 理解角度 从对单通道二维图像做卷积的角度理解： f 为图像像素值对位置的函数，g 为实现某种功能的滤波器（又称为卷积核、模板），其具体操作为对两矩阵的哈达玛积的所有元素求和。 能实现这种这种功能的原因是由卷积结果的形式决定的：卷积结果的形式可表示为$f*g = g(f) = af_{0}+bf_2+…$，对邻域像素的线性组合即为线性滤波器。 而为了实现滤波器这种功能，所以采用卷积的形式来操作：在单通道二维图像上实现对邻域像素的线性组合抽象为数学表示便是，两向量在索引服从一定关系的约束下在某个索引区间内的内积，即卷积。 要想实现更加复杂的非线性滤波器，需要用一大堆的简易非线性滤波器，并通过很多层的组合得到。NIN（network in network）提出了一种“偷吃步”的方法（MLPconv）来降低计算复杂度和模型复杂度。可从两个角度来理解 MLPconv： （假设第一层的卷积核与输入图像相同大小） 从结构形式的角度：$k_1$ 个卷积核输出到 $k_1$ 个神经元上，再假设第二层的卷积核与第一层神经元数大小相同，$k_2$ 个卷积核输出到 $k_2$ 个神经元上，即对$k_1$种线性滤波的非线性激活结果再次进行$k_2$种线性滤波，依此继续连接；能够由此实现任意一种非线性滤波器的原因与传统神经网络能够模拟任意函数的原理一样。 故 MLPconv 本质为一个传统的神经网络（理解本质时用），即将一个传统的神经网络作为卷积核，神经网络隐藏神经元数由卷积核数决定，即希望得到的非线性滤波器个数。 由此，可将线性滤波器看作 MLPconv 的特殊情况，即一个感知机。 从物理含义的角度：$k_1$ 个卷积核输出$k_1$个特征图（此处每个图仅为一个点），而第二层的每个感知机是对$k_1$个通道的特征图的同一个位置作线性组合操作，然后做非线性激活，该操作可理解为对$k_1$个通道的特征图的同一个位置做 1×1 大小卷积核的卷积，或者理解为对$k_1$级联的特征图做 1×1 大小卷积核的池化（理解为池化，是目的导向的，因为此操作的目的是对不同特征进行不同方向的聚合；其与池化不同，因为参数都是要学习的，而非固定的）；能够由此实现任意一种非线性滤波器的原因是不同特征的不同方向的多次聚合能够得到一种任意一种非线性特征。 故 MLPconv 等价于一个 k×k 卷积层后缀数个 1×1 卷积层（思考网络结构及实际编码时用）。 其中 1×1 卷积层也有人称之为跨通道参数的级联池化（cccp），实现跨通道的信息整合。 1×1 卷积层的意义主要在于线性组合，并作非线性激活；想当于做了一次非线性组合。 简单来说，非 NIN 结构的多层卷积是跨越了不同尺寸的感受野，在相同范围内的感受野只有一次简易非线性滤波；而 NIN 结构的多层卷积是作用在同一尺寸上的感受野，可认为在相同范围内的感受野由一次复杂的非线性滤波，能够提取更强的非线性特征。 针对不同尺度特征自然需要不同大小的卷积核，考虑到不同尺度特征可能属于同一级别的抽象，故提出了 inception 结构。该结构本质是对几个不同大小卷积核的 MLPconv 的结果的 concat。该结构的合理性是基于以下两点原因的： MLPconv 本质为一个小型神经网络，1×1 卷积层与 k×k 卷积层的先后顺序影响不大，又考虑到通道数庞大的可能性，先做 1×1 卷积可以极大地减少参数数量，提高泛化能力。 不同尺度特征自然需要不同大小的卷积核，而不同尺度特征可能属于同一级别的抽象。 inception-v1 结构如下图所示（从左至右，为提取尺度越来越大的特征，最左侧的特征可认为是在该抽象程度上尺度为零的特征）： ​ 对 inception module 如此设计的理解： 结合 NIN 结构和 1 * 1 卷积可升降维减少参数，实现在相同尺寸的感受野中提取到更加非线性的特征； 考虑到 GPU 只对密集矩阵计算作了优化，没有对稀疏矩阵计算做优化，又考虑到稀疏矩阵可分解成密集矩阵计算的原理，通过在多个尺寸上同时进行卷积再聚合的方式，实现网络的稀疏结构。比如当要输出一个 256 通道的 feature map 时，若只用一种卷积核进行卷积，256 个输出特征便均是在同一尺度范围上的，极有可能是一个稀疏分布的特征集；而 inception 结构会在多个尺度上提取特征（如 1 1 的提取 96 个特征，3 3 的提取 96 个特征，5 × 5 的提取 64 个特征），这样相关性更强的低级特征便会被组合称为高级特征，而不相关的低级特征（非关键特征）便会被弱化而不使用，同样输出 256 个特征，inception 结构输出的特征“冗余”信息会更少，从而间接地提高了网络的稀疏性。（因为网络越稀疏收敛速度越快，且越不易过拟合，故都追求更加稀疏的网络） VGG-net 中的 trick：用多个 3×3 卷积核代替更大面积的卷积核，这不仅减少了参数，减轻过拟合，而且多次卷积代替一次卷积的方案更增加了非线性，有利于特征的提取。后来在 inception-v3 中发现，非对称的卷积结构拆分比用多个 3×3 卷积进行拆分的效果更明显，参数也更少。这被称为 Factorization in Small Convolutions 思想。 ResNet 中的 trick：引入shortcut， 解决了网络退化问题； 使得信息更容易在各层间流动，实现了前向的特征重用； 使得深层网络成为了很多浅层网络的集成，比如一个残差块只有两条通路的 resnet，若有 4 个残差块，那便是 $2^4$ 个浅层网络的集成。 在神经网络上的实现方法及理由（任务依赖的正则化） 局部感知：每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息； 参数共享：图像具有一种“静态性”的属性：图像的一部分的统计特性与某些其他部分是一样的，即平移不变性的特征。由此提出了卷积核和特征图的概念。 池化（polling）池化过程本质为特征突出过程，去除特征图中的无用像素点。（查看西瓜书的样本不均衡问题） 对深度模型的一些优化 ReLuhttps://www.zhihu.com/question/59031444/answer/177786603 LRNLRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。 很少再用，现多用 dropout。 dropout 一种更恶化环境训练、更舒服环境战斗的思想。 Hinton 认为 dropout 是通过特殊的训练策略实现的隐式的模型集成。 达到了一种Vote的作用。对于全连接神经网络而言，我们用相同的数据去训练5个不同的神经网络可能会得到多个不同的结果，我们可以通过一种vote机制来决定多票者胜出，因此相对而言提升了网络的精度与鲁棒性。同理，对于单个神经网络而言，如果我们将其进行分批，虽然不同的网络可能会产生不同程度的过拟合，但是将其公用一个损失函数，相当于对其同时进行了优化，取了平均，因此可以较为有效地防止过拟合的发生。 减少神经元之间复杂的共适应性。当隐藏层神经元被随机删除之后，使得全连接网络具有了一定的稀疏化，从而有效地减轻了不同特征的协同效应。也就是说，有些特征可能会依赖于固定关系的隐含节点的共同作用，而通过Dropout的话，就有效地组织了某些特征在其他特征存在下才有效果的情况，增加了神经网络的鲁棒性。 简而言之：Dropout在实践中能很好工作是因为其在训练阶段阻止了神经元的共适应。 应对梯度爆炸/弥散ResNet ResNet 是一种学习残差的框架，即用非线性层去显式地拟合一个残差映射，而不再是去拟合一个期望的潜在映射。 解决了随着网络层数不断加深，求解器不能找到解决途径的问题。 code: resnet_tf 深度神经网络在 ReLU 和 BN 层的加入后，网络变深不再有梯度弥散的问题，但却会出现随着网络的加深，准确度反而下降的现象，即退化现象。 ResNet 使得下述所需的函数变得更易拟合求解得到：通过几个非线性映射的堆叠去拟合一个恒等映射，由此使得对某个低级特征贡献不大的输入可以一下子被拉到更深的层，其对更高级特征的贡献由更深层的权重学习到。至于输入对哪种抽象特征的贡献度更大，在求解最优化的过程中可自动学习到。 从集成学习的观点，可以将更深的网络看做是在增加该指数的幂次（ ResNet ），将更宽的网络看做是在增加该指数的基底（ ResNeXt ）。 实现方面的细节反向传播卷积层的梯度反传卷积神经网络中的梯度反传是 2D，与传统神经网络中的 1D 反传不同，但其数学本质相同（见梯度反传），只是表现形式上有略微的不同： 当已得到卷积层误差，求对卷积核参数的梯度时，以卷积层误差为卷积核对与卷积核参数处于同一层的 feature map 做卷积（1D 反传时是误差与激活值直接做内积）得到在卷积核参数处的梯度； 若要继续反传得到图中卷积层的上一层（图中称为池化层是默认一个卷机后接一个池化）误差，须将卷积核参数旋转 180 度后对卷积层误差做 full 类型（其他类型还有 valid，same ）的卷积，便得到了上一层的误差，进而递归地用第一条。 池化层的梯度反传池化层不需要计算梯度，因为没有要训练的参数，但在梯度反传时，误差的形状需要发生变化以保证和上一层的参数能够对位，处理方式的原则是：保证传递的误差总和不变： 对于均值池化，便是把误差每个元素等分成n份传递给前一层； 对于最大值池化，需要先在前传时记住最大值的位置，反传时将误差对应到所记录的位置上，其他位置置零。 后记对 CNN 的研究主要有三种方法： 观察学习到的 filter 通过对抗样本 通过破坏网络结构，观察性能变化]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[待办事项]]></title>
    <url>%2F2018%2F04%2F10%2F%E5%BE%85%E5%8A%9E%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[[ ] 回归为何能用于分类【机器学习——线性模型】 [ ] weight-decay 中的$\lambda$与常数 C 的解析关系是什么？【机器学习——线性模型】 [ ] 为什么 X 存在多重共线性时，最小二乘法求得的 w 在数值上会很大？【机器学习——线性模型】 [ ] 为什么l1正则化更易导致模型稀疏（即解更易在角上取得） [ ] SGD原理 [ ] ​]]></content>
      <categories>
        <category>board</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[信息检索与知识管理]]></title>
    <url>%2F2018%2F04%2F02%2F%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E4%B8%8E%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[信息检索电子期刊数据库 北京万方数据股份有限公司网上数据库联机检索系统 文献索取：图书馆学术交流与文献互助联盟 重庆维普中文科技期刊数据库 知网：中国学术期刊全文数据库 电子图书数据库 全球最大的中文在线图书馆：超星电子图书 中文在线电子图书 学位论文全文数据库 万方“中国学位论文全文数据库“ 中国知网“中国博士学位论文全文数据库”；中国知网“中国优秀硕士学位论文全文数据库” 读秀知识库百链中英文学术搜索国家科技图书文献中心]]></content>
      <categories>
        <category>工具的使用</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——广义线性模型]]></title>
    <url>%2F2018%2F04%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[线性模型线性回归 所有变量的可表示的数量上的减少、衰退 regress，称为回归，即确认独立变量的过程。比如，独立变量和其他非独立变量之间的关系近似于线性时称为线性回归。 回归是一种降维方法，减少的维度为非独立变量个数。 线性回归模型 输入空间映射到整个实数区间 $$\min_w ||Xw-y||$$ 最小二乘 VS 正交投影最小二乘法与正交投影变换是对同一个问题两种不同等价形式的解决方法，且为相同的结果： 同一个问题：对一堆点求其回归方程，即$$ \min_w ||Xw - y ||$$该问题的两种等价形式为 线性方程组$Xw=y$ 的近似解 最小二乘问题$x^*=\min_w (Xw-y)^T(Xw-y)$ 对于第一个等价形式，X 的列向量的线性组合无法得到 y，那么将 y 正交投影至 X 的列空间，便可得到线性方程组的一种近似解：$Xw=X(X^TX)^{-1}X^T y \Rightarrow w = (X^TX)^{-1}X^T y$ 。 对于第二个等价形式，可用导数零点解决该无约束最优化问题：$\frac{\partial (Xw-y)^T(Xw-y)}{\partial w}=2X^T(Xw-y)=0 \Rightarrow w=(X^TX)^{-1}X^T y$ 。 更进一步： 若$X^TX$不是满秩矩阵，即$X$非列满秩，则最小二乘法与正交投影变换都失效了，此时$w$自由度为$n-rank(X^TX)$，常用的一个解是加入一个归纳偏好$\min ||w||_2$：将$X^TX$进行奇异值分解进而求得伪逆，从而得到$w$。 通常$X^TX$是可逆的，因为样本数$\gg$ 特征维度+1。但实践中常因为数值稳定性用$X^+$代替$(X^TX)^{-1}X^T$。 如果输入矩阵X中存在线性相关或者近似线性相关的列，那么输入矩阵 X 就会变成或者近似变成奇异矩阵（singular matrix）。这是一种病态矩阵，矩阵中任何一个元素发生一点变动，整个矩阵的行列式的值和逆矩阵都会发生巨大变化。这将导致最小二乘法对观测数据的随机误差极为敏感，进而使得最后的线性模型产生非常大的方差，这个在数学上称为多重共线性（multicollinearity）。在实际数据中，变量之间的多重共线性是一个非常普遍的现象。 对数几率回归（逻辑回归） 几率（odds）：（样本作为正例的可能性 / 反例的可能性，即$\frac{y}{1-y}$）正例的赔率，反映了样本作为正例的相对可能性。 对数几率回归模型 输入空间映射到 [0, 1] 区间 $$\min_w ||Xw- \ln \frac{y}{1-y}||$$ 在形式上仍是线性回归，是对线性回归模型的扩展，即令模型逼近$y$的衍生物，这里的对数几率函数起到了将线性回归模型的预测值与真实标记联系起来的作用，称之为联系函数。当考虑其使用时称之为逻辑回归模型$\frac{1}{1+e^{-Xw}}$。 局部加权线性回归（LWR） 解决对不规则函数进行回归时容易出现的欠拟合与过拟合问题 其流程是，每次预测时都需要调用所有的样本$X$，结合预测点来拟合回归曲线。 其原理是，选择与预测点$x^*$相近的点来做线性回归，忽略远处的点对预测的影响。 其实现方式是加权最小二乘：$$\minw ||\lambda (Xw - y)||\s.t. \quad \lambda{i} = e^{-||x_i - x^*||_2^2}$$ 缺点：对于每一个要预测的点，都要重新依据整个数据集计算一个线性回归模型出来，使得算法代价极高。 线性判别分析（LDA）待加 广义线性模型 大多数的概率分布都能表示成指数分布族的形式，如高斯分布，对噪声和不确定性进行建模；伯努利分布，对有两个离散结果的事件建模；多项式分布（Multinomial），对有K个离散结果的事件建模；泊松分布（Poisson），对计数过程进行建模，如网站访问量的计数问题；指数分布（Exponential），对有间隔的证书进行建模，如预测公交车的到站时间的问题；等等。通过进一步的推导，就能得到各自的线性模型，这大大扩展了线性模型可解决问题的范围。 广义线性模型：用某种指数分布去逼近数据真实分布的广义线性回归。 $$P(y; \eta)=be^{\eta^T T(y) - a}$$ GLM 的三个假设： $p(y | x; w ) \sim be^{\eta^T T(y) - a}$ ：y 基于 x 的条件概率服从指数分布族中以$\eta$为参数的某个分布； 学习的目标是预测 T(y) 基于 x 的条件期望，因为 T(y) 通常为 y，即目标函数为$E(y∣x; w)$，故线性模型的本质是让某个指数分布的期望去逼近 y； $\eta$ 和 x 的关联是线性的，即$\eta = w^T x$，从而 linear regression 线性函数是高斯分布的期望 $u$ 在线性回归模型上的表现形式，即在噪声影响下最可能的值。 因为$u$ 在实数区间取值，故应用于回归。 $$\sigma = z$$ 高斯分布：$$p(y;u) = \frac{1}{\sqrt{2 \pi} \sigma}e^{- \frac{(y-u)^2}{2 \sigma^2}}\= \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2}y^2} e^{uy - \frac{1}{2}u^2}$$将高斯分布与广义线性模型对比得到：$$\eta = u \Rightarrow u = \eta$$从而得到线性回归的数学模型（假设集）。 sigmod logistic 函数是伯努利分布的期望 $\phi$ 在线性回归模型上的表现形式，即单次正例事件发生的概率；或说是伯努利分布体现在线性回归模型上的函数。 因$\phi$在 (0, 1) 区间取值，故应用于分类。 $$logisticfunction \quad \sigma = \frac{1}{1+e^{- z}}$$ 伯努利分布：$$p(y; \phi) = \phi^y (1-\phi)^{N-y}\\quad \quad \quad \quad \quad = e^{(\ln \frac{\phi}{1-\phi})y + \ln (1-\phi)^N}$$当只针对一个样本来看时，伯努利分布便降为 0 / 1 分布；当针对整体来看时，y 便是正例发生的次数统计，目标函数为$sum( \phi)$ 。 将伯努利分布与广义线性模型对比得到：$$\eta = \ln \frac{\phi}{1- \phi} \Rightarrow \phi = \frac{1}{1+e^{- \eta}}$$从而得到逻辑回归的数学模型（假设集）。 softmax softmax 函数是多项式分布的期望$\phi_i$在线性回归模型上的表现形式，即某类事件发生的概率。 因$\phi_i$在 (0, 1) 区间取值，故应用于分类。 $$softmaxfunction \quad \sigma = \frac{e^{zi}}{\sum{j=i}^{k} e^{z_j}}$$ 上述的伯努利分布与多项式分布均是针对一个样本来说的。 回归用于分类其本质仍是回归，只是在最后一步做了处理以用于分类， 总结线性模型的思想是用某种指数分布去逼近数据真实分布，从而实现某种预测；其实现方法是让某个指数分布的期望去逼近 y，然后用最小二乘或最大似然之类的方法构建模型。 任务的不同类型是依据期望的取值空间划分的。 Q 次多项式回归 线性不可分数据可能圆形可分，或者其他二次曲线可分，甚至更一般化 Q 次曲线可分，由此引出 Q 次多项式回归的想法。 Q 次多项式特征转换 人工提取特征的过程可认为是某种特征转换，此处提出的是一般化的 Q 次多项式特征转换。 $$z = \phi (x) = (1, x_1, x_2, x_1^2, x_2^2, x_1 x_2)$$ 对于线性不可分的 X 可经过 Q 次多项式特征转换$\phi$（下文简称为特征转换）变为线性可分的 z。该过程可细述为： 特征转换$\phi$将 d+1 维的 X 空间转换为 $C{d+Q}^Q$维的 Z 空间，其中 $C{d+Q}^Q = O(d^Q)$ ；VC 维也由 d+1 变为了 $C_{d+Q}^Q$，模型复杂度呈指数级增长，记为$\mathcal{H}_1$升级为了$\mathcal{H}_Q$，其关系为$\mathcal{H}_1 \subset \mathcal{H}_2 \subset … \subset \mathcal{H}_Q$。 legendre polynomials：正交化的特征转换。避免同数量级的 x 经普通的特征转换后在不同特征下存在好几个数量级的差距，如 $x$与$x^{10}$，从而使得在特征转换后 w 仍能具有同等的影响力。 特征转换后的模型的 VC 维的推导由公式$$C_n^m+Cn^{m-1} = C{n+1}^m$$得转换后 x 的 Q 次项的项数为：$$C_d^Q+(Q-1)C_d^{Q-1}+…+Cd^1=C{d+Q-1}^Q ,$$故，转换后 x 小于等于 Q 次项的项数为：$$C{d+Q-1}^Q + C{d+Q-2}^{Q-1}+…+C{d+Q-Q}^1+C{d-1}^0\=C{d+Q-1}^Q + C{d+Q-2}^{Q-1}+…+C{d}^1+C{d}^0\=C{d+Q-1}^Q + C{d+Q-2}^{Q-1}+…+C{d+1}^1\=C{d+Q-1}^Q + C{d+Q-1}^{Q-1}\=C{d+Q}^Q$$ 线性软间隔支撑向量机详见支撑向量机 核模型核函数核脊回归（KRR）常用 SVR 替代 核逻辑回归（KLR）常用 probabilistic SVM 替代 模型的选择过拟合 过拟合：指先前的模型$E{in}&lt;E{out}$，而现在升级后的模型$E{in}$更小、$E{out}$更大，称这种现象为过拟合； 泛化能力差：指该模型$E{in}&lt;&lt;E{out}$。 出现原因（4个）模型复杂度过高、数据量有限、随机噪声或确定性噪声过大。 低复杂度数据下，为什么高复杂度模型会出问题： 在保证泛化误差的置信度为$1-\delta$ 的前提下（若无此保证，机器学习则无从谈起），由 VC bound 得到泛化误差为： $$\epsilon = \sqrt{\frac{8}{N} \ln (\frac{4(2N)^{d_{VC}}}{\delta})},$$ ​ 故当在低复杂度数据的情况下，复杂度越高的模型会出现更高的泛化误差。 确定性噪声指隐含模式可能是复杂度很高的模式，而这种高复杂度的模式就像在某种低复杂度模式上加入了随机噪声，从可行性考虑将其归为噪声。称之为确定性是因为在确定了输入数据 X 后，“噪声”便可由隐含模式确定地得到，而不再是随机的。 直觉上，噪声相当于降低了有效数据的数量，相较于有效数据的数量模型成为高复杂度的模型，进而出现问题，解释见第一条。也有人解释为该模型更有可能将噪声的模式也学习进去。 根本原因还是第一条。 解决方法（4个） 从简单模型开始； 对数据进行清理（删掉高噪声数据）或修剪（修正错误标记数据）； 人工增加数据（可能影响原始数据的真实分布，数据的增加方式应尽可能依照原始数据的隐含模式）； 正则化。 附：正则化（regularization) 其思想是将高复杂度模型进行退化。 命名由来：对不适定问题（ill-posed problems）的近似解。 两种解释 从机器学习的角度： 其思想是将高复杂度模型进行退化，故用稀疏假设集来降低模型的复杂度。比如加入约束条件$\sum bool(w \neq 0) \leqslant 2$记为$\mathcal{H}_2’$，显然$\mathcal{H}_2 \subset \mathcal{H}2’ \subset \mathcal{H}{10}$。不过因为布尔操作，该问题的求解已被证明是 NP-hard 问题。 为使原 NP-hard 问题易解，将其转化为 soft 版本：约束条件换成$\sum w^2 \leqslant C$，记为$\mathcal{H}(C)$，显然$\mathcal{H}{10}(0)\subset \mathcal{H}{10}(1)\subset … \subset \mathcal{H}{10}( \infin) = \mathcal{H}{10}$。 进而机器学习模型（将约束条件看做$\mathcal{A}$的一部分，假设集$\mathcal{H}$仍为那个大的假设集$\mathcal{H}_{10}$，能产生多大数量的假设要看数据）变为$$\minw E{in}(w)\s.t. \quad \sum w^2 \leqslant C$$由拉格朗日乘子法及一些粗略的化简得到等价问题：$$\minw E{in}(w) + \lambda ||w||_2^2, \quad \lambda &gt; 0.$$其中常数 C 隐含在 $\lambda$中。 从统计的角度： X 存在多重共线性时，最小二乘法求得的 w 在数值上会很大，如果 x 有一点小小的变化，输出结果会有很大的差异，即对X 中的噪声非常敏感，所以其解会非常不稳定。而若能限制 w 的增长，模型对噪声的敏感度便会降低，由此引出脊回归（Ridge regression）：$$L = \minw ||E{in}(w)||_2^2+ \lambda ||w||_2^2 ,$$即在原损失函数上加入 w 的2-范数的惩罚项。 对损失函数求导置零得到解：$$w = (X^TX+ \lambda I)^{-1} X^Ty$$ 从贝叶斯推断的角度： 正则化是对待推断参数的先验。 正则化方向 损失函数的构造也是从这三个方向出发考虑的。 任务依赖：看任务的具体特性，进行某种特殊的正则化。 普遍看似合理的方向：使模型更平滑或更简单（因为随机噪声或确定性噪声都是非平滑的），由此得到 sparsity regularizer（稀疏正则化）$||w||_1$。 更易于求解的方向：由此得到 weight-decay regularizer（权重衰减正则化）$||w||_2^2$。 验证集作为$\mathcal{D}_{out}$的替代来进行模型选择的可行性 用$E_{in}$作选择易过拟合，不可靠； 假设$g_1$、$g_2$是$\mathcal{H}_1$、$\mathcal{H}2$由$E{in}$选出的，若再由$E{in}$选出$g^$，那么你的$g^$对应的模型复杂度便是$d{VC}(\mathcal{H}_1 \cup \mathcal{H}_2)$，样本复杂度不变而模型复杂度变高，自然易出现过拟合。 用$E{test}$作选择是不诚实的做法，因为$E{test}$的结果是用于作报告用的。 理应用$\mathcal{D}{out}$做选择，但这是不可能的，由此引出$\mathcal{D}{val}$作为替代的解决方案。 $$\mathcal{D} = \mathcal{D}{train} \cup \mathcal{D}{val}$$ 验证集需在数据集中随机选（经验值为$\frac{N}{5}$），这保证了验证集与总体独立同分布。 根据霍夫丁不等式，当验证集与总体独立同分布时，$E{val}$能在一定程度上代表$E{out}$： $$E{out}-E{val} = \mathcal{O}(\sqrt{\frac{\log M}{N_{val}}})$$ 模型选择的流程：不同复杂度的模型在训练集上得到各自的解$g$后，再测试各个解在$E_{val}$上的表现，将表现最好的模型在整个数据集上再次训练得到最终的$g^*$，并将该解在测试集上的表现作为最终汇报结果。 $E_{val}$的几种操作方案 当计算力允许的情况下，选择 K 折交叉验证；否则，选用20%作为验证集直接得结果；而若模型有解析解，可以选用留一交叉验证。 因为验证集是从手中的数据集中分离出来的，故其大小的选择存在如下困境： 过小，则$E{val}$到$E{out}$的泛化误差太大，即验证集没有作模型选择的能力； 过大，则$E{in}$到$E{out}$的泛化误差太大，即训练集没有训练该模型的能力，或说更易出现过拟合现象。 经验值为$\frac{N}{5}$，但考虑到$E_{val}$的稳定性，还提出了以下两种方案： leave-one-out cross validation： ​ 除上图所示的保证之外，每次做训练的训练集也是极大化的，故模型的解也是最可靠的，自然得到的验证结果也是最可靠的。由此可见$E_{loocv}$是极佳方案，但当模型没有解析解，而是用迭代优化来求解时，该模型选择的方法因计算消耗太大而在实践中不常用。 K-fold cross validation（经验值 K=10）： $E{CV} = \frac{1}{K} \sum{i=1}^K E_{val}^{(i)}$ ###]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——基础]]></title>
    <url>%2F2018%2F03%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[机器学习基础 基础术语 名称 含义 模型 计算模型$\mathcal{A}$（学习法则）与数学模型$\mathcal{H}$（假设集）的合称为机器学习模型 $E_{in}$ 假设 h 在已得到的资料上与真实模式 f 的误差 $E_{out}$ 假设 h 在未见过的资料上与真实模式 f 的误差 h $\mathcal{A}$ 从假设集$\mathcal{H}$ 中取出的一个假设函数 g 机器学习模型最终确定的在当前任务中用于代替真实模式 f 的估计模式 若无特殊说明，一般模型一词特指数学模型$\mathcal{H}$ 。 前言ML / DM / statistics ML 与 DM 很难区分 ML 是利用资料计算出接近真实模式 f 的估计模式 g statistics 是利用资料推断一个尚不知结果的进程的结果的概率 应用场景当任务存在某种潜在模式，但不能很容易地程式化地总结出来时。（前提是与该模式有关的资料是要能够获取到的） 分类实现的可行性 模型与数据集大小两者之间没有先确定谁再确定谁的先后顺序：因为成本问题，数据集自然希望需要得越少越好，当数据量过小时便不可选复杂度过高的模型；而为使模型误差能够足够小，模型复杂度便须足够高，高复杂度的模型需要高样本复杂度的数据。总之，这是一个数据成本与模型误差博弈的过程。 VC（Vapnik-Chervonenkis）维 $d_{VC}$ 的含义： 模型自由参数的个数，或称为模型的自由度（向量$w$ 的维度） 模型的强度：表示模型什么时候还能够 shatter 注意：机器学习模型的 VC 维指数学模型的有效 VC 维，会根据计算模型变化（如加入不同的正则化项）而变化；常说的模型复杂度是指数学模型有效 VC 维对应的复杂度。 断点（breakpoint） shatter：若模型包含某次 N 个输入样本可能出现的所有情况（即模型能产生至少 $2^N$ 种假设），则称该输入能被该模型 $\mathcal{H}$ shatter 成长函数 $m_{\mathcal{H}}(N)$：模型能产生的最多（相同样本量不同样本的情况下，模型会产生不同的假设个数，此处为最多）的假设个数关于样本量的函数 断点 breakpoint：若N=k，成长函数首次不是指数级时，称 k 为最小断点 VC 维：$d_{VC} = $（最小断点k ）- 1，即模型能 shatter 的最大样本数 当断点出现后，模型的成长函数便与模型的细节（线性分类器还是圆形分类器等细节）无关了：当 $N \geqslant k$ 时，$m{\mathcal{H}}(N) \leqslant B(N,k)$ ，进而能得到 B(N, k) 的表格，由表格可推得 $B(N,k) \leqslant \sum^{k-1}{i} C_N^i \leqslant N^{k-1}$ 。 模型复杂度由霍夫丁不等式（给出了训练集误差无法代表整体误差的概率上限）：$$P{ |E{in}(h) - E{out}(h)|&gt; \epsilon } \leqslant 2e^{-2 \epsilon^2 N}$$知，针对任意一个假设 h，只要取样容量 N 足够大，不好的取样发生的概率很小。 因为数据集的好坏应该是针对模型来说的，故只有下列式子足够小，才能说数据集是好的（从直观上来说数据集好是指手中的资料已经可以代表所有已知和未知的资料了）：$$P{ \exists h \epsilon \mathcal{H}, s.t. |E{in}(h) - E{out}(h)|&gt; \epsilon } = P{\sum{h \epsilon \mathcal{H}} [ |E{in}(h) - E{out}(h)|&gt; \epsilon ] }$$设模型能产生的假设个数为$M$，由和事件的概率$ \leqslant$ 概率的和得到不好的取样发生的概率为：$$P{\sum{h \epsilon \mathcal{H}} [ |E{in}(h) - E{out}(h)|&gt; \epsilon ] } \leqslant 2 M e^{-2 \epsilon^2 N}\^{用E{in}’ 替代E{out}}{又{|E{in}-E{in}’| &gt; \epsilon / 2} \Leftrightarrow {|E{in}-(E{in}+E{in}’)/2| &gt; \epsilon / 4} }\Longrightarrow\\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \leqslant 4(2N)^{d{VC}} e^{-\frac{1}{8} \epsilon^2 N}\\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad = \delta \quad (VC bound)$$由 VC bound得到：$$\epsilon = \sqrt{\frac{8}{N} \ln (\frac{4(2N)^{d{VC}}}{\delta})}$$一般将该值记为$\Omega(N, \mathcal{H}, \delta)$ ，称为模型复杂度，它是样本复杂度、模型的 VC 维和 VC bound 的函数。故在置信度为$1-\delta$ 的情况下，泛化误差$|E{in}-E{out} | \leqslant \Omega(N, \mathcal{H}, \delta)$ 。模型复杂度本质是由多个因素影响（此处说的模型是数学模型即假设集，该模型的 VC 维决定了它能 shatter 的最大样本数，模型复杂度越高能够 shatter 的样本数越大；模型的 VC bound 决定了该模型结果【预测结果与泛化误差合称为模型结果】的不可信度，模型复杂度越高模型结果的不可信度越低；样本复杂度决定了针对该模型手中样本的好坏，模型复杂度越高手中样本变坏的可能性越大，另一种理解为样本复杂度决定了该模型能产生的假设个数，模型复杂度越高能产生的假设个数越多）的函数，而泛化误差本质是在一定置信度下的一个数，二者恰巧在数量上相等。 虽然给出预期的置信度、泛化误差和$d{VC}$ 就能得到针对“模型能从样本中学到点什么东西”这件事模型所需的样本复杂度，但一般令$N \approx 10 d{VC}$ 就足够了。 常见模型的 VC 维 举例：n 维的二分类感知机，VC 维为 n+1 当样本量 N=n+1 时，$X \epsilon R^{(n+1)*(n+1)}$ ，存在能够被该模型 shatter 的样本：令样本 X 可逆，则任意一种二分类情况 y 都可以被一个 w 划分出，因为 $w = X^{-1}y$ ； 当样本量 N=n+2 时，$X \epsilon R^{(n+2)*(n+1)}$ ，没有一个能够被该模型 shatter 的样本：因为 n+2 个样本中总有一个样本能被其它 n+1 个样本线性表示，设线性表示的系数为 $a1, … , a{n+1}$ ，则模型无法产生 $(sign(a1), …, sign(a{n+1}), -1)$ 这种二分类情况。 故VC 维为 n+1。 可行性分析 no free lunch 定理：若只坚持 f 是未知的，而不作出任何假设，那么在已知资料以外的部分去说我一定学到了什么东西（即找到了能够满足在已知资料以外的部分 $g \approx f$ 的$g$ ）这件事是做不到的。故机器学习的模型一般都是有某种归纳偏好的。$$\downarrow 对已知和未知资料作出假设：所有数据均是独立同分布的$$ 若样本量够大，模型的 VC 维为有限值，则由霍夫丁不等式可认为样本内误差可以泛化到样本外误差，即 g 能够具有很好的泛化能力；又若计算模型能够从数学模型中找到使得样本内误差趋于零的假设 h 作为 g，则认为学习成功。 加入噪声后噪声是指标签中的噪声，来源多为： 打标签过程人一时走神打错标签 打标签时不同的人有不同的标准 打标签的数据本身有噪声 对于含有噪声的标签，可以认为产生样本的数据源从真实模式 f(X) 变成了 P(y|X) ，即加入了些微抖动，但这只是一个变量的替换，并不影响 VC bound 的成立，故依然认为能够成功学习。 总结机器学习能否成功，就考虑两件事： 数学模型复杂度是否足够高，以有能力使得训练集误差够小； 泛化误差（样本复杂度与模型 VC 维决定）是否足够低，以保证训练集误差能够代表整体误差。 至于学习速度便是计算模型去考虑的事情了。 在此可做个比拟：数学模型是天资，是人本身的智商，而计算模型是学习方法。方法用对了再加上高天资便能平步青云。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机硬件基础]]></title>
    <url>%2F2018%2F03%2F29%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[硬盘基础知识主引导扇区（512字节）包括三部分： MBR：主引导记录或主引导程序，用于硬盘启动时将系统控制转给指定的操作系统。 4个分区表（总64字节）：可为三个主分区表和一个扩展分区表或者四个主分区表。扩展分区表作为特殊的存在，标定了任意数量个逻辑分区表的位置；而主分区表和逻辑分区表标定的是主分区和逻辑分区的位置。 结束标志（2字节）]]></content>
      <categories>
        <category>cs</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数学——信息论基础]]></title>
    <url>%2F2018%2F03%2F29%2F%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[熵 熵可以认为是随机变量的数字特征，其含义为为确定某一随机变量所需信息量的平均。 设$ X \sim p(x)$ ，则 $H(X) = H(p) = -\sum p(x) \log_2 p(x)$ ，单位为比特。 若设 $Y = -\log_2 p(X)$ ，则$H(X)=E(Y)$ ，即随机变量的熵为随机变量函数的期望。$ -\log_2 p(X)$ 意为为确定随机变量 $X$ 的值所需的信息量，若$p(x)$ 越小，则所需信息量越大。 综上，熵$H(X)$ 是 $X$ 信息量的期望。 注：$X$ 指随机变量，$x$ 指随机变量取的值。 方差 VS 熵方差衡量某一变量概率分布的散度（ $X - E(X)$ ），方差越小偏差越小，置信度越高。 熵衡量某一变量概率分布的不确定性（ $-\log (P(X))$ ），熵越小不确定性越小，置信度越高。 基本概念 联合熵：描述二维随机变量$(X_1, X_2)$的熵，$(X_1 , X_2) \sim p(x_1, x_2)$ 。因为概率越小不确定性越大，故联合一个新的随机变量后熵会增大。 条件熵：描述条件随机变量$(X_2 | X_1)$的熵，$H(X_2 | X_1) = E(H(X_2| X_1=x_1))$ ，是在条件 $X_1$ 下引入 $X_2$ 后增加的不确定性。 互信息：$I(X_2;X_1)$ 描述确定了随机变量 $X_1$ 后能给 $X_2$ 提供多少信息量，即 $I(X_2;X_1) = H(X_2) - H(X_2 | X_1)$ ，由 $I(X_1;X_1) = H(X_1)$ ，故熵又称为自信息。 交叉熵衡量两个分布的差异用相对熵，又称 KL(Kullback-Leibler) 散度：$$D(f||g) = \sum{x \epsilon X} f(x) \log \frac{f(x)}{g(x)}\= \sum{x \epsilon X}(- f(x) \log g(x)) - H(X)$$因为数据集定了后，$H(X)$ 自然就成了常数，而剩余的一项因其形式，称之为交叉熵，记为 $CEH(f, g)$ ，为方便计算，在比较大小时，常用交叉熵代替 KL 散度。 从最大似然估计角度理解交叉熵设 g 为估计分布，f 为由样本中统计出的分布，N 为总样本数，由最大似然估计可得其似然函数为$$\prod g(x)^{Nf(x)},$$取对数得：$$\log \prod g(x)^{Nf(x)}\= \sum \log g(x)^{Nf(x)}\= N \sum f(x) \log g(x)$$故最小化交叉熵等价于最大化最大似然函数。 从完美编码角度理解交叉熵对分布 f 编码所需的最小信息量为 $CEH(f,f)$ ，而 $CEH(f,g)$ 均比该值大，称为不完美编码，其中 $g \ne f$ 。]]></content>
      <categories>
        <category>数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——凸优化]]></title>
    <url>%2F2018%2F03%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%87%B8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[凸集 集合$C$ 内任意两点间的线段上的点仍在该集合内，则称该集合为凸集。表示为$\theta x+(1-\theta)y \in C, \theta \in (0,1)$ ，其中$\theta x+(1-\theta)y$ 称为点$x$ 与$y$ 的凸组合。 无约束最优化数值优化牛顿法 本质为找函数的零点，用在最优化上便是找函数导数的零点。 $$w_{t+1} = wt - (\frac{\partial ^2 L}{\partial w \partial w^T})^{-1} \frac{\partial L}{\partial w}|{w=w_t}$$ 牛顿法相比起梯度往往法收敛速度更快，特别是迭代值距离收敛值比较近的时候，每次迭代都能使误差变成原来的平方，但是在高维时矩阵的逆计算会非常耗时。 解析优化有约束最优化拉格朗日乘子法 通过引入拉格朗日乘子，可将有 d 个变量与 k 个约束条件的最优化问题转化为 d+k 个变量的无约束优化问题。 等式约束$$\min_x f(x),\s.t. \quad g(x)=0.$$ 设 x 为 d 维向量，则约束曲面 $g(x)=0$是 d-1 维曲面，目标函数$f(x)$是 d 维曲面。 因曲面梯度与自身法线共线，所以约束曲面上任意点的梯度$\nabla g(x)$必正交于约束曲面； 又目标函数在约束曲面上的最优点$x^$对应的梯度$\nabla f(x^)$必正交于约束曲面（不正交意味着不相切，不相切意味着至少相交于两个点，故可继续沿反梯度方向$-\nabla f(x) $找到更优解，因为梯度方向是函数增加最快的方向），故最优点$x^$对应的梯度$\nabla f(x^)$ 和 $\nabla g(x)$ 必共线，即必存在$\lambda \neq 0$使得 $$\nabla f(x) + \lambda \nabla g(x) = 0,$$由此得到等价的无约束最优化问题：$$\min_{x, \lambda} f(x)+ \lambda g(x). \quad (\lambda \neq 0)$$ 不等式约束$$\min_x f(x),\s.t. \quad g(x) \le b.$$ 等价问题（称为强对偶问题）对于不等式约束分两种情况讨论： 当最优点 x 位于 g(x)-b&lt;0 区域时，约束形同虚设，可直接求 f(x) 的最小值，这等价于等式约束中将 $\lambda$ 置零； 当最优点 x 位于 g(x)-b=0 区域时，不等式约束便退化为等式约束了。 将分类讨论的问题合为一个式子，如下（记 g(x)-b=h(x) ）：$$\minx \max{\lambda} f(x)+ \lambda h(x), \quad \lambda \ge 0$$称上式为原不等式约束最优化问题的等价问题，等价的原因为： 通过对 $\lambda$ 做大于等于零的限制，便可实现在沿 $\lambda$ 方向求最大值时，将$h(x)&lt;0$的点通过置$\lambda$为零删掉（若$\lambda$可为负数，那么在求最大值时无法删掉这些点，因为在这些点上的最优解$\lambda \ne 0$），然后因为剩下的点对应的$\lambda$均为大于零的数，故基于这个结果在沿$x$方向上求最小值时，是在$h(x)=0$的区域内求得的使 f(x) 最小的最优解，从而完成上述分类讨论的第二种情况； 当然，若没有大于零的$\lambda$，那便是直接在无约束情况下求得的最优解，即分类讨论的第一种情况的解。 拉格朗日函数由此，原不等式约束最优化问题可转化为如下不等式约束的拉格朗日函数的最优化问题：$$\min_x f(x)+ \lambda h(x)\s.t. \quad h(x) \le 0\\quad \quad \lambda \ge 0\\quad \quad \lambda h(x) = 0$$称以上三个约束条件为KKT（Karush-Kuhn-Tucker）条件。 为使用拉格朗日函数，须将等价问题做如下变型： 对于等价问题（$\lambda \ge 0$省略不写，并记$L(x, \lambda)= f(x)+ \lambda h(x)$），因为$$\minx \max{\lambda} L(x, \lambda) \ge \min_x L(x, \lambda)$$所以$$\minx \max{\lambda} L(x, \lambda) \ge \max_{\lambda} \min_x L(x, \lambda)$$称$ L(x, \lambda)$为拉格朗日函数，称$\lambda$为对偶变量，称$\minx L(x, \lambda)$为拉格朗日对偶函数，称等价问题为强对偶问题，称$\max{\lambda} \min_x L(x, \lambda)$为弱对偶问题，称原不等式约束最优化问题为主问题。 求解强对偶问题与弱对偶问题统称为对偶问题，无论主问题凸性如何，对偶问题始终是凸优化问题。一般弱对偶问题不等价于强对偶问题，但当主问题为凸优化问题，且可行域中至少有一点使不等式约束严格成立，那么弱对偶问题便可等价于强对偶问题，这称为强对偶性。 当强对偶性成立后，直接求解弱对偶问题便可得到主问题的解，具体为：通过拉格朗日乘子法可将不等式约束的最优化问题转为满足 KKT 条件的拉格朗日对偶函数，通过将拉格朗日函数对原变量和对偶变量的导数置零，并结合KKT条件便得到弱对偶问题的解，由强对偶性进而得主问题的解。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——前馈神经网络]]></title>
    <url>%2F2018%2F03%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[前馈神经网络 (feed-forward nueral network) 前馈神经网络一般有两种，linear perceptron network 和 RBF network，该文主要叙述前一种，其学习规则是梯度下降法，是一种无约束的最优化算法。 NN示例代码 符号及标记 符号 含义 $L$ 代价函数 / 损失函数 / 最优化目标函数 $w$ “轴突”权重 $\sigma$ 神经元“树突”的激活函数 $z$ 带权输入 $z^l=w^{l-1}a^{l-1}$ ，即从上个神经元轴突传到该神经元树突上的值 $a$ 神经元的激活值 $\delta$ 中间变量，称为某层的误差，专用于反向传播算法 注 1：零层为真实数据，尚未前传，自然没有所谓的误差 $\delta^0$；因第零层的“轴突”上的权重 $w^0$ 尚未学习成功而导致的第一层的神经元的带权输入 $z^1$ 的误差，记为 $\delta^1$ 。 注 2：每层“轴突”上的权重$w$ 的行数为希望学到的模式数，列数为输入数据的维度。 注 3：做某一层的前传的时候，那一层的恒一神经元（用于模式中的偏置）才会被重新激活；当做为输出层时，该层的恒一神经元处于闭塞状态，是看不见的。当做某一层的反传的时候，是用该层神经元的误差$\delta$ 求loss对上一层“轴突”权重的偏导，而该层的恒一神经元没有所谓的误差，上一层的恒一神经元却有“轴突”权重，自然也有偏导，故只有求“轴突”权重偏导的那一层的恒一神经元会被重新激活，其余均处于闭塞状态。 学习法则 若 $y$ 与 $(x_1,\dotsb,x_m)$ 线性相关，且采样没有噪声，则直接采$m$个样本点求解线性方程就能得到参数 $\omega$ 的唯一解。为应对非线性相关的数据，采用迭代最优化（iterative optimization）的方法。 参数更新：梯度下降法 梯度是个向量，指函数变化增加最快的地方，故沿负梯度的方向便能到达函数的极小值处。 迭代优化的参数更新通式为：$$w(t+1) \leftarrow w(t)+\alpha \frac{v(t)}{||v(t)||} \quad , \alpha &gt; 0$$其中参数的确定由降低 loss 函数的方法确定。对 loss 函数进行一阶泰勒展开：$$L(w(t+1)) \approx L(w(t))+ \alpha v(t)^T \nabla L(w(t))$$要使 $L(w(t+1))$ 最小，须使 $ v(t)^T \nabla L(w(t))$ 最小，故令 $v(t)= -\nabla L(w(t))$ 。为使得学习率在陡的地方大，缓的地方小，令 $\alpha \propto ||\nabla L(w(t))||$ ，从而得到梯度下降法的参数更新式：$$w(t+1) \leftarrow w(t)-\alpha_0 \nabla L(w(t))$$其中 $\alpha_0$ 称为 fixed learning rate，而真实的 learning rate $\alpha$ 的大小随梯度的大小变化而变化。 求梯度：反向传播算法$$\delta^l=(w^l\delta^{l+1})*\sigma’_{z^l}\\frac{\partial L}{\partial w^{l-1}}=\frac{1}{n}\delta^{l}(a^{l-1})^T$$ 在用矩阵编程计算梯度时，无需考虑具体矩阵乘积的细节和含义，在得到反向传播的标量表达式后，只需依照两条规则即可写出梯度的矩阵算式： 依据标量表达式确定算式的结构； 依据loss对该层参数偏导的形状调整矩阵的顺序和形状。 优化SGD待续 加快 mini-batch 训练的3种方法动量法$$M(t) = \alpha M(t-1) - \epsilon \frac{\partial L}{\partial w}\w += M(t)$$ 动量法能够减小高曲率方向上的震荡，使得小球尽快地损失掉重力势能。窃以为，公式结合物理原则，应为（尚未测试）：$$v(t) = \alpha M(t-1) - \epsilon (1 + \frac{1}{|\frac{\partial L}{\partial w}|})\w+=v(t)\M(t) = \frac{v(t)}{\alpha}$$ Ilya Sutskever 在2012年提出了一种优化版本：先在历史累计方向上前进一大步，然后在新位置上计算梯度并修正方向。可以这么理解，最好犯错之后去改正它。 自适应学习率每个神经元入度的不同导致了流入不同神经元的“树突”权值的最佳学习率各不相同。当入度很大时，每个“树突”权值改变一点点，累积的改变量就很大了，很容易过量；而当入度很小时反之。所以一般采用一个全局学习率，然后根据对每个神经元各自做适当调整：$$w+=-\epsilon g \frac{\partial L}{\partial w}$$初始化局部 $g=1$ ，如果下次该权值的梯度符号不变则增加 $g+=0.05$ ，否则减小为 $g*=0.95$ 。 注意： 将 $g$ 限制在某个合理的范围，比如[0.1, 10] 或 [.01, 100]。 使用 full-batch 或很大的 mini-batch，毕竟这样保证了梯度的符号不易受 mini-batch 的采样误差影响。 综合自适应学习率和动量更新法，以当前梯度和当前速度的符号来决策 $g$ 的变化。 RMSProp：将梯度除以历史数量级全局学习率之所以难选，主要是因为每个期望的最终的权值的数量级相差巨大。在 full batch 中，可以利用梯度的符号来替代权值的更新量，从而解决这个问题。RProp结合了“只用符号”和“自适应学习率”的思想，但它违反了 SGD 的中心思想（当学习率很小的时候，权值更新量其实是当前mini-batch的梯度和历史梯度的平均。举例来讲，假设某个权值在9个批次中的梯度是+0.1，在第10个批次中的梯度是-0.9，我们希望这个权值大致不变。），故不适用于 mini-batch。而RMSProp便融合了mini-batch的高效性、mini-batch间的有效平均和RProp的稳定性。 总结 对小数据集（10000以内）或者没有多少重复数据的大数据集，应当使用full-batch的一些优化方法，如Conjugate gradient、 LBFGS。然后试试adaptive learning rates, rprop …，它们是为神经网络而设计的方法。 对含有重复数据的大数据集，应当使用mini-batch。尝试动量法SGD，rmsprop 或LeCun的最新研究成果。 常用的激活函数及其对应的 loss 函数 假设激活函数$\sigma(z)=z$，则 $L(\omega , b)=\frac{1}{n}\sum ||y- { \omega{L-1}[\omega{L-2}(…x)] }||_2$，而我们平常所说的loss函数是与网络结构无关的“基础loss函数”。 线性激活函数与均方差 loss 函数$$\sigma(z)=z\L=\frac{1}{2n}\sum_x||a(x)-y(x)||^2$$ $\delta^L=a-y$ （因为 $\sigma_z’=1$ ）； $\delta^l=w^l\delta^{l+1},(l=1,…,L-1)$。 sigmod 激活函数与交叉熵 loss 函数$$\sigma(z)=\frac{1}{1+e^{-z}}\L=-\frac{1}{n}\sum_x[y\ln a+(1-y)\ln (1-a)]$$ $\delta^L=a-y$ （因为采用交叉熵 loss，约掉了 $\sigma’(z)$ ）； $\delta^l=w^l\delta^{l+1}a^l(1-a^l)$ ，其中“ $*$ ”是Hadamard积。 用交叉熵而非均方差的原因是为了解决神经元饱和问题（指某些神经元可能因权重初始化不当或者确实已经学到了很成熟的地步，而处于 $\sigma’(z)​$ 值很小的激活值位置，进而导致在该处的梯度几近为零的现象；而其后果是神经元激活值会维持很长一段时间的近似稳定状态，这在训练终期是代表收敛的好现象，但若是在训练初期（即误差比较大时）， loss 下降会十分缓慢，严重拖慢收敛速度）：使用交叉熵会使得采用 sigmod 激活的神经网络中的梯度表达式中的 $\sigma’(z)​$ 被约掉，从而解决了神经元饱和问题。 可根据激活函数和希望的梯度形式反推得到所需的loss函数，见神经网络与深度学习（Michael Nielsen）3.1.3节。 最小化交叉熵 loss 函数等价于最大化以 sigmod 为参数的对数似然：$$\mathrm{likelyhood} = yi \ln[ \prod{i=1}^N \sigma(z_i)] +(1-yi) \ln[ \prod{i=1}^N (1-\sigma(z_i))]$$ softmax 激活函数与对数似然 loss 函数（right 损失函数）$$\sigma(z_i)=\frac{e^{z_i}}{\sum_ie^{z_i}}\L=-\frac{1}{n}\sum_x\ln a_I,(a_I为真实类别对应神经元的激活值)$$ $\delta^L=a-y$ ； $\delta^l=w^l\delta^{l+1}a^l(1-a^l)$ ，其中“ $*$ ”是Hadamard积。 因loss只求在真实类别神经元上的偏差，而激活函数的分母中所有神经元都包含，故$\delta^L$的求解分为两部分 设 对该学习算法的一些理解 在梯度回传的过程中，$\omega$向量可能会变得非常大，则带权步长的移动只会引起在那个方向上微小的变化，以致很难有效地探索各种$\omega$模式。（大分量相对不怎么移动，小分量却相对移动很大，此处相对是指与分量自身相比，故会卡在某个方向上，以致很难有效地探索。）正则化的效果是让网络倾向于学习小一点的权重，让$\omega$只负责方向，而让$b$负责激活空间的位置。 另一个角度是，更小的权重意味着网络的行为不会因为噪声而改变太大，一个无规范化的网络可以使用大的权重来学习包含训练数据中的噪声的大量信息的复杂模型，而规范化的网络受限于根据训练数据中常见的模式来构造相对简单的模型，能够抵抗训练数据中的噪声的影响。 在迭代过程中，还会出现神经元饱和问题，从梯度公式的角度想，是$\sigma’(z)$或$a$过小导致的梯度过小，从而引起学习缓慢的问题，解决方法便是构造合适的函数将梯度公式中的$\sigma’(z)$约掉；从网络的$loss$函数角度想，是寻找极低点时中途出现了原地踱步（小梯度）的情况，解决方法便是选用不同的激活函数与基础$loss$函数的搭配，从而得到形状更好的网络$loss$函数；从模式的可激活空间角度想，是样本$x$在模式$\omega$的激活空间上的分量值（带权输入）处于激活函数的平缓处（极低变化率）导致的模式$\omega$寻找进度迟缓，解决方法便是消除激活函数变化率对模式$\omega$的迭代寻找的影响。 ​对导数的理解：导数的2-范数越大，说明在该点越不稳定，或说前面输入的波动会对输出造成很大的影响，故亦可称导数为该点的不稳定度或不成熟度。即可将 $\delta​$ 理解为该神经元的不成熟度。 实际应用中遇到的问题最优化问题：更新频率与幅度究竟应该为多大泛化问题：如何防止过拟合？数据的两种噪音]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[numpy概览]]></title>
    <url>%2F2018%2F03%2F08%2Fnumpy%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[术语numpy数据类型 python中的数据类型详解 numpy中自定义的数据类型需用元组表示： 12np.array([[ ( np.array([...]), np.array([...]) ) ]], dtype=[('inputs', 'O'), ('targets', 'O')]) 用[[]]将自定义的数据类型包裹起来的优点是，可用[0,0]的索引方式将object类型的数据解析出来。 ref &gt;&gt; numpy dtype类详解]]></content>
      <categories>
        <category>script</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习——感知机学习算法]]></title>
    <url>%2F2018%2F03%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%84%9F%E7%9F%A5%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[PLA(perceptron learning algorithm) 二分类感知机可简单理解为对输入向量线性组合取符号 PLA示例代码 线性可分问题中感知机的收敛性 符号 含义 $w_{t}$ 第 t 次错误修正后的权重向量 $(x_t,y_t)$ 第 t 次发现的错误样本点 $w_f$ 真实模式 f 对应的最佳权重向量 有所有样本中的最大长度为$R^2=\max_n||x_n||^2$，设$\rho=\min_n {y_n\frac{w_f^T}{||w_f}x_n}$，则$$wf^Tw{t+1}=w_f^T(wt+y{t+1}x_{t+1})=w_f^Twt+y{t+1}wf^Tx{t+1}\geqslant w_f^Tw_t+\rho ||wf||,\||w{t+1}||^2=||wt+y{t+1}x_{t+1}||^2\leqslant ||w_t||^2+R^2.$$假设找到最佳权重向量需修正错误T次，则由以上两组迭代不等式和初始条件$w_0=0$得到$$w_f^Tw_T\geqslant T\rho ||w_f||,\||w_f^T||^2\leqslant TR^2.$$进而有$$(\frac{w_f^T}{||w_f||}\frac{w_T}{||w_T||})^2\geqslant TC \quad 其中C=\frac{\rho^2}{R^2},为一常量.$$故$T\leqslant \frac{R^2}{\rho^2}$，即对于有限样本的线性可分问题，PLA 收敛。 感知机的学习法则 所谓学习法则，就是权值更新的策略，即在迭代过程中如何进行权值更新。 感知器法则：依照知错就改的演算策略寻找最佳的权重向量（学习到的是各个特征的权重），但若样本线性不可分则不能收敛； $\Delta$法则：用最小二乘法、牛顿迭代法或梯度下降法等最优化方法寻找最小的输出误差（无法说清学习到的是什么，只是冲着最优结果去的），但对于误差曲面被拉长的情况进行学习较为困难。 感知机的局限感知机模式识别只学习特征权重，特征探测器需要手写，即特征需人工提取，故较依赖于人工提取特征的好坏，而该能力较依赖工作者特殊领域的专业经验。由此，发展出了第二代神经网络，即加入了隐藏层，从而实现了特征检测模块的训练，即不但学习特征权重还可以学习特征表示。 注：学习流入 hidden units 的 weight 其实就是学习feature。对于如何表达概念目前有三种观点： 特征论：概念由一系列特征表示，便于解释概念之间的相似性、便于将概念表示为向量； 结构主义论：概念不是孤立的，是关系图谱中的一个节点，由与其它概念的关系决定； 神经网络利用特征向量构成关系图谱：许多神经元表达一个概念，一个神经元同时参与很多个概念的表示，这种多对多的表示被称为“distributed representation”。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数学——概率论基础.md]]></title>
    <url>%2F2018%2F03%2F04%2F%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94%E6%A6%82%E7%8E%87%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[基本假设为解决贝特朗悖论出现的问题，作以下三点假设（概率论公理）： 样本空间$\Omega$：一个随机实验的所有可能输出的集合； 事件空间$\mathcal{F}$：$\mathcal{F}$为$\Omega$上的$\sigma$代数，是概率论中的定义域；其元素为事件，事件是$\Omega$中某些样本的集合； 测度$P$：$\mathcal{F}$上的测度$P:\mathcal{F}\to R$，是$\sigma$可加且非负的集合函数。 随机变量数学期望 $\xi$ 的数学期望可理解为在 $\Omega$ 上的加权和（权重为概率值，对应于常规均值中某数出现的频率，有的资料称之为概率平均），采用的计算方法为L-S积分：如设$\xi=\sum_{i=1}^nai \chi{A_i}$ ，即将事件$A_i$ 映射为实数$ai$ ，则$E(\xi)=\int{\Omega} \xi(\omega) \mathrm{d}p(\omega)=\sum{i=1}^n \int{\Omega} ai \chi{A_i} \mathrm{d}p(\omega)$ ，即将$\Omega$ 中不可分事件$\omega$ 按值域划分为n类（每一类中的$p(\omega)$ 都相等，假设$A_1$ 中有10个$\omega$ ，则$p(A1)=10*\mathrm{d}p(\omega)$ ），并在每一类中积分出一个值，最后将这n个值求和。当n为有限值，结果为$\sum{i=1}^n a_i p(Ai)$ ；当$n\to \inf$ ，结果为$\int{-\inf}^{+\inf} a f(a)\mathrm{d}a$ 。 综上，将数学期望作$\xi$ 在$\Omega$ 上的“加权和”看待。 条件数学期望 $E(\xi)$ ： $=E(\xi \chi_{\Omega})$ ：在$\Omega$ 上$\xi$ 的加权和 $=E(\xi | \Omega)$ ：在$\Omega$ 上$\xi$ 的加权平均 $E(\xi \chiA)$ ：在$A$ 上$\xi$ 的加权和（实质为在$\Omega$ 上$\xi \chi{A}$ 的加权平均，为方便思考，如此定义其含义） $E(\xi | A)$ ：$\xi$ 在$A$ 上的加权和，然后在$A$ 上取平均，等于$\frac{E(\xi \chi_A)}{P(A)}$ $P(A)$ ：$\chi_A$ 在$\Omega$ 上的加权和 $P(A | B)$ ：$\chi_A$ 在$B$ 上的加权平均 总结： $P(AB)=E( \chi_A \chi_B)$ ：单纯地在$A$ 、$B$ 交集上的加权和（也可认为在$\Omega$ 上做的归一化） $P(A|B)=E(\chi_A |B)$ ：对在$A$ 、$B$ 交集上的加权和还做了归一化，谁为条件便是在哪个集上做的归一化（A、B独立的情况暂不考虑） 归一化：在哪个集上做归一化便是将全集缩减为了那个集 全数学期望公式 $$\sum{i=1}^n E(\xi \chi{B_i}) = E(\xi \chiB)\\Rightarrow \sum{i=1}^n P(B_i)E(\xi | B_i) = P(B)E(\xi | B)$$ 将条件期望乘上条件的概率，可看做是将这个区域上的均值取消求平均的操作 全概率公式 $$E(\chiA)=\sum{i=1}^n E(\chiA \chi{Bi})\\Rightarrow P(A) = \sum{i=i}^n P(A | B_i) P(B_i)$$ 贝叶斯法则 $$P(Bi | A) = \frac{E(\chi{B_i} \chi_A)}{P(A)} = \frac{P(A|B_i)P(B_i)}{P(A)}$$ $A$ 为出现的新事件，$B_i$ 为欲求其概率的事件，$P(B_i)$ 为先验概率，多由经验知识得到； ​由该法则可得到基于最小错误率的贝叶斯决策规则：对于出现的$A$ ，$P(A|B_i)P(B_i)$ 最大的$B_i$ 为最可能发生的事件。其中，$A$ 为被归一化（该概念见条件数学期望的引注）到的集合，在比较大小时可去掉，不影响大小比较的结果。 机器学习中常用的方法最大似然估计 Maximum_Likelihood_Estimation 数学中似然函数 = $\prod_ip_i$（其中 $p_i$ 为样本$x_i$ 的概率，一般能用一个含待估计的参数和 $x_i$ 表示的解析式表示）表示已出现的样本出现的概率，而已出现的样本通常为最可能出现的，故可通过最大化该似然函数来估计想要的参数。有时会对似然函数取对数以简化运算。 在机器学习中常用最大化以假设函数$h$ 为参数的似然函数的方法使得假设函数$h$ 逼近隐藏模式$f$ 。]]></content>
      <categories>
        <category>数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux环境编程基础]]></title>
    <url>%2F2018%2F03%2F01%2Flinux%E7%8E%AF%E5%A2%83%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[内核待续 文件描述符内核为每个进程维护一个文件打开记录表，文件描述符为该文件在表中的索引值。 文件采用引用计数方式访问文件，当引用计数器为零时，内存管理机制便会对其进行垃圾回收。 对于以下选中的情况，引用计数器会自加1： [x] 创建文件 [ ] 对该文件建立一个硬链接 [x] 对该文件建立一个软链接 [x] 每有一个进程访问该文件 一个文件包括三项，目录项（dentry结构）、索引节点（inode结构）和文件数据。 目录项包括文件名和指向索引节点的指针等信息（当用unlink函数删除该项时，便在该目录中看不见该文件）； 索引节点包括链接数、文件所有者、文件在磁盘的位置等文件属性； 文件数据便是在磁盘上的数据块。 当对该文件建立硬链接（ln src src_ln）时，会额外创建dentry结构和inode结构，但两者的节点号一样（节点号可认为是文件数据的地址）；而当建立符号链接（ln -s src src_ln）时，则是额外创建了一个文本文件，里面包含了源文件的位置信息。两者完全不同，也不能抽象地比较优劣，只能就具体情况来说。 文件系统 实际文件系统（linux为ext文件系统）包括引导块、超级块（记录文件系统的管理信息）、索引节点区（保证了ext文件系统只有一个根节点）和数据区，其中超级块与索引节点区是区分文件系统有无的界限； 虚拟文件系统只存于内存中，它的存在使得操作系统能够兼容足够多的文件系统，具体是将其他文件系统进行封装，并挂载到ext文件系统的目录树下，以使得内核能够以一致的方式访问其他文件系统； 特殊文件系统proc，也只存于内存中，是内核的窗口，用于查看内核运行的实时信息。 库 在编译链接的过程中，链接器搜索静态库（archives）时会链接所有已引用却未处理的符号，而未引用或已处理的符号不会从静态库中链接出来，故链接库的链接位置应放置在命令行尾部，否则会出现符号未处理的情况； 调用动态库的方式类似于对数据文件的读取，故只有在程序执行时才会装入内存，注意编译链接时须有链接选项-ldl； 创建静态库的命令为ar cr libtest.a test1.o test2.o，编译成动态库的命令为g++ -share -fPIC -o libtest.so test1.o test2.o。 因为c不支持函数重载（即与c++函数签名方式不同），所以在c/c++混合编程时，须用extern “c” {}将c代码封装。 僵尸进程在每个进程退出的时候，内核释放该进程所有的资源，包括打开的文件、占用的内存等，但是仍然为其保留一定的信息(包括进程号，退出状态，运行时间等)， 只有父进程通过wait/waitpid来取时才释放，否则其进程号就会一定被占用，这样就导致了僵尸进程的问题。]]></content>
      <categories>
        <category>cs</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数学——线性代数基础]]></title>
    <url>%2F2018%2F02%2F28%2F%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[对矩阵乘积的各种viewpoint 列向量的inner product $ x^Ty $ ：n个元素之积的和 列向量的outer product $xy^T$： 对两个列向量各元素之积的陈列【主】； 对列向量$x$分别进行n次倍乘（y 存储放大倍数）的并行运算，并堆叠成一个矩阵。 矩阵向量之积$Ax$： 1个列向量分别与n个列向量的inner product的并行运算（将$A$看做行向量的堆叠）【主】； 对n个列向量的线性组合，即对n个列向量各赋予不同权重后的组合（将$A$看做列向量的堆叠）。 矩阵矩阵之积$AB$： n组列向量的outer product的和，即n个矩阵（每个矩阵为同一索引下的向量的outer producter，即 A 取第几列，B 便取第几行）的和（$A$ by columns, $B$ by rows）【主】； 应用：反向传播、方差 n个列向量分别进行矩阵向量之积$Ab$的并行运算； 对两个矩阵各列向量的inner product的陈列【多用于结合标量的性质进行证明矩阵性质的证明】。 矩阵的一些性质迹 满足交换性和轮换性：$\mathrm{tr}AB=\mathrm{tr}BA$，$\mathrm{tr}ABC=\mathrm{tr}CAB=\mathrm{tr}BCA$ 范数 把线性空间的一个元素（向量、矩阵、……）与一个非负实数相联系，在许多场合下，这个非负实数可以作为向量或者矩阵大小的一种度量，这个非负实数便称为范数（当说长度时，特指的是内积空间中向量的2-范数）。元素与实数的映射关系需满足4点才能称为范数：非负性、定性、齐次性和三角不等式。 范数分类：p-范数、加权范数（$||x||_A=\sqrt{x^TAx}$）、F-范数（$\sqrt{\mathrm{tr}(A^TA)}$）、…… 矩阵的列空间与零空间 $L(x_1,x_2)$意为向量$x_1$与$x_2$张成的空间； $R(A)$ 意为矩阵$A$的列向量张成的空间，称为$A$的列空间或$A$的值域； $R(A)^{\bot}$意为 $A$的列空间的正交补空间； $N(A)$意为$A$的零空间或$A$的核空间。 $A$的零空间是$A^T$的列空间的正交补空间，而A^T的列空间维度与A的列空间维度在数量上相等，故$\mathrm{dim}N(A)=\mathrm{dim}R(A^T)^{\bot}=\mathrm{dim}R(A)^{\bot}=n-rank(A)$，为简化书写与推导，常一步写为$\mathrm{dim}N(A)=n-rank(A)$。 行列式 一种几何上的解释：$\mathrm{abs}|A|$ 意为矩阵$A$的行向量限制性地张成的空间的体积。（限制性地张成，是指用于线性组合的系数在(0,1)间取值。） $|AB|=|A||B|$ $|A^{-1}|=\frac{1}{|A|}$ 如果$A$奇异，那么$A$的行向量线性相关，限制性地张成的空间是n维空间中的“flat sheet”，故此时$|A|=0$。 二次型与矩阵的定性 n个变量的二次多项式（每一项的次数都为2的多项式）称为二次型。矩阵形式的二次型为一标量$x^TAx$，其中$A$称为二次型矩阵，虽然无论$A$是否对称，$x^TAx$总是一个二次型，但A的表现形式（是否对称）并不影响这个标量的结果，故若无特殊说明，默认二次型矩阵为一对称阵。 根据二次型的正负，将矩阵分为正定阵（PD）、半正定阵（PSD）、负定阵（ND）、半负定阵（NSD）和不定阵。 度量阵$A^TA$必为半正定，若$A$的形状为“$\mathrm{I}$“，且列满秩，则该矩阵必正定。 半正定阵加正定阵必为正定阵。 矩阵的特征向量与特征值 默认特征向量特指单位化的特征向量（虽然因为有正负还是不唯一，但已足够）。 $\mathrm{tr}A=\sum_{i=1}^n\lambda_i$ $|A|=\prod_{i=1}^n\lambda_i$ 对称阵的特征向量与特征值 对称阵的特征值均为实数，特征向量两两正交。 若对称阵的特征值均为正数则必正定（由对称阵必能正交对角化推得），依此类推。 针对最优化问题$$\mathrm{min}x^TAx \quad s.t. ||x||_2^2=1$$结果为$A$的最小特征值，$x$为其对应的特征向量。因为$A$为对称阵，故能正交对角化$A=U\Lambda U^T$，原问题便转化为了：将原默认基换为$U$的列向量构成的基，于是$x$便成为了$y$，满足$y=U^{-1}x$ （其过程为$x$向$U$的各个列向量方向投影，并依列向量次序组织成为$y$），因为$U$为正交阵，故$||y||_2^2=1$依然成立，与此同时，优化目标变为$\mathrm{min}y^T\Lambda y$，可看做（问题本质并非这样，但脱离问题只就该式子的数学形式来看可如此看待；这种办法只可用于求解不能用于理解）求单位向量$y$的方向使其在度量矩阵$\Lambda$下长度最小，故结果为$A$的最小特征值，而$x=Uy$ ，即只取最小特征值对应的特征向量。 对此常用结果可记住：要使一单位向量在某一度量阵下的内积最小，须使该单位向量等于该度量阵最小特征值对应的特征向量，此时内积便为该度量阵的最小特征值。 线性变换 在线性空间（对加法和数乘封闭的集合）中，借助于基的概念，线性空间中的元素（矩阵、多项式、函数等）的运算能够转化为向量的运算，线性变换（满足$T(kx_1 + lx_2) = kT(x_1) + lT(x_2)$的$T()$称为线性变换）的运算能够化为方阵的运算，从而一般线性空间中的问题都能够转化为列向量空间中的问题。故可认为线性代数主要研究列向量空间。 基 A 下的向量 Ax，称 x 为坐标向量，Ax 为向量。在计算时默认用坐标向量，以做线性变换 P 为例进行说明： 换基之前：坐标向量 x；线性变换后的坐标向量为 Px； 换基之后：坐标向量 $y=C_{A \rightarrow B}^{-1}x$；线性变换后的坐标向量为 $C^{-1}PCy$； 其中，$C=A^{-1}B$称为过渡矩阵，亦可看做新基在旧基下的坐标； 通法： 换基操作为对向量左乘新基的逆，或者对坐标向量左乘过渡矩阵的逆，从而得到该向量在新基下的坐标，由此换基操作可看做由坐标向量获得完整向量表示（即坐标向量左乘当前基）的逆操作； 线性变换操作为对坐标向量左乘表示线性变换的矩阵，因同一线性变换在不同基下的矩阵表示不同，故只能作用于坐标向量上，或说作用于坐标向量上便是作用于完整的向量上：$T(Ax) = APx = A(Px)$； 通过对在换基$A \rightarrow B$前后的同一向量$Ax$做相同的线性变换$P$可以看出： 两矩阵 $P, Q$ 相似是指这两个矩阵是在不同基 $A, B$ 下表示的同一线性变换$T()$，即 $T(AC)=APC=B(B^{-1}A)PC=B(C^{-1}(PC))=BQ=BQE=T(BE)$ ，由此看出一旧基下的矩阵在新基下的相似矩阵为，旧基下带线性变换属性的新基映射到新基下的坐标，若未带线性变换属性，则映射出的坐标为 $E$ 。由此可持这种观点：线性变换是向量的属性，当向量处在不同的基下时表现为不同的矩阵形式。 对表示线性变换的矩阵$P$ 左乘过渡阵的逆右乘过渡阵便可得到该矩阵在新基下的相似矩阵。 若要求相似阵为对角阵，那么： 当原矩阵为实对称阵时，存在为正交阵的过渡阵；又若原矩阵的基为 $E$ ，那么该过渡阵为原矩阵的一个完备的标准正交特征向量系。 当原矩阵非对称时，只能通过正交对角分解得到 $U^TPV = diag(\sigma_1, …)$ ，其中 V 是 $P^TP$ 的一个完备的标准正交特征向量系（前提为$P$ 非奇异），$U=PV diag(\sigma_1, …)^T$ ；而若当 P 为奇异阵时，可进一步拓展为通过奇异值分解得到 $U^TPV = diag(\sigma_1, …)$ ，各变量含义相同。 度量阵 线性空间中已有加减，而数乘算不上线性空间内的积，故引入内积的概念，其作用是度量夹角、长度，直觉上可以将 (x, y) 看做 x 在 y 上的投影与 y 长度的乘积。 任何内积空间均应满足不等式$|(x,y) \le |x||y|$ 。 因为向量的长度自然是不应变的，故内积的定义不应依赖于基，由此在定义内积时应用向量而非坐标向量。 比如在线性空间中定义 $(x,y)=x^TGy=\sum_i \sumj g{i,j} x_i y_j$ ，其中 G 可以看作是在不同维度上的度量尺，至于具体工作细节可从该式看出：取基 Y，满足$(Y^{-1})^T(Y^{-1})=G$ ，则 $(x, y)= x^T (Y^{-1})^T(Y^{-1})y = (Y^{-1}x)^T(Y^{-1}y)$ ，从而实现度量尺的功能。因为计算时采用坐标向量的习惯，使得度量尺在不同基下的矩阵表示不同，称之为度量阵。 同一度量尺在不同基下的表示称为合同。 投影阵 若 $L \oplus M=R^n$ ，则将 $x \epsilon R^n$ 沿着 M 到 L 的投影变换在$R^n$ 的基下的矩阵称为投影矩阵，记为$P{L,M}$ ，且$P{L,M} = (X,0)(X,Y)^{-1}$ 。 投影的本质：所谓投影便是依投影面 L 重新建立坐标系，并只保留投影面上的坐标，其余坐标置零。故私以为更准确的词应该叫截取，截取矩阵。 当 M 为 L 的正交补空间时，即$X^T Y = 0$ ，称其为正交投影：$$P_L=(X,0)(X,Y)^{-1} = X(X^T X)^{-1}X^T = XX^+ \quad (前提：X列满秩)$$在统计学中又称$P_L$ 为 hat matrix，记为 H： H：求列向量在 X 列空间上的投影 I=H：求列向量对 X 列空间的余 正交阵，对称阵与投影阵矩阵微积分附：一些常见的矩阵相关的思考方向 设$X=(x_1,x_1,…,x_N)$，则 $$|| \sum_{i=1}^N x_i || 2^2 = \sum{i=1}^N \sum_{j=1}^N x_i^Tx_j^T = (X^TX).sum()$$]]></content>
      <categories>
        <category>数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[makefile基础]]></title>
    <url>%2F2018%2F02%2F20%2Fmakefile%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[文件基本格式12target:prerequisites[tab] commands 若prerequisites中有一个文件比target文件新，则执行commands所定义的命令。若显式指明target为伪目标，即.PHONY:target，则make命令将跳过文件检查，直接执行对应的命令，由此避免了因当前目录下有target文件而不会执行命令的问题。 若make未指定目标则缺省执行第一个目标。 命令按行解析，每行命令都在单独的进程中执行。 在[tab]后commands前添加@，可关闭回显，常用于echo命令。 include filename命令可将其他文件包含进来，若在该命令行首添加-，表示忽略可能会出现的文件包含错误。 变量 变量在使用时展开，形式上类似宏替换，如此引用$(var)，而引用shell变量时用$$。 变量定义的四种格式： var=value在执行时才扩展 var:=value在定义时便扩展，直接使用右侧的现值 var?=value若变量为空则设置该值，否则维持原值 var+=value将值追加到变量尾部，继承上次的操作符，若未定义过则自动解释为=特殊变量 内置变量$(CC)当前使用的编译器 $(MAKE)当前使用的make工具 自动变量 $@当前目标 $^所有先决条件，$?比目标更新的所有先决条件。 $&lt;第一个先决条件 多行变量123define var commandsendef 主要用于定义命令包，每行命令都在单独的进程中执行，故展开时有可能导致脚本错误。 静态模式：以%通配12345objs=main.o library.oall:$(objs) $(cc) -o a $(objs)$(objs):%.o:%.c $(CC) -c $&lt; -o $@ 条件判断基本格式12345conditional-directive commandselse commandsendif 可用的条件判断 ifeq(var1,var2) 两参数是否相等 ifneq(var1,var2) 两参数是否不等 ifdef var 变量是否已定义 ifndef var 变量是否未定义 循环可使用shell循环123456objs = file1 file2rulefor: for filename in `echo $(objs)`; \ do \ touch $$filename; \ done \保证了多行命令在同一进程下执行，因命令是在shell下执行的，故filename为shell变量，自然用$$引用。 字符串替换函数示例123456789comma := ,empty := space := $(empty) $(empty)str1 := a b cstr2 := $(subst $(space),$(comma),$(str))result: @echo $(str1) @echo $(str2) @echo "done"]]></content>
      <categories>
        <category>cpp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mc文件管理器命令备忘]]></title>
    <url>%2F2018%2F02%2F20%2Fmc%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E5%99%A8%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98%2F</url>
    <content type="text"><![CDATA[文件信息查看与修改1234C-x i 显示文件详细信息，重复指令关闭窗口（类似stat命令）C-u 左右面板互换（用于改变列表显示模式）C-x c 修改文件权限信息（space为反选键）[alt]-t 切换列表显示模式 快捷访问：1234C-x h 添加文件至收藏夹C-\ 显示文件收藏夹列表[tab] 切换活动面板C-s 在当前目录下查找文件 文件移动与管理1234+ 可用正则匹配符选中多个文件- 取消选中[insert] 多选模式下反选当前高亮文件F-5,6,8 对当前高亮文件或选中的多个文件进行复制、更名或移动、删除操作，目标为另一个面板所在路径 命令行123C-w 删除光标前所有字符C-b C-f C-a C-e 移动光标（方向键在此不起作用）[esc] [tab] 命令补全（重复两次会显示补全列表） 辅助指令12[esc] 1 帮助手册C-o 与父shell进行切换（切换后父shell工作目录为活动面板工作目录，用于更多的shell指令） 主题倾向1yadt256]]></content>
      <categories>
        <category>工具的使用</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LaTex简易教程]]></title>
    <url>%2F2017%2F12%2F15%2FLaTex%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[第一章文章结构控制 建立整体框架：1234567891011\documentclass&#123;article&#125; \begin&#123;document&#125;\section&#123;第一节&#125;\label&#123;sec:sec_1&#125;\section&#123;第二节&#125;\label&#123;sec:sec_2&#125; \subsection&#123;第二节第一子节&#125; \label&#123;sec:sec_2_1&#125; something...\end&#123;document&#125; 公式控制 段落内公式： 1something...$X$ somthing... something…$X$ somthing… 独立于段落外的公式： 123\[\Delta f\approx (\nabla f, \nabla X)\] $$ \Delta f\approx (\nabla f, \nabla X) $$ 公式对齐控制： 1234\begin&#123;align*&#125;\frac&#123;\partial&#123;\mathrm&#123;f&#125;&#125;&#125;&#123;\partial&#123;x&#125;&#125; &amp;= \frac&#123;\partial&#123;f&#125;&#125;&#123;\partial&#123;x^T&#125;&#125;, \\\nabla_x\mathrm&#123;f&#125; &amp;= \nabla_&#123;x^T&#125;f.\end&#123;align*&#125; $$ \frac{\partial{\mathrm{f}}}{\partial{x}} = \frac{\partial{f}}{\partial{x^T}}, $$ 列表控制：1234\begin&#123;enumerate&#125; \item 第一项 \item 第二项\end&#123;enumerate&#125; 或者 1234\begin&#123;itemize&#125; \item 第一项 \item 第二项\end&#123;itemize&#125; 后记：实用工具LaTex编译环境使用sharelatex即可。当使用中文时，建议用XeLaTex进行build。 LaTex特殊符号的查询可使用Detexify工具得到特殊符号的LaTex语法。 公式编辑的实时显示可使用Markdown-MathJax-editor-online工具检查公式语法的正确性。 ref: LaTex 完整教程]]></content>
      <categories>
        <category>script</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[shell中括号使用总结]]></title>
    <url>%2F2017%2F08%2F14%2Fshell%E4%B8%AD%E6%8B%AC%E5%8F%B7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[${var}限定变量名称范围 1234#!/bin/sh# test.shvar=testecho $&#123;var&#125;12 效果如下 12$ ./test.sh$ var12 $(cmd)命令替换 123#!/bin/sh# test.shecho $(ls) 运行效果与 ls 相同 {}或()一串命令的执行 1234$ &#123; echo 1;echo "A";&#125; &gt; tmp$ cat ./tmp$ 1$ A ${var:-string},${var:+string},${var:=string},${var:?string}几种特殊的替换结构 ${var:-string}若var为空，则结果为&quot;string&quot;，否则结果为$var。 ${var:=string}若var为空，则结果为&quot;string&quot;，并将该字符串赋给var，否则结果为$var。 ${var:+string}若var不为空，则结果为&quot;string&quot;，否则结果为$var。 ${var:?string}若var不为空，则结果为$var，否则将&quot;string&quot;输出到标准错误中，并从脚本退出。 $((exp))POSIX标准的扩展计算，只要符合C的运算符都可用在$((exp))，甚至是三目运算符。 ${var%pattern},${var%%pattern},${var#pattern},${var##pattern}四种模式匹配替换结构 []或[[]]类似test ()在子shell中运行 {}{1..30} 就是1-30，或者是/{,s}bin/表示/bin/和/sbin/，ab{c,d,e}表示abc、abd、abe reference]]></content>
      <categories>
        <category>script</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[简要剖析linux操作系统下执行一个程序的原理]]></title>
    <url>%2F2017%2F05%2F21%2F%E7%AE%80%E8%A6%81%E5%89%96%E6%9E%90linux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B8%8B%E5%AE%9E%E4%BE%8B%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F%E7%9A%84%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[预备知识 程序只是一段代码，独立的程序没有任何实际意义，只有将它置于一定的环境（上下文）内才有其意义。比如说，一个用C#写成的一段hello world代码在windows环境下编译生成的一个程序文件，将它放在ARM机里，在它实例化之前（即执行前），它与ARM机里其他可以执行的程序是一样的，只是一个占据着一定内存空间的二进制文件；一旦将这些二进制文件执行，实例化后产生的东西（称为进程）便具有了实际意义，即这个进程所能实现的功能，当然前提是它能在该环境中实例化，前面所提的C#程序文件便不能在ARM机中实例化。 综上，程序可认为是进程的抽象，一个程序可以实例化出任意多个进程。 linux系统中所有的进程都是在0号进程的基础上fork生成的，故在开机后，运行的任何一个程序都是在某个程序的基础上fork生成后继而执行的。 因此，若要分析执行一个程序的原理，可用任意一个具体的例子进行说明，其中关键性部分几乎不会存在差异。 例子以在shell中执行ls命令为例，进行说明。 预备知识1——简易“子栈”建立“子栈”的概念。从一个函数A跳转到另一个函数B时会创建一个临时堆栈，称函数A所在的堆栈为父栈，称函数B所在的堆栈为子栈。在一个子栈中运行的基本框架如下所示： 12345pushl %ebp /*enter*/ //保存父栈环境movl %esp %ebp //新建子栈环境...movl %ebp %esp /*leave*/ //恢复父栈环境popl %ebp //恢复父栈环境 note: push和pop都是依sp寻址的。 为深入理解该框架，要明确以下几点：​ 首先要明确一个CPU在某一确定的时刻只能运行在一个堆栈空间中，当建立起子栈后，CPU便已运行在了子栈中，当前起作用的堆栈空间便是子栈，而当CPU离开子栈后，起作用的堆栈空间便成了父栈，且方才用过的子栈不复存在，即永远也不可能再次回到方才起作用的子栈，但能再次创建一个新的子栈。这也是将子栈称为临时堆栈、子函数的变量称为临时变量的原因。从根本上来看，除最初开机时建立的堆栈外，所有的堆栈都是临时堆栈，而子父关系便是相对而言。 其次由该框架可发现简易子栈与父栈在地址空间上是连续的,而若将子栈动态信息另存于系统中以实现再次寻回时（见正文），其框架只是加入了对子栈动态信息的保存和重载（因为子栈与父栈在地址空间上不再连续。而无法连续的原因是从子栈返回父栈后，子栈的数据极有可能被覆盖。所以连续的前提是确保子栈不会被第二次使用）。 最后要明确正文中的堆栈之间实际上不是父子关系，而是平等的。依然采用子栈、父栈的名称是为了结合该简易子栈的框架方便理解。依据时间顺序，称后创建的为子栈。 预备知识2——GCC内联汇编基本格式：1234567asm volatile( 汇编语句模板: 输出部分: 输入部分: 破坏描述部分);//即格式为asm("statements":output_regs:input_regs:clobbered_regs); 常用寄存器加载代码说明:|代码|说明||-|:-||a|使用寄存器eax||b|使用寄存器ebx||c|使用寄存器ecx||d|使用寄存器edx||m|使用内存地址||o|使用内存地址并可以加偏移值||r|使用任意动态分配的寄存器||+|表示操作数可读可写||=|输出操作数，输出值将替换前值| 预备知识3——ELF文件待续 正文若将之前所述的子栈的信息保存在系统中，便可实现在运行于某子栈环境中的进程退出前，总是能够再次找到该子栈。可采用类似于如下的结构实现子栈信息的保存：1234struct Thread &#123; unsigned long ip; unsigned long sp;&#125;; 由此，对于进程，可用如下结构进行描述：123456789struct PCB&#123; int pid; unsigned long task_entry; //进程入口点 char stack[STACK_SIZE]; volatile long state; struct Thread thread; struct PCB *next;&#125;; 运行一个进程基本需要以下第三步： 创建0号进程： 123456789asm volatile( "movl %1,%%esp\n\t" "pushl %1\n\t" //将0号进程栈底指针压栈 "movl %1,%%ebp\n\t" "pushl %0\n\t" "ret\n\t" //启动0号进程 : :"c"(task[0].thread.ip),"d"(task[0].thread.sp)); 调度到一个尚未创建栈环境的进程x（即，创建子栈）： 1234567891011asm volatile( "pushl %%ebp\n\t" /*保存父栈环境*/ "movl %%esp,%0\n\t" "movl $1f,%1\n\t" "movl %2,%%ebp\n\t" /*新建子栈环境*/ "movl %2,%%esp\n\t" "pushl %3\n\t" /*启动x号进程*/ "ret\n\t" :"=m"(task[NOW]-&gt;thread.sp),"=m"(task[NOW]-&gt;thread.ip) :"m"(task[x]-&gt;thread.sp),"m"(task[x]-&gt;thread.ip)); 调度到已经存在栈环境（即，至少运行过一次）的进程x（即，切换栈环境）： 123456789101112asm volatile( "pushl %%ebp\n\t" /*保存父栈环境*/ "movl %%esp,%0\n\t" "movl $1f,%1\n\t" "movl %2,%%esp\n\t" /*切换栈环境*/ "pushl %3\n\t" /*切换至x号进程*/ "ret\n\t" "1:\t" //下条指令的地址即为$1f对应的地址 "popl %%ebp\n\t" //此处pop出的是上次x进程被切换出去时push进去的ebp :"=m"(task[NOW]-&gt;thread.sp),"=m"(task[NOW]-&gt;thread.ip) :"m"(task[x]-&gt;thread.sp),"m"(task[x]-&gt;thread.ip)); 结合以上三点，便可总结调度器切换进程的简要步骤（省略进程的优先级策略等）： 保存栈环境：进程断点压栈（编译器自动生成该指令），进程栈底指针压栈，保存进程栈顶指针，并保存恢复栈环境指令的首地址$1f。 转移栈环境：给栈顶指针赋值，并弹出新进程栈底指针和断点。 假设可执行文件是静态链接的（不考虑共享库），实例一个程序只需将程序的文本段、数据段、bss段和堆栈段映射到进程线性区，然后结合上述的第二点，调度到这个尚未创建栈环境的进程，之后只运行上述的第三点即可，从而便实现了对一个程序的实例化。 附1：fork()返回两次的本质待续 refernece0: C语言ASM汇编内嵌语法zz reference1: linux内核分析 附2：进程与线程的区别 区别： 同一进程中的线程间共享数据段、代码段和虚拟地址空间（页表），而进程间不共享，对于栈环境和管理运行期信息的数据结构，进程和线程均不共享 优劣对比： 对于频繁创建和销毁的任务，线程更有优势： 因数据段、代码段和虚拟地址空间的共享，线程消耗内存更少、消耗时间更短 对于需要频繁切换的任务，线程更有优势： 线程间共享虚拟地址空间，切换线程不需更新内核的页表缓存 对于多任务并发的情况，没有定论： 视运行环境而定，一般情况下，因多线程可用栈空间被限制在了进程的栈空间中，对于巨额工作量任务的情况多进程更快，而小额工作量任务的情况反之 总： 线程多用于开发细粒度并行性，进程用于开发粗粒度并行性 线程间易共享数据，进程间须使用进程间通讯机制]]></content>
      <categories>
        <category>cs</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[vim-使用教程]]></title>
    <url>%2F2017%2F05%2F05%2Fvim-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[vim 插件配置安装vim123456$ git clone git@github.com:vim/vim.git$ cd vim/$ sudo apt install python-dev python3-dev ruby-dev libtolua-dev libx11-dev libghc-gtk-dev libghc-gtk3-dev python-gtk2-dev libghc-ncurses-dev$ ./configure --with-features=huge --enable-pythoninterp --enable-rubyinterp --enable-luainterp --enable-perlinterp --with-python-config-dir=/usr/lib/python2.7/config-x86_64-linux-gnu/ --enable-gui=gtk2 --enable-cscope --prefix=/usr$ make$ make install more infoUbuntu Packages Search 安装 vundle1$ git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim .vimrc 文件配置参考 .vimrc 安装插件进入vim执行：123:PluginInstall:PluginClean #卸载前先删掉.vimrc文件中的配置语句:PluginUpdate 安装YCM插件1234$ sudo apt-get install llvm-dev clang$ sudo apt install cmake$ cd ~/.vim/bundle/YouCompleteMe$ ./install.py --clang-completer --system-libclang more infouse vim as IDE gvim 的使用工程环境配置修改~/.indexer_files 用于产生标签 123--------------- ~/.indexer_files --------------- [project] /home/redmud/practice/project/src/ 根据你工程情况只需调整 .ycm_extra_conf.py 的 flags 部分 参考.ycm_extra_conf.py 快捷键的使用1234567891011121314151617181920212223242526;ilt #打开标签页;hw #跳转左侧窗口p #右侧窗口显示标签对应函数/word #在文件内查找关键字;sp #在工程内查找关键字&lt;CR&gt; #进入关键字对应文件p #暂时在右侧显示对应文件q #退出u #撤销上步操作 ctl+r #撤销上步的撤销;ud #显示撤销树;;fx #跳至x字符;cc #注释;cu #取消注释;rw #在文件内不确认得全文替换单词mx #设定/取消该行为标签xm, #自动设定一个可用书签名mn #跳转至下个书签mda #删除当前文件所有书签ctl+o #跳回上次鼠标位置]]></content>
      <categories>
        <category>工具的使用</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用hexo+gitpage搭建个人博客（ubuntu16.04环境）]]></title>
    <url>%2F2017%2F05%2F05%2F%E4%BD%BF%E7%94%A8hexo%2Bgitpage%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2(ubuntu16.04%E7%8E%AF%E5%A2%83)%2F</url>
    <content type="text"><![CDATA[博客搭建安装npm nodejs git hexo工具1234$ sudo apt-get install npm$ sudo apt-get install nodejs$ sudo apt-get install git$ npm install -g hexo 创建博客文件夹123$ hexo init BLOG #假设为BLOG$ cd BLOG$ npm install #安装依赖 主题更改123456$ hexo clean$ git clone https://github.com/iissnan/hexo-theme-next themes/next$ cd themes/next$ git pull$ hexo generate$ hexo server 现在打开http://localhost:4000/ ，就发现新的主题安装成功了。按ctl+c退出 创建git仓库[图] 配置1234$ cd BLOG$ git config --global user.name "username" # username为你自己的github用户名$ git config --global user.email "email@example.com"$ ssh-keygen -t rsa -C "email@example.com" 为github仓库添加SSH keys（id_rsa.pub）12345$ git init$ git add README.md$ git commit -m "first commit"$ git remote add origin git@github.com:"username"/"username".github.io.git #关联远程仓库$ git push -u origin master More info: git command 修改BLOG目录下的_config.yml文件1234deploy: type: github repo: git@github.com:"username"/"username".github.io.git branch: master 安装一个Hexo的插件1$ npm install hexo-deployer-get --save 修改BLOG目录下的_config.yml文件1234deploy: type: git repo: git@github.com:"username"/"username".github.io.git branch: master hexo部署12$ hexo generate$ hexo deploy 浏览 打开网页https://“username”.github.io 后期使用创建新的post,并生效1234$ hexo new "MyNewPost"$ gedit source/_posts/MyNewPost.md$ hexo generate$ hexo deploy 全局配置 _config.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# Hexo Configuration## Docs: http//hexo.io/docs/configuration.html## Source: https//github.com/hexojs/hexo/# Site #站点信息title: #标题subtitle: #副标题description: #站点描述，给搜索引擎看的author: #作者email: #电子邮箱language: zh-CN #语言# URL #链接格式url: #网址root: / #根目录permalink: :year/:month/:day/:title/ #文章的链接格式tag_dir: tags #标签目录archive_dir: archives #存档目录category_dir: categories #分类目录code_dir: downloads/codepermalink_defaults:# Directory #目录source_dir: source #源文件目录public_dir: public #生成的网页文件目录# Writing #写作new_post_name: :title.md #新文章标题default_layout: post #默认的模板，包括 post、page、photo、draft（文章、页面、照片、草稿）titlecase: false #标题转换成大写external_link: true #在新选项卡中打开连接filename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsehighlight: #语法高亮 enable: true #是否启用 line_number: true #显示行号 tab_replace:# Category &amp; Tag #分类和标签default_category: uncategorized #默认分类category_map:tag_map:# Archives2: 开启分页1: 禁用分页0: 全部禁用archive: 2category: 2tag: 2# Server #本地服务器port: 4000 #端口号server_ip: localhost #IP 地址logger: falselogger_format: dev# Date / Time format #日期时间格式date_format: YYYY-MM-DD #参考http//momentjs.com/docs/#/displaying/format/time_format: H:mm:ss# Pagination #分页per_page: 10 #每页文章数，设置成 0 禁用分页pagination_dir: page# Disqus #Disqus评论，替换为多说disqus_shortname:# Extensions #拓展插件theme: landscape-plus #主题exclude_generator:plugins: #插件，例如生成 RSS 和站点地图的- hexo-generator-feed- hexo-generator-sitemap# Deployment #部署，将 lmintlcx 改成用户名deploy: type: git repo: 刚刚github创库地址.git branch: master More info: reference2]]></content>
      <categories>
        <category>工具的使用</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[git 使用教程]]></title>
    <url>%2F2017%2F05%2F04%2Fgit-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[使用ssh背景 用作认证的私钥最好通过passphrase进行加密，否则会有很大安全隐患，只要私钥泄露，别人就能访问你能访问的所有远程机器，不过每次登陆都得输入passphrase。 ssh-agent启动后，可通过ssh-add将私钥加入agent，使用ssh-agent后，只需在将key纳入agent管理时输入passphrase，之后的ssh相关操作就不必输入passphrase了。如果ssh-agent中有多个私钥, 会依次尝试，直到认证通过或遍历所有私钥。 more infoSSH Agent Forwarding原理An Illustrated Guide to SSH Agent Forwarding 指令12345$ ssh-kengen -t rsa -C "some words(email usually)"$ ssh-agent -s$ ssh-add ~/.ssh/id-rsa #添加私钥$ ssh-add ~/.ssh/id-rsa2$ vim ~/.ssh/config config 文件类似如下123456789Host redmudbupt.github.com HostName github.com IdentityFile /home/redmud/.ssh/id_rsa User git host bkseastone.github.com HostName github.com IdentityFile /home/redmud/.ssh/id_rsa2 User git 应用123$ git remote add origin git@bkseastone.github.com:bkseastone/lab_test.git #添加远程库$ git config user.name "one_name" $ git config user.email "one_email" #给仓库设置局部用户名和邮箱 git 使用 more infogit command]]></content>
      <categories>
        <category>工具的使用</category>
      </categories>
  </entry>
</search>
