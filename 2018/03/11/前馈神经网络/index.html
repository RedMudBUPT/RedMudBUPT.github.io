<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="RedMudBUPT" type="application/atom+xml" />






<meta name="description" content="前馈神经网络 (feed-forward nueral network) 前馈神经网络一般有两种，linear perceptron network 和 RBF network，该文主要叙述前一种，其学习规则是梯度下降法，是一种无约束的最优化算法。 NN示例代码   符号及标记   符号 含义     $L$ 代价函数 / 损失函数 / 最优化目标函数   $w$ “轴突”权重   $\sigma">
<meta property="og:type" content="article">
<meta property="og:title" content="前馈神经网络">
<meta property="og:url" content="http://yoursite.com/2018/03/11/前馈神经网络/index.html">
<meta property="og:site_name" content="RedMudBUPT">
<meta property="og:description" content="前馈神经网络 (feed-forward nueral network) 前馈神经网络一般有两种，linear perceptron network 和 RBF network，该文主要叙述前一种，其学习规则是梯度下降法，是一种无约束的最优化算法。 NN示例代码   符号及标记   符号 含义     $L$ 代价函数 / 损失函数 / 最优化目标函数   $w$ “轴突”权重   $\sigma">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/nn_net1.jpg">
<meta property="og:updated_time" content="2018-03-18T12:14:44.778Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="前馈神经网络">
<meta name="twitter:description" content="前馈神经网络 (feed-forward nueral network) 前馈神经网络一般有两种，linear perceptron network 和 RBF network，该文主要叙述前一种，其学习规则是梯度下降法，是一种无约束的最优化算法。 NN示例代码   符号及标记   符号 含义     $L$ 代价函数 / 损失函数 / 最优化目标函数   $w$ “轴突”权重   $\sigma">
<meta name="twitter:image" content="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/nn_net1.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/03/11/前馈神经网络/"/>





  <title>前馈神经网络 | RedMudBUPT</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">RedMudBUPT</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">RedMud's blog called RedMudBUPT</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/11/前馈神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="redmud">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/photo.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RedMudBUPT">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">前馈神经网络</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-11T18:16:33+08:00">
                2018-03-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/神经网络/" itemprop="url" rel="index">
                    <span itemprop="name">神经网络</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
            <!--noindex-->
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/03/11/前馈神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count hc-comment-count" data-xid="2018/03/11/前馈神经网络/" itemprop="commentsCount"></span>
                </a>
              </span>
              <!--/noindex-->
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="前馈神经网络-feed-forward-nueral-network"><a href="#前馈神经网络-feed-forward-nueral-network" class="headerlink" title="前馈神经网络 (feed-forward nueral network)"></a>前馈神经网络 (feed-forward nueral network)</h1><blockquote>
<p>前馈神经网络一般有两种，linear perceptron network 和 RBF network，该文主要叙述前一种，其学习规则是梯度下降法，是一种无约束的最优化算法。</p>
<p><a href="https://github.com/bkseastone/Neural-Networks-for-Machine-Learning/blob/master/feedforwardNN/feedforwardNN.py" target="_blank" rel="noopener">NN示例代码</a></p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/RedMudBUPT/gitpage_img/master/nn/nn_net1.jpg" alt="nn_net1"></p>
<h2 id="符号及标记"><a href="#符号及标记" class="headerlink" title="符号及标记"></a>符号及标记</h2><table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:left">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$L$</td>
<td style="text-align:left">代价函数 / 损失函数 / 最优化目标函数</td>
</tr>
<tr>
<td style="text-align:center">$w$</td>
<td style="text-align:left">“轴突”权重</td>
</tr>
<tr>
<td style="text-align:center">$\sigma$</td>
<td style="text-align:left">神经元“树突”的激活函数</td>
</tr>
<tr>
<td style="text-align:center">$z$</td>
<td style="text-align:left">带权输入 $z^l=w^{l-1}a^{l-1}$ ，即从上个神经元轴突传到该神经元树突上的值</td>
</tr>
<tr>
<td style="text-align:center">$a$</td>
<td style="text-align:left">神经元的激活值</td>
</tr>
<tr>
<td style="text-align:center">$\delta$</td>
<td style="text-align:left">中间变量，称为某层的误差，专用于反向传播算法</td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>注 1</em>：零层为真实数据，尚未前传，自然没有所谓的误差 $\delta^0$；因第零层的“轴突”上的权重 $w^0$ 尚未学习成功而导致的第一层的神经元的带权输入 $z^1$ 的误差，记为 $\delta^1$ 。</p>
<p><em>注 2</em>：每层“轴突”上的权重$w$ 的行数为希望学到的模式数，列数为输入数据的维度。</p>
<p><em>注 3</em>：做某一层的前传的时候，那一层的恒一神经元（用于模式中的偏置）才会被重新激活；当做为输出层时，该层的恒一神经元处于闭塞状态，是看不见的。当做某一层的反传的时候，是用该层神经元的误差$\delta$ 求<em>loss</em>对上一层“轴突”权重的偏导，而该层的恒一神经元没有所谓的误差，上一层的恒一神经元却有“轴突”权重，自然也有偏导，故只有求“轴突”权重偏导的那一层的恒一神经元会被重新激活，其余均处于闭塞状态。</p>
</blockquote>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><ul>
<li>梯度是个向量，指函数变化增加最快的地方，故沿负梯度的方向便能到达函数的极小值处。</li>
<li>在具体使用时，求得函数在某点（即<strong>此时网络的权重值与 <em>loss</em> 函数值</strong>）的梯度后，梯度在每一维上的分量值是该函数对该点在该维上的变化率（体现函数增量），函数在哪一维上的变化率越大，该点便在哪一维上距离函数的极点越远，为保证该点在所有维上同时到达函数的极点处，须采用带权步长进行移动（即每一维上的步长大小不一），其具体实现为基础步长（又称学习率）$\alpha​$乘上该点的梯度（即函数变化率越大的维上步长越大）。</li>
<li>上段中所说的函数是指理想的函数 $C(\omega)=\frac{1}{n}\sum|y-a|_2$ ，即将无穷个这样的样本点 $(x_1,\dotsb,x<em>m,y)</em>{n\to \infty,m&lt;\infty}$ 考虑进去得到的 $C(\omega)$ 。若采样没有噪声，且 $y$ 与 $(x_1,\dotsb,x<em>m)$ 线性相关，则直接采$m$个样本点求解线性方程就能得到参数 $\omega$ 的唯一解。由此可见，我们的方法是用线性函数来逼近非线性函数，构成神经网络后，便是用无数个线性网络的叠加来逼近任意非线性函数。假设激活函数$\sigma(z)=z$，则 $C(\omega,b)=\frac{1}{n}\sum|y-{\omega</em>{L-1}[\omega_{L-2}(\dotsb x)]}|_2$ ，故平常所说的<em>loss</em>函数是与网络结构无关的“基础<em>loss</em>函数”。</li>
</ul>
<h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>$$<br>\delta^l=(w^l\delta^{l+1})*\sigma’_{z^l}<br>\<br>\frac{\partial L}{\partial w^{l-1}}=\frac{1}{n}\delta^{l}(a^{l-1})^T<br>$$</p>
<p>在用矩阵编程计算梯度时，无需考虑具体矩阵乘积的细节和含义，在得到反向传播的标量表达式后，只需依照两条规则即可写出梯度的矩阵算式：</p>
<ol>
<li>依据标量表达式确定算式的结构；</li>
<li>依据<em>loss</em>对该层参数偏导的形状调整矩阵的顺序和形状。</li>
</ol>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>待续</p>
<h3 id="加快-mini-batch-训练的4种方法"><a href="#加快-mini-batch-训练的4种方法" class="headerlink" title="加快 mini-batch 训练的4种方法"></a>加快 mini-batch 训练的4种方法</h3><h4 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h4><p>$$<br>M(t) = \alpha M(t-1) - \epsilon \frac{\partial L}{\partial w}<br>\<br>w += M(t)<br>$$</p>
<ul>
<li><p>动量法能够减小高曲率方向上的震荡，使得小球尽快地损失掉重力势能。窃以为，公式结合物理原则，应为（尚未测试）：<br>$$<br>v(t) = \alpha M(t-1) - \epsilon (1 + \frac{1}{|\frac{\partial L}{\partial w}|})<br>\<br>w+=v(t)<br>\<br>M(t) = \frac{v(t)}{\alpha}<br>$$</p>
</li>
<li><p>Ilya Sutskever 在2012年提出了一种优化版本：先在历史累计方向上前进一大步，然后在新位置上计算梯度并修正方向。可以这么理解，最好犯错之后去改正它。</p>
</li>
</ul>
<h4 id="自适应学习率"><a href="#自适应学习率" class="headerlink" title="自适应学习率"></a>自适应学习率</h4><p>每个神经元入度的不同导致了流入不同神经元的“树突”权值的最佳学习率各不相同。当入度很大时，每个“树突”权值改变一点点，累积的改变量就很大了，很容易过量；而当入度很小时反之。所以一般采用一个全局学习率，然后根据对每个神经元各自做适当调整：<br>$$<br>w+=-\epsilon g \frac{\partial L}{\partial w}<br>$$<br>初始化局部 $g=1$ ，如果下次该权值的梯度符号不变则增加 $g+=0.05$ ，否则减小为 $g*=0.95$ 。</p>
<blockquote>
<p>注意：</p>
<ol>
<li>将 $g$ 限制在某个合理的范围，比如[0.1, 10] 或 [.01, 100]。</li>
<li>使用 full-batch 或很大的 mini-batch，毕竟这样保证了梯度的符号不易受 mini-batch 的采样误差影响。</li>
<li>综合自适应学习率和动量更新法，以当前梯度和当前速度的符号来决策 $g$ 的变化。</li>
</ol>
</blockquote>
<h4 id="RMSProp：将梯度除以历史数量级"><a href="#RMSProp：将梯度除以历史数量级" class="headerlink" title="RMSProp：将梯度除以历史数量级"></a>RMSProp：将梯度除以历史数量级</h4><p>全局学习率之所以难选，主要是因为每个期望的最终的权值的数量级相差巨大。在 full batch 中，可以利用梯度的符号来替代权值的更新量，从而解决这个问题。<a href="https://zhidao.baidu.com/question/1367991976404469819.html" target="_blank" rel="noopener">RProp</a>结合了“只用符号”和“自适应学习率”的思想，但它违反了 SGD 的中心思想（当学习率很小的时候，权值更新量其实是当前mini-batch的梯度和历史梯度的平均。举例来讲，假设某个权值在9个批次中的梯度是+0.1，在第10个批次中的梯度是-0.9，我们希望这个权值大致不变。），故不适用于 mini-batch。而RMSProp便融合了mini-batch的高效性、mini-batch间的有效平均和RProp的稳定性。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>对小数据集（10000以内）或者没有多少重复数据的大数据集，应当使用full-batch的一些优化方法，如Conjugate gradient、 LBFGS。然后试试adaptive learning rates, rprop …，它们是为神经网络而设计的方法。</li>
</ul>
<ul>
<li>对含有重复数据的大数据集，应当使用mini-batch。尝试动量法SGD，rmsprop 或LeCun的最新研究成果。</li>
</ul>
<h3 id="常用的激活函数及其对应的-loss-函数"><a href="#常用的激活函数及其对应的-loss-函数" class="headerlink" title="常用的激活函数及其对应的 loss 函数"></a>常用的激活函数及其对应的 <em>loss</em> 函数</h3><h4 id="线性激活函数与均方差-loss-函数"><a href="#线性激活函数与均方差-loss-函数" class="headerlink" title="线性激活函数与均方差 loss 函数"></a>线性激活函数与均方差 <em>loss</em> 函数</h4><p>$$<br>\sigma(z)=z<br>\<br>L=\frac{1}{2n}\sum_x||a(x)-y(x)||^2<br>$$</p>
<ul>
<li>$\delta^L=a-y$ ；</li>
<li>$\delta^l=w^l\delta^{l+1},(l=1,…,L-1)$（因为 $\sigma_z’=1$ ）。</li>
</ul>
<h4 id="sigmod-激活函数与交叉熵-loss-函数"><a href="#sigmod-激活函数与交叉熵-loss-函数" class="headerlink" title="sigmod 激活函数与交叉熵 loss 函数"></a>sigmod 激活函数与交叉熵 <em>loss</em> 函数</h4><p>$$<br>\sigma(z)=\frac{1}{1+e^{-z}}<br>\<br>L=-\frac{1}{n}\sum_x[y\ln a+(1-y)\ln (1-a)]<br>$$</p>
<ul>
<li>$\delta^L=a-y$ ；</li>
<li>$\delta^l=w^l\delta^{l+1}<em>a^l</em>(1-a^l)$ ，其中“ $*$ ”是<a href="https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/5446029?fr=aladdin#4" target="_blank" rel="noopener">Hadamard积</a>。</li>
</ul>
<blockquote>
<ol>
<li>用交叉熵而非均方差的原因是为了解决神经元饱和问题（指☞神经元因处于$\sigma’(z)$ 值很小的激活值位置而导致的梯度极小，进而带权步长极小的问题）。</li>
<li>对不同的$y$ 用不同的熵，故名为交叉熵，本质为用熵作为 <em>loss</em> 函数，熵越小，不确定信息越少，自然判断结果越可信。个人认为只是为了解决sigmod作为激活函数时导致的神经元饱和问题，与熵的定义没有关系。</li>
<li>该 <em>loss</em> 函数还可通过最大对数似然得到。</li>
<li>随机变量$x\sim (0,1)$ 和模型$a(x)$ 之间的交叉熵定义为</li>
</ol>
<p><strong>综：</strong>在目标值为1，而输出值为0的时候，梯度非常大；而当目标值与输出值几乎相等的时候，梯度接近0，模型不发生改变。<strong>（待分析）</strong></p>
</blockquote>
<h4 id="softmax-激活函数与对数似然-loss-函数（right-损失函数）"><a href="#softmax-激活函数与对数似然-loss-函数（right-损失函数）" class="headerlink" title="softmax 激活函数与对数似然 loss 函数（right 损失函数）"></a>softmax 激活函数与对数似然 <em>loss</em> 函数（right 损失函数）</h4><p>$$<br>\sigma(z_i)=\frac{e^{z_i}}{\sum_ie^{z_i}}<br>\<br>L=-\frac{1}{n}\sum_x\ln a_I,(a_I为真实类别对应神经元的激活值)<br>$$</p>
<ul>
<li>$\delta^L=a-y$ ；</li>
<li>$\delta^l=w^l\delta^{l+1}<em>a^l</em>(1-a^l)$ ，其中“ $*$ ”是<a href="https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/5446029?fr=aladdin#4" target="_blank" rel="noopener">Hadamard积</a>。</li>
</ul>
<blockquote>
<ol>
<li>更适用于将输出激活值解释为概率的场景。</li>
<li>设</li>
</ol>
</blockquote>
<h2 id="对该学习算法的一些理解"><a href="#对该学习算法的一些理解" class="headerlink" title="对该学习算法的一些理解"></a>对该学习算法的一些理解</h2><ol>
<li><ul>
<li>在梯度回传的过程中，$\omega$向量可能会变得非常大，则带权步长的移动只会引起在那个方向上微小的变化，以致很难有效地探索各种$\omega$模式。（大分量相对不怎么移动，小分量却相对移动很大，此处相对是指与分量自身相比，故会卡在某个方向上，以致很难有效地探索。）正则化的效果是让网络倾向于学习小一点的权重，让$\omega$只负责方向，而让$b$负责激活空间的位置。</li>
<li>另一个角度是，更小的权重意味着网络的行为不会因为噪声而改变太大，一个无规范化的网络可以使用大的权重来学习包含训练数据中的噪声的大量信息的复杂模型，而规范化的网络受限于根据训练数据中常见的模式来构造相对简单的模型，能够抵抗训练数据中的噪声的影响。</li>
</ul>
</li>
<li>在迭代过程中，还会出现神经元饱和问题，从梯度公式的角度想，是$\sigma’(z)$或$a$过小导致的梯度过小，从而引起学习缓慢的问题，解决方法便是构造合适的函数将梯度公式中的$\sigma’(z)$约掉；从网络的$loss$函数角度想，是寻找极低点时中途出现了原地踱步（小梯度）的情况，解决方法便是选用不同的激活函数与基础$loss$函数的搭配，从而得到形状更好的网络$loss$函数；从模式的可激活空间角度想，是样本$x$在模式$\omega$的激活空间上的分量值（带权输入）处于激活函数的平缓处（极低变化率）导致的模式$\omega$寻找进度迟缓，解决方法便是消除激活函数变化率对模式$\omega$的迭代寻找的影响。</li>
<li>​</li>
</ol>
<h2 id="实际应用中遇到的问题"><a href="#实际应用中遇到的问题" class="headerlink" title="实际应用中遇到的问题"></a>实际应用中遇到的问题</h2><h3 id="最优化问题：更新频率与幅度究竟应该为多大"><a href="#最优化问题：更新频率与幅度究竟应该为多大" class="headerlink" title="最优化问题：更新频率与幅度究竟应该为多大"></a>最优化问题：更新频率与幅度究竟应该为多大</h3><h3 id="泛化问题：如何防止过拟合？数据的两种噪音"><a href="#泛化问题：如何防止过拟合？数据的两种噪音" class="headerlink" title="泛化问题：如何防止过拟合？数据的两种噪音"></a>泛化问题：如何防止过拟合？数据的两种噪音</h3>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/08/numpy概览/" rel="next" title="numpy概览">
                <i class="fa fa-chevron-left"></i> numpy概览
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/03/14/凸优化/" rel="prev" title="凸优化">
                凸优化 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          
  
    <div class="comments" id="comments">
      <div id="hypercomments_widget"></div>
    </div>
  


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/photo.png"
                alt="redmud" />
            
              <p class="site-author-name" itemprop="name">redmud</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/bkseastone" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:bkseastone@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://plus.google.com/101415219341008595729" target="_blank" title="Google">
                    
                      <i class="fa fa-fw fa-google"></i>Google</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-globe"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://cherrysunshine.site/" title="同窗_刘" target="_blank">同窗_刘</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前馈神经网络-feed-forward-nueral-network"><span class="nav-number">1.</span> <span class="nav-text">前馈神经网络 (feed-forward nueral network)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#符号及标记"><span class="nav-number">1.1.</span> <span class="nav-text">符号及标记</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降法"><span class="nav-number">1.2.</span> <span class="nav-text">梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度"><span class="nav-number">1.2.1.</span> <span class="nav-text">梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播算法"><span class="nav-number">1.2.2.</span> <span class="nav-text">反向传播算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SGD"><span class="nav-number">1.2.3.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#加快-mini-batch-训练的4种方法"><span class="nav-number">1.2.4.</span> <span class="nav-text">加快 mini-batch 训练的4种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#动量法"><span class="nav-number">1.2.4.1.</span> <span class="nav-text">动量法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#自适应学习率"><span class="nav-number">1.2.4.2.</span> <span class="nav-text">自适应学习率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RMSProp：将梯度除以历史数量级"><span class="nav-number">1.2.4.3.</span> <span class="nav-text">RMSProp：将梯度除以历史数量级</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#总结"><span class="nav-number">1.2.4.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的激活函数及其对应的-loss-函数"><span class="nav-number">1.2.5.</span> <span class="nav-text">常用的激活函数及其对应的 loss 函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#线性激活函数与均方差-loss-函数"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">线性激活函数与均方差 loss 函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sigmod-激活函数与交叉熵-loss-函数"><span class="nav-number">1.2.5.2.</span> <span class="nav-text">sigmod 激活函数与交叉熵 loss 函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax-激活函数与对数似然-loss-函数（right-损失函数）"><span class="nav-number">1.2.5.3.</span> <span class="nav-text">softmax 激活函数与对数似然 loss 函数（right 损失函数）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对该学习算法的一些理解"><span class="nav-number">1.3.</span> <span class="nav-text">对该学习算法的一些理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实际应用中遇到的问题"><span class="nav-number">1.4.</span> <span class="nav-text">实际应用中遇到的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#最优化问题：更新频率与幅度究竟应该为多大"><span class="nav-number">1.4.1.</span> <span class="nav-text">最优化问题：更新频率与幅度究竟应该为多大</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#泛化问题：如何防止过拟合？数据的两种噪音"><span class="nav-number">1.4.2.</span> <span class="nav-text">泛化问题：如何防止过拟合？数据的两种噪音</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">redmud</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	

		<script type="text/javascript">
		_hcwp = window._hcwp || [];

		_hcwp.push({widget:"Bloggerstream", widget_id: 98984, selector:".hc-comment-count", label: "{\%COUNT%\}" });

		
		_hcwp.push({widget:"Stream", widget_id: 98984, xid: "2018/03/11/前馈神经网络/"});
		

		(function() {
		if("HC_LOAD_INIT" in window)return;
		HC_LOAD_INIT = true;
		var lang = (navigator.language || navigator.systemLanguage || navigator.userLanguage || "en").substr(0, 2).toLowerCase();
		var hcc = document.createElement("script"); hcc.type = "text/javascript"; hcc.async = true;
		hcc.src = ("https:" == document.location.protocol ? "https" : "http")+"://w.hypercomments.com/widget/hc/98984/"+lang+"/widget.js";
		var s = document.getElementsByTagName("script")[0];
		s.parentNode.insertBefore(hcc, s.nextSibling);
		})();
		</script>

	












  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
